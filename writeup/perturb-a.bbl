\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and Telgarsky]{ba17}
Bartlett, P., Foster, D., and Telgarsky, M.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock \emph{NeurIPS}, 2017.

\bibitem[Bonnabel(2013)]{bo13}
Bonnabel, S.
\newblock Stochastic gradient descent on riemannian manifolds.
\newblock \emph{IEEE Transactions on Automatic Control}, 2013.

\bibitem[Bottou(1991)]{bo91}
Bottou, L.
\newblock Stochastic gradient learning in neural networks.
\newblock \emph{Neuro-N\^imes}, 1991.

\bibitem[Cauchy(1847)]{ca47}
Cauchy, A.-L.
\newblock M\'ethode g\'en\'erale pour la r\'esolution des syst\'emes
  d'\'equations simultan\'ees.
\newblock \emph{Comptes rendus de l'Acad\'emie des Sciences}, 1847.

\bibitem[Chaudhari \& Soatto(2018)Chaudhari and Soatto]{ch18}
Chaudhari, P. and Soatto, S.
\newblock Stochastic gradient descent performs variational inference, converges
  to limit cycles for deep networks.
\newblock \emph{ICLR}, 2018.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{di17}
Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y.
\newblock Sharp minima can generalize for deep nets.
\newblock \emph{ICLR}, 2017.

\bibitem[Dyson(1949{\natexlab{a}})]{dy49a}
Dyson, F.
\newblock The radiation theories of tomonaga, schwinger, and feynman.
\newblock \emph{Physical Review}, 1949{\natexlab{a}}.

\bibitem[Dyson(1949{\natexlab{b}})]{dy49b}
Dyson, F.
\newblock The $s$ matrix in quantum electrodynamics.
\newblock \emph{Physical Review}, 1949{\natexlab{b}}.

\bibitem[Gell-Mann \& Goldberger(1954)Gell-Mann and Goldberger]{ge54}
Gell-Mann, M. and Goldberger, M.
\newblock Scattering of low-energy photons by particles of spin $\frac{1}{2}$.
\newblock \emph{Physical Review}, 1954.

\bibitem[Goyal et~al.(2018)Goyal, Doll\'{a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{go18}
Goyal, P., Doll\'{a}r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola,
  A., Tulloch, A., Jia, Y., and He, K.
\newblock Accurate, large minibatch sgd.
\newblock \emph{Data @ Scale}, 2018.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{ho17}
Hoffer, E., Hubara, I., and Soudry, D.
\newblock Train longer, generalize better.
\newblock \emph{NeurIPS}, 2017.

\bibitem[Jastrz\k{e}bski et~al.(2018)Jastrz\k{e}bski, Kenton, Arpit, N.,
  Fischer, Y., and A.]{ja18}
Jastrz\k{e}bski, S., Kenton, Z., Arpit, D., N., B., Fischer, A., Y., B., and
  A., S.
\newblock Three factors influencing minima in sgd.
\newblock \emph{Arxiv Preprint}, 2018.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{ke17}
Keskar, N., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{ICLR}, 2017.

\bibitem[Kiefer \& Wolfowitz(1952)Kiefer and Wolfowitz]{ki52}
Kiefer, J. and Wolfowitz, J.
\newblock Stochastic estimation of the maximum of a regression function.
\newblock \emph{Annals of Mathematical Statistics}, 1952.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{le15}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{Nature}, 2015.

\bibitem[Liao et~al.(2018)Liao, Miranda, Banburski, Hidary, and Poggio]{li18}
Liao, Q., Miranda, B., Banburski, A., Hidary, J., and Poggio, T.
\newblock A surprising linear relationship predicts test performance in deep
  networks.
\newblock \emph{Center for Brains, Minds, and Machines Memo 91}, 2018.

\bibitem[Nesterov(2004)]{ne04}
Nesterov, Y.
\newblock Lectures on convex optimization: Minimization of smooth functions.
\newblock \emph{Springer Applied Optimization 87, Section 2.1}, 2004.

\bibitem[Neyshabur et~al.(2017{\natexlab{a}})Neyshabur, Bhojanapalli,
  McAllester, and Srebro]{ne17a}
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N.
\newblock Exploring generalization in deep learning.
\newblock \emph{NeurIPS}, 2017{\natexlab{a}}.

\bibitem[Neyshabur et~al.(2017{\natexlab{b}})Neyshabur, Tomioka, Salakhutdinov,
  and Srebro]{ne17b}
Neyshabur, B., Tomioka, R., Salakhutdinov, R., and Srebro, N.
\newblock Geometry of optimization and implicit regularization in deep
  learning.
\newblock \emph{Chapter 4 from Intel CRI-CI: Why and When Deep Learning Works
  Compendium}, 2017{\natexlab{b}}.

\bibitem[Penrose(1971)]{pe71}
Penrose, R.
\newblock Applications of negative dimensional tensors.
\newblock \emph{Combinatorial Mathematics and its Applications}, 1971.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{ro51}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{Pages 400-407 of The Annals of Mathematical Statistics.}, 1951.

\bibitem[Roberts(2018)]{ro18}
Roberts, D.
\newblock Sgd implicitly regularizes generalization error.
\newblock \emph{NeurIPS: Integration of Deep Learning Theories Workshop}, 2018.

\bibitem[Stein(1956)]{st56}
Stein, C.
\newblock Inadmissibility of the usual estimator for the mean of a multivariate
  normal distribution.
\newblock \emph{Berkeley Symposium on Mathematical Probability}, 1956.

\bibitem[Werbos(1974)]{we74}
Werbos, P.
\newblock Beyond regression: New tools for prediction and analysis.
\newblock \emph{Harvard Thesis}, 1974.

\bibitem[Wu et~al.(2018)Wu, C., and E]{wu18}
Wu, L., C., M., and E, W.
\newblock How sgd selects the global minima in over-parameterized learning.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Yaida(2019)]{ya19}
Yaida, S.
\newblock Fluctuation-dissipation relations for stochastic gradient descent.
\newblock \emph{ICLR}, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and Vinyals]{zh17}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{ICLR}, 2017.

\end{thebibliography}
