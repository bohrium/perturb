%   author: samtenka
%   change: 2020-06-01
%   create: 2020-05-29
%   descrp: LaTeX source for perturb project
%   to use: compile along with perturb.bib and diagram and plot directories

%==============================================================================
%=====  LATEX PREAMBLE  =======================================================
%==============================================================================

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Document Styling  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}      

%\usepackage[nonatbib]{neurips_2020}
\usepackage{natbib}
%\usepackage[final]{neurips_2020}

%---------------------  mathematics  ------------------------------------------

\usepackage{amsmath, amssymb, amsthm, amsfonts}
\usepackage{mathtools, nicefrac}

\usepackage{setspace}
%---------------------  tables  ----------------------------------------------- 

\usepackage{booktabs}
\usepackage{array}
\newcolumntype{L}{>{$}l<{$}}

%---------------------  graphics and figures  ---------------------------------

\usepackage{graphicx}
\usepackage{wrapfig, float, subfigure}
\usepackage{hanging, txfonts, ifthen}

\newcommand{\ofsix}[1]{
    {\tiny \raisebox{0.04cm}{$\substack{
        \ifthenelse{\equal{#1}{0}}{{\color{moor}\blacksquare}}{\square}
        \ifthenelse{\equal{#1}{2}}{{\color{moor}\blacksquare}}{\square}    
        \ifthenelse{\equal{#1}{4}}{{\color{moor}\blacksquare}}{\square} \\
        \ifthenelse{\equal{#1}{1}}{{\color{moor}\blacksquare}}{\square}    
        \ifthenelse{\equal{#1}{3}}{{\color{moor}\blacksquare}}{\square}
        \ifthenelse{\equal{#1}{5}}{{\color{moor}\blacksquare}}{\square}
    }$}}
}

%---------------------  colors  -----------------------------------------------

\usepackage{xcolor, framed}
\definecolor{moolime}{rgb}{0.90,1.00,0.90}
\definecolor{moosky}{rgb}{0.90,0.90,1.00}
\definecolor{moopink}{rgb}{1.00,0.90,0.90}
\definecolor{moor}{rgb}{0.8,0.2,0.2}
\definecolor{moog}{rgb}{0.2,0.8,0.2}
\definecolor{moob}{rgb}{0.2,0.2,0.8}
\definecolor{mooteal}{rgb}{0.1,0.6,0.4}

%---------------------  intertext: footnotes and hyperlinks  ------------------ 

\usepackage[perpage]{footmisc}
\renewcommand*{\thefootnote}{
    \color{red}
    \arabic{footnote}
    %\fnsymbol{footnote}
} 

\usepackage{hyperref}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Theorem Environments  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  mathematical results  ---------------------------------

\theoremstyle{plain}
    \newtheorem*{klem*}{Key Lemma}
    \newtheorem{thm}{Theorem}
    \newtheorem*{thm*}{Theorem}
    \newtheorem{cor}{Corollary}
    \newtheorem{prop}{Proposition}

%---------------------  mathematical questions  -------------------------------

    \newtheorem{conj}{Conjecture}
    \newtheorem{quest}{Question}
    \newtheorem*{quest*}{Question}
    \newtheorem*{quests*}{Questions}

%---------------------  definitions, answers, remarks  ------------------------

\theoremstyle{definition}
    \newtheorem{defn}{Definition}
    \newtheorem*{answ*}{Answer}
    \newtheorem{rmk}{Remark}
    \newtheorem*{midea*}{Main Idea}
    \newtheorem*{rmk*}{Remark}
    \newtheorem{exm}{Example}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Custom Math Commands  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  expanding containers  ---------------------------------

\newcommand{\wrap}[1]{\left(#1\right)}
\newcommand{\wasq}[1]{\left[#1\right]}
\newcommand{\wang}[1]{\left\langle#1\right\rangle}
\newcommand{\wive}[1]{\left\llbracket#1\right\rrbracket}
\newcommand{\worm}[1]{\left\|#1\right\|}
\newcommand{\wabs}[1]{\left|#1\right|}
\newcommand{\wurl}[1]{\left\{#1\right\}}

\newcommand{\partitionbox}[1]{
    \text{
        \fboxsep=0.5pt
        \tiny
        \fbox{#1}
    }
}

%---------------------  special named objects  --------------------------------

\newcommand{\Free}{\mathcal{F}}
\newcommand{\Forg}{\mathcal{G}}
\newcommand{\Mod}{\mathcal{M}}
\newcommand{\Hom}{\text{\textnormal{Hom}}}
\newcommand{\Aut}{\text{\textnormal{Aut}}}
\newcommand{\image}{\text{\textnormal{im}}}
\newcommand{\uvalue}{\text{\textnormal{uvalue}}}
\newcommand{\rvalue}{\text{\textnormal{rvalue}}}
\newcommand{\edges}{\text{\textnormal{edges}}}
\newcommand{\ords}{\text{\textnormal{ords}}}
\newcommand{\parts}{\text{\textnormal{parts}}}
\newcommand{\SGD}{\text{\textnormal{SGD}}}
\DeclareMathOperator*{\Avg}{\text{\sffamily A}}
\newcommand{\expc}{\mathbb{E}}
\newcommand{\expct}[1]{\mathbb{E}\left[#1\right]}

%---------------------  fancy letters  ----------------------------------------

\newcommand{\Aa}{\mathcal{A}}
\newcommand{\Bb}{\mathcal{B}}
\newcommand{\Cc}{\mathcal{C}}   \newcommand{\CC}{\mathbb{C}}
\newcommand{\Dd}{\mathcal{D}}
\newcommand{\Ee}{\mathcal{E}}
\newcommand{\Ff}{\mathcal{F}}
\newcommand{\Gg}{\mathcal{G}}
\newcommand{\Hh}{\mathcal{H}}
\newcommand{\Ll}{\mathcal{L}}
\newcommand{\Mm}{\mathcal{M}}
\newcommand{\Nn}{\mathcal{N}}   \newcommand{\NN}{\mathbb{N}}
\newcommand{\Oo}{\mathcal{O}}
\newcommand{\Pp}{\mathcal{P}}
\newcommand{\Qq}{\mathcal{Q}}   \newcommand{\QQ}{\mathbb{Q}}
\newcommand{\Rr}{\mathcal{R}}   \newcommand{\RR}{\mathbb{R}}
\newcommand{\Ss}{\mathcal{S}}
\newcommand{\Tt}{\mathcal{T}}
\newcommand{\Uu}{\mathcal{U}}
\newcommand{\Vv}{\mathcal{V}}
\newcommand{\Ww}{\mathcal{W}}
\newcommand{\Xx}{\mathcal{X}}
\newcommand{\Yy}{\mathcal{Y}}
\newcommand{\Zz}{\mathcal{Z}}   \newcommand{\ZZ}{\mathbb{Z}}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Pictures  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  pictures with specified width or height  --------------

\newcommand{\plotmoow}[3]{\includegraphics[width=#2          ]{../#1}}
\newcommand{\plotmooh}[3]{\includegraphics[         height=#3]{../#1}}
\newcommand{\pmoo}[2]{\includegraphics[height=#1]{../plots/#2}}

%---------------------  inline diagrams of various sizes  ---------------------

\newcommand{\sizeddia}[2]{
    \begin{gathered}
        \includegraphics[scale=#2]{../diagrams/#1.png}
    \end{gathered}
}
\newcommand{\bdia}[1]{\protect \sizeddia{#1}{0.22}}
\newcommand{\dia} [1]{\protect \sizeddia{#1}{0.18}}
\newcommand{\mdia}[1]{\protect \sizeddia{#1}{0.14}}
\newcommand{\sdia}[1]{\protect \sizeddia{#1}{0.10}}

\newcommand{\mend}{\hfill $\Diamond$}

\newcommand{\Gauss}{\textsc{Gauss}}
\newcommand{\Archimedes}{\textsc{Archimedes}}
\newcommand{\MeanEstimation}{\textsc{Mean Estimation}}

%==============================================================================
%=====  FRONT MATTER  =========================================================
%==============================================================================

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Title and Author  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\title{%
    A Perturbative Analysis of Stochastic Descent
}

\author{%
    \textbf{Samuel C.~Tenka} \\
    Computer Science and AI Lab \\
    Massachusetts Institute of Technology \\
    Cambridge, MA 02139 \\
    \texttt{colimit@mit.edu}
}

\begin{document}

    \maketitle
    
    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Abstract  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    
    \begin{abstract}
        %
        %-------------  hammer and general nail  ------------------------------
        %
        We analyze Stochastic Gradient Descent (SGD) at small learning rates.
        Unlike prior analyses based on stochastic differential equations, our
        theory models discrete time and hence non-Gaussian noise.
        %
        %-------------  applications  -----------------------------------------
        %
        We prove that gradient noise systematically pushes SGD toward flatter
        minima.  We characterize when and why flat minima overfit less than
        other minima.  We generalize the Akaike Info.\ Criterion (AIC) to a
        smooth estimator of overfitting, hence enabling gradient-based model
        selection.  We show how non-stochastic GD with a modified loss function
        may emulate SGD.
        %
        %-------------  mention of experiments  -------------------------------
        %
        We verify our predictions on convnets for CIFAR-10 and Fashion-MNIST.
    \end{abstract}
    
%==============================================================================
%=====  INTRODUCTION  =========================================================
%==============================================================================

\section{Introduction}

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Orienting Invitation  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    %-----------------  object of study  --------------------------------------

    Practitioners benefit from the intuition that SGD approximates noiseless GD
    \cite{bo91}.  In this paper, we refine that intuition by showing how
    gradient noise biases learning toward certain areas of weight space.
    %
    %-----------------  vs ODE and SDE  ---------------------------------------
    %
    Departing from prior work, we model discrete time and hence non-Gaussian
    noise.  Indeed, we derive corrections to continuous-time, Gaussian-noise
    approximations such as ordinary and stochastic differential equations (ODE,
    SDE).
    For example, we construct a loss landscape on which SGD eternally cycles
    counterclockwise, a phenomenon impossible with ODEs. 
    %
    %-----------------  organization Plan  ------------------------------------
    %
    Leaving the rigorous development of our general theory to {\color{red}
    APPENDIX}, our paper body highlights and intuitively discusses the theory's
    main corollaries.

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Soft Benefits: Physical Intuition and Further Applications  ~~~

    %-----------------  retrospective  ----------------------------------------
    %
    Our work offers a novel viewpoint of SGD as many concurrent interactions
    between weights and data.  Diagrams such as $\sdia{c(01-2-3)(02-12-23)}$,
    analogous to those of \cite{fe49, pe71}, depict these interactions. 
    %
    %-----------------  prospective  ------------------------------------------
    %
    In the appendix, we discuss this bridge to physics --- and its relation to
    Hessian methods and natural GD --- as topics for future research.  We also
    discuss how this work may ameliorate or exacerbate the learning community's
    disproportionate contribution to climate change.  More broadly, our work
    adds to the body of theory on optimization in the face of uncertainty,
    theory that may one day inform solutions to emerging issues in user privacy
    and pedestrian safety.

    \subsection{Example of diagram-based computation of SGD's test loss}

        \newcommand{\nb} { \nabla }
        \newcommand{\lx} { l_x(\theta) }
        \newcommand{\teq} { \triangleq }
        \newcommand{\ex}[1] { \expc_x \wasq{#1} }

        If we run SGD for $T$ gradient steps with learning rate $\eta$ starting
        at weight $\theta_0$, then by Taylor expansion we may express the
        expected test loss of the final weight $\theta_T$ in terms of
        statistics of the loss landscape evaluated at $\theta_0$.  Our
        technical contribution is to organize the computation of this Taylor
        series via combinatorial objects we call
        \emph{diagrams}:
        \begin{midea*}[Informal]
            We can enumerate all diagrams, and assign to each diagram a number
            depending on $\eta, T$, such that summing these numbers over all
            diagrams yields SGD's expected test loss.  Restricting to 
            diagrams with $\leq d$ edges leads to $o(\eta^d)$ error.
        \end{midea*}

        Deferring details to later sections and appendices, we illustrate this
        work flow.  First, let $l_x(\theta)$ be weight $\theta$'s loss on
        datapoint $x$.  We define a tensor$\,\leftrightarrow\,$diagram
        dictionary:
        \begin{center}
            \begin{tabular}{ll}
                $G \teq \ex{\nb\lx}       \teq \mdia{MOO(0)(0)}     $ &                                                             \\
                $H \teq \ex{\nb\nb\lx}    \teq \mdia{MOO(0)(0-0)}   $ & $ C \teq \ex{(\nb\lx - G)^2} \teq \mdia{MOOc(01)(0-1)}    $ \\
                $J \teq \ex{\nb\nb\nb\lx} \teq \mdia{MOO(0)(0-0-0)} $ & $ S \teq \ex{(\nb\lx - G)^3} \teq \mdia{MOOc(012)(0-1-2)} $ 
            \end{tabular}
        \end{center}

        Here, $G, H, J$ denote the loss's derivatives w.r.t.\
        $\theta$, and $G, C, S$ denote the gradient's 
        cumulants w.r.t.\ the randomness in $x$.
        Each $\nabla^d l_x$ corresponds to a node with $d$ edges emanating, and
        fuzzy outlines group nodes that occur within the same expectation.  

        We may pair together the loose ends of the above (and of
        analogues with more edges) to obtain
        \emph{diagrams}.\footnote{
            A diagram's colors and geometric layout lack meaning: we
            {\color{moor} color} only for convenient reference, e.g.\ to
            a diagram's ``green nodes''.  Only the topology of a diagram
            --- not its size or angles --- appear in our theory.
        }
        E.g., we may join
        $
            C = \sdia{MOOc(01)(0-1)}
        $
        with
        $
            H = \sdia{MOO(0)(0-0)}
        $
        to get
        $
            \sdia{c(01-2)(02-12)}
        $.
        As another example, we may join two copies of
        $
            G = \sdia{MOO(0)(0)}
        $
        with two copies of
        $
            H = \sdia{MOO(0)(0-0)}
        $
        to get
        $
            \sdia{c(0-1-2-3)(01-12-23)} 
        $.
        Intuitively, each diagram represents the interaction of its parts: of
        gradients ($G$), noise ($C, S, \cdots$) and curvature ($H, J, \cdots$). 
        {\color{red} APPENDIX} interprets these diagrams physically.
        
        %
        \begin{exm} \label{exm:first}
            Does non-Gaussian noise affect SGD?
            Specifically, since the skew $S$ measures non-gaussianity, let's
            compute how $S$ affects test loss. The recipe is to identify the
            fewest-edged diagrams containing $S = \sdia{MOOc(012)(0-1-2)}$.  In
            this case, there is one fewest-edged diagram ---
            $\sdia{c(012-3)(03-13-23)}$; it results from joining $S$ with
            $J=\sdia{MOO(0)(0-0-0)}$.  To evaluate a diagram, we multiply its
            components (here, $S, J$) with exponentiated $\eta H$'s, one for
            each edge:
            \begin{align*} %\label{eqn:nongauss}
                -\frac{\eta^3}{3!}
                \sum_{\mu\nu\lambda}
                    S_{\mu\nu\lambda}
                    \frac{
                        1 - \exp(-T\eta (H_{\mu\mu} + H_{\nu\nu} + H_{\lambda\lambda}))
                    }{
                        \eta (H_{\mu\mu} + H_{\nu\nu} + H_{\lambda\lambda})
                    }
                    J_{\mu\nu\lambda}
            \end{align*}
            This is $S$'s leading order contribution to SGD's test loss
            written in an eigenbasis of $\eta H$.
        \end{exm}
        \begin{rmk}
            For large $T$ and isotropic $\eta H$, this becomes
            $
                - (\eta^3/3!)
                \sum_{\mu\nu\lambda}
                    S_{\mu\nu\lambda} J_{\mu\nu\lambda} / 3 \eta \wabs{H}
            $.
            Since $J = \nabla H$, $J / \wabs{H}$ measures the relative change
            in curvature $H$ w.r.t.\ $\theta$.  So non-gaussian noise affects
            SGD proportion to the logarithmic derivative of curvature.
        \end{rmk}

%==============================================================================
%=====  BACKGROUND AND NOTATION  ==============================================
%==============================================================================

\subsection{Background, notation, and assumptions} \label{sect:background}
       
    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Tensor Conventions  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    %\subsection{Tensor conventions}
        We sometimes implicitly sum repeated Greek indices: if a covector $A$
        and a vector $B$\footnote{
            Vectors/covectors are also called column/row vectors.
        } have coefficients $A_\mu, B^\mu$, then 
        $
            A_\mu B^\mu
            \triangleq
            \sum_\mu A_\mu \cdot B^\mu
        $.
        %To expedite dimensional analysis,
        We regard the learning rate as an
        inverse metric $\eta^{\mu\nu}$ that converts gradient covectors to
        displacement vectors \citep{bo13}.  We use the learning rate
        $\eta$ to raise indices: e.g.,
        $
            H^{\mu}_{\lambda}
            \triangleq
            \eta^{\mu\nu} H_{\nu\lambda}
        $ and
        $
            C^{\mu}_{\mu}
            \triangleq
            \sum_{\mu \nu} \eta^{\mu\nu} \cdot C_{\nu\mu}
        $.
        Though $\eta$ is a tensor, we may still define $o(\eta^d)$: a quantity
        $q$ \emph{vanishes to order $\eta^d$} when $\lim_{\eta\to 0} q/p(\eta)
        = 0$ for some homogeneous degree-$d$ polynomial $p$.

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  The Loss Landscape  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    %\subsection{Loss landscape}

        %-------------  the landscape  ----------------------------------------

        We fix a loss function $l:\Mm\to\RR$ on a space $\Mm$ of weights.  We
        fix a distribution $\Dd$ from which unbiased estimates of $l$ are
        drawn.  We write $l_x$ for a generic sample from $\Dd$ and $(l_n: 0\leq
        n<N)$ for a training sequence drawn i.i.d.\ from $\Dd$.  We refer both
        to $n$ and to $l_n$ as \emph{training points}.  We assume Appendix
        {\color{red} FILL IN}'s hypotheses, e.g.\ that $l, l_x$ are
        analytic and that moments exist.
        %
        %-------------  specialization to a common case  ----------------------
        %
        E.g., our theory models $\tanh$ networks with cross entropy loss on
        bounded data --- with arbitrary weight sharing, skip connections, soft
        attention, dropout, and weight decay.
        
    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Names of SGD Parameters  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    %\subsection{SGD terminology}
        %SGD performs $\eta$-steepest descent on the estimates $l_n$.
        %
        Our general theory describes SGD with any number
             $N$ of training points,
             $T$ of updates, and 
             $B$ of points per batch.
        SGD then runs $T$ many updates (i.e. $E=TN/B$ epochs, i.e. $M=T/N$
        updates per point)
        $
            \theta^\mu
            \coloneqq
            \theta^\mu -
            \eta^{\mu\nu} \nabla_\nu
                \sum_{n\in \Bb_t} l_n(\theta) / B
        $,
        where $\Bb_t$ is the $t$th batch.  Our paper's body --- but not
        appendices --- will assume \textbf{SGD has $E=B=1$ and GD has $T=B=N$}
        unless otherwise stated.

%==============================================================================
%    RELATED WORK    
%==============================================================================

\subsection{Related Work} \label{sect:related}

    %--------------------------------------------------------------------------
    %           Analyzing Overfitting; Relevance of Optimization; SDE Errs  
    %--------------------------------------------------------------------------

    Several research programs treat the overfitting of SGD-trained networks
    \citep{ne17a}.  E.g., \cite{ba17} controls the Rademacher complexity of
    deep hypothesis classes, leading to optimizer-agnostic generalization
    bounds.  Yet SGD-trained networks generalize despite their ability to
    shatter large sets \citep{zh17}, so generalization must arise from not only
    architecture but also optimization \citep{ne17b}.  Others approximate
    SGD by SDE to analyze implicit regularization (e.g.\ \cite{ch18}), but,
    per \cite{ya19a}, such continuous-time analyses cannot treat SGD noise
    correctly.
    %
    %%--------------------------------------------------------------------------
    %%           We Extend Dan's Approach                     
    %%--------------------------------------------------------------------------
    %
    We avoid these pitfalls by Taylor expanding around $\eta=0$ as in
    \cite{ro18}; unlike that work, we generalize beyond order $\eta^1$ and
    $T=2$.
    
    %--------------------------------------------------------------------------
    %           Phenomenology of Rademacher Correlates such as Hessians
    %--------------------------------------------------------------------------

    Our theory is vacuous for large $\eta$.  Other analyses treat
    large-$\eta$ learning phenomenologically, whether by finding empirical
    correlates of gen.\ gap \citep{li18}, by showing that \emph{flat} minima
    generalize (\cite{ho17}, \cite{ke17}, \cite{wa18}), or by showing that
    \emph{sharp} minima generalize (\cite{st56}, \cite{di17}, \cite{wu18}).
    Our theory reconciles these clashing claims.
    
    %--------------------------------------------------------------------------
    %           Our Work vs Other Perturbative Approaches            
    %--------------------------------------------------------------------------

    Prior work analyzes SGD perturbatively: \cite{dy19} perturb in inverse
    network width, using 't Hooft diagrams to correct the Gaussian Process
    approximation for specific deep nets.  Perturbing to order $\eta^2$,
    \cite{ch18} and \cite{li17} assume uncorrelated Gaussian noise, so they
    cannot describe SGD's gen\. gap.  We use Penrose diagrams to compute test
    losses to arbitrary order $\eta$.  We allow for correlated, non-Gaussian
    noiseand thus \emph{any} smooth architecture.  E.g., we do not assume
    information-geometric relationships between $C$ and
    $H$,
    \footnote{
        Disagreement of $C$ and $H$ is typical in modern learning \citep{ro12,
        ku19}.
    }
    so we may model VAEs. 

%==============================================================================
%=====  DIAGRAM CALCULUS FOR SGD  =============================================
%==============================================================================

\section{Theory, Specialized to $E=B=1$ SGD's Test Loss} \label{sect:calculus}

    %\subsection{Diagrams, embeddings, and re-summed values}
        \begin{wraptable}{r}{5cm}
            \begin{spacing}{0.8}
            \begin{tabular}{p{5cm}}
                \textbf{Examples}:
                The diagrams
                $\sdia{c(0-1)(01)}$, $\sdia{c(012-3)(03-13-23)}$ each have $2$
                parts; $\sdia{c(0-12-3)(03-13-23)}$, $\sdia{c(01-2-3)(02-13-23)}$
                each have $3$.
                %
                Corollaries \ref{cor:overfit}, \ref{cor:epochs},
                \ref{cor:batch} do not assume $E=B=1$, so they feature
                $\sdia{c(01)(01)}$ and $\sdia{c(01-2)(01-12)}$, generalized
                diagrams that violate the path condition. 
                %
                The diagrams $\sdia{c(0-1-2)(02-12)}$, $\sdia{c(01-2)(01-12)}$
                are irreducible; due to their green nodes,
                $\sdia{c(0-1-2)(01-12)}$, $\sdia{c(01-2-3)(03-12-23)}$ are not.
            \end{tabular}
            \end{spacing}
        \end{wraptable}

        A \emph{diagram} is a finite rooted tree equipped with a partition
        of its nodes obeying the \emph{path condition}: no path from leaf to
        root may encounter any part more than once.
        We specify the root by drawing it rightmost.  We draw the parts of 
        the partition by grouping the nodes within each part via fuzzy
        outlines. 
        %
        A diagram is \emph{irreducible} when each of its degree-$2$ nodes is in
        a part of size one.
        %
        An \emph{embedding} $f$ of a diagram $D$ is an injection from the
        diagram's parts to (integer) times $0 \leq t \leq T$ that sends the
        root to $T$ and such that, for each path from leaf to root, the
        corresponding sequence of times is increase.  E.g., $f$ might send
        $\sdia{c(01-2-3)(02-13-23)}$'s red part to $t=3$ and its green part to
        $t=4$, but not vice versa.
        %
        {\color{red} Let $\wabs{\Aut_f(D)}$ count the graph automorphisms of
        $D$ that commute with $f$.}

        %%%%%%%%%

        Up to unbiasing terms,\footnote{
            E.g., we actually define $\sdia{MOOc(01)(0-1)}$ to be the cumulant
            $C = \ex{(\nb\lx - G)^2}$, not the moment $\ex{(\nb\lx)^2}$.
            This centering is routine (see {\color{red} APPENDIX}), tedious to
            keep notating, and un-germane, so we ignore it.
        }
        the \emph{re-summed value} $\rvalue_f(D)$ is constructed as follows.
        %
        \textbf{Node rule}: insert a factor a $\nabla^d l_x$for each degree $d$
        node. 
        %
        \textbf{Edge rule}: for each edge whose endpoints $f$ sends to times
        $t, t^\prime$, insert a factor of $K^{\wabs{t^\prime-t}-1} \eta$
        where $K \triangleq (I-\eta H)$.
        %
        \textbf{Outline rule}: group the nodes in each part within expectation
        brackets $\expc_x{}$.
        %
        %
        E.g., if $f$ maps $\sdia{c(012-3)(03-13-23)}$'s red part to time $t =
        T-\Delta t$, then (the red part gives $S$; the green part, $J$):
        $$
            \rvalue_f\wrap{\sdia{c(012-3)(03-13-23)}} = 
            S_{\mu\lambda\rho}
                (K^{\Delta t-1}\eta)^{\mu\nu}
                (K^{\Delta t-1}\eta)^{\lambda\sigma}
                (K^{\Delta t-1}\eta)^{\rho\pi}
            J_{\nu\sigma\pi}
        $$
        In fact, we may integrate this expression per Remark
        \ref{rmk:integrate} to recover Example \ref{exm:first}.

    \subsection{Main result}

    %\subsection{Recipe for SGD's expected test loss}
        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        %~~~~~  Recipe for Test Loss  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        Theorem \ref{thm:resum} expresses SGD's test loss as a sum over
        diagrams.  A diagram with $d$ edges scales as $O(\eta^d)$, so the
        following is a series in $\eta$.  We will truncate the series to small
        $d$, thus focusing on few-edged diagrams and easing the combinatorics
        of embeddings.
        \begin{thm}[Special Case] \label{thm:resum}
            For any $T$: for $\eta$ small enough, SGD has expected test loss
            \begin{equation*} \label{eq:resum}
                \sum_{\substack{\text{irreducible} \\ \text{diagrams}~D }}
                \sum_{\substack{\text{embeddings} \\ f~\text{of}~D}}
                \frac{(-1)^{|\edges(D)|}}{\wabs{\Aut_f(D)}}
                \,
                {\rvalue_f}(D)
            \end{equation*}
        \end{thm}

        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        %~~~~~  Convergence  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
        \begin{thm}[Long-Term Behavior near a Local Minimum] \label{thm:converge}
            If $\theta_\star$ locally minimizes $l$ and for some positive form
            $Q$, $Q < \nabla\nabla l_x(\theta_\star)$ for all $x$, then when we
            initialize SGD sufficiently close to $\theta_\star$, the
            $d$th-order truncation of Theorem \ref{thm:resum} converges as $T$
            diverges.
        \end{thm}

        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        %~~~~~  Simplifications  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
        \begin{rmk} \label{rmk:integrate}
            We may approximate sums by integrals and $(I-\eta H)^t$ by $\exp(-
            \eta H t)$, reducing to a routine integration of exponentials
            at the cost of an error factor $1 + o(\eta)$.
        \end{rmk}
      
    \subsection{Insights from the Formalism}
    
        \subsubsection{SGD descends on a $C$-smoothed landscape and prefers
        minima flat w.r.t.\ $C$.}
    
            \begin{cor}[Computed from $\sdia{(01-2-3)(02-12-23)}$] \label{cor:entropic}
                Initialized at a test minimum, and run for long times $T \gg
                1/\eta H$, SGD drifts with an expected time-averaged velocity
                of
                $$
                    v^\lambda
                    =
                    \frac{\eta^3}{T}
                    \sum_{\mu\nu}
                        C_{\mu\nu}
                        \frac{1}{\eta (H_{\mu\mu} + H_{\nu\nu})}
                        J_{\mu\nu\lambda}
                        \frac{1}{H_{\lambda\lambda}}
                    + o(\eta^2)
                    ~~~~~
                    ~~~~~
                    ~~~~~
                    \text{in an eigenbasis for}~\eta H
                $$
            \end{cor}
            
            Intuitively, $D = \sdia{c(01-2-3)(02-12-23)}$
            contains a subdiagram $\sdia{c(01-2)(02-12)} = (K\eta)^2 CH$.
            By a routine check, $CH+o(\eta^2)$ is the loss increase upon
            convolving $l$ with a $C$-shaped Gaussian.  Since
            $D$ connects the subdiagram to {\color{red} to the test
            measurement} via $1$ edge, it couples $CH$ to $l$'s linear part, so
            it represents a displacement of $\theta$ away from high $CH$.  In
            short, \emph{SGD descends on a covariance-smoothed landscape}.
            That is, SGD prefers from among a valley's minima those that are
            flat w.r.t.\ $C$.  Figure \ref{fig:cubicandspring} (left)
            illustrates this intuition.
    
            \cite{ya19b} reports a small-$T$ version of this result that
            scales with $\eta^3$.  Meanwhile, Corollary \ref{cor:entropic}'s
            large-$T$ analysis scales with $\eta^2$.  Our analysis integrates
            the noise over many updates, hence amplifying the contribution of
            $C$, and experiments verify this scaling law.
            %
            We do not make \cite{we19b}'s assumptions of thermal equilibrium,
            fast-slow mode separation, or constant covariance.  This generality
            reveals novel dynamics: that the velocity field above is
            generically non-conservative (Section \ref{subsect:entropic}).
      
        \subsubsection{Both flat and sharp minima overfit less} \label{subsect:curvature-and-overfitting}
            \begin{cor}[from $\sdia{(01-2)(02-12)}$, $\sdia{(01)(01)}$]\label{cor:overfit}
                Initialize GD at a test minimum.  The test-loss-increase and the
                gen.-gap (test minus train loss) due to training are,
                with errors $o(\eta^2)$ and $o(\eta^1)$:
                $$
                    \frac{C_{\mu\nu}}{2N} ~
                        \wrap{(I - \exp(-\eta T H))^{\otimes 2}}^{\mu\nu}_{\rho\lambda}
                        \wrap{H^{-1}}^{\rho\lambda}
                    ~~~~~ \text{and} ~~~~~
                    \frac{C_{\mu\nu}}{N} ~
                        \wrap{I - \exp(-\eta T H)}^{\nu}_{\lambda}
                        \wrap{H^{-1}}^{\lambda\mu}
                $$
                %In contrast to the later-mentioned Takeuchi estimate, this does not
                %diverge as $H$ shrinks.
            \end{cor}
            This gen.\ gap tends with large $T$ 
            to $C_{\mu\nu}(H^{-1})^{\mu\nu}/N$.  For maximum
            likelihood (ML) estimation in well-specified models near the ``true''
            minimum, $C=H$ is the Fisher metric, so we recover AIC:
            $(\textnormal{number of parameters})/N$.  Unlike AIC, our more general
            expression is descendably smooth, may be used with MAP or ELBO tasks
            instead of just ML, and does not assume a well-specified model.
    
            \begin{figure}[h!]
                \centering
                \plotmooh{diagrams/entropic-force-diagram}{}{0.32\columnwidth} 
                \plotmooh{diagrams/sharp}{}{0.31\columnwidth}
                \caption{
                    {\bf Novel phenomena.}
                    {\bf Left}:
                        Gradient noise induces a flow
                        toward minima \emph{with respect to to the
                        covariance}.  Our analysis assumes neither thermal
                        equilibrium nor fast-slow mode separation, but we label
                        ``fast and slow directions'' to ease comparison with
                        \cite{we19b}.  Red densities depict a typical locations
                        for $\theta$ in each cross-section of the valley,
                        and the spatial variation of curvature corresponds to
                        $J_{\mu\nu\lambda}$. 
                    {\bf Right}:
                        Noise structure determines how curvature affects
                        overfitting.  Geometrically, for a vector-perturbed
                        landscape, small Hessians are favored (top row), while
                        for covector-perturbed landscapes, large Hessians are
                        favored (bottom row).  Corollary \ref{cor:overfit}
                        shows how the implicit regularization of fixed-$\eta T$
                        descent interpolates between the two rows.
                }
                \label{fig:cubicandspring}
            \end{figure}
    
        \subsubsection{High-$C$ regions repel small-$E, B$ SGD}
            \label{subsect:epochs-batch}
    
            \begin{cor}[Epoch Number] \label{cor:epochs}
                To order $\eta^2$, $M=1$ SGD with learning rate $\eta$ has 
                $
                     \wrap{\frac{M-1}{M}}\wrap{\frac{B+1}{B}}\wrap{\frac{N}{2}}
                     \wrap{\nabla_\mu C^{\nu}_{\nu}} G^\mu / 2
                $
                less test loss than $M=M$ SGD with learning rate $\eta/M$.
            \end{cor}
        
            %Considering $\sdia{c(01-2)(01-12)}$:
            \begin{cor}[Batch Size] \label{cor:batch}
                The expected test loss of pure SGD is, to order $\eta^2$,
                less than that of pure GD by
                $
                      \frac{M(N-1)}{2} ~
                      \wrap{\nabla_\mu C^{\nu}_{\nu}} G^\mu / 2
                $.
                Moreover, if $\hat{C}$ is a smooth unbiased estimator of $C$, then
                GD on a modified loss 
                $
                    \tilde l_n = l_n +
                        \frac{N-1}{4N} ~
                        \hat{C}_\nu^\nu(\theta)
                $
                has an expected test loss that agrees with SGD's to second order.
                We call this method GDC.
            \end{cor}
    
        \subsubsection{Non-Gaussian noise affects SGD but not SDE}
    
            Stochastic Differential Equations (SDE: see \cite{li18}) are a popular
            theoretical approximation to SGD, but SDE and SGD differ in several
            ways.  For instance, the inter-epoch noise correlations in multi-epoch
            SGD measurably affect SGD's final test loss (Corollary
            \ref{cor:epochs}), but SDE assumes uncorrelated gradient updates.  Even
            if we restrict to single-epoch SDE, differences arise due to time
            discretization and non-gaussian noise. 
            %
            \begin{cor}[$\sdia{c(01-2)(02-12)}$, $\sdia{c(012-3)(03-13-23)}$] \label{cor:vsode}
                SGD's test loss is
                $
                    \frac{T}{2} ~ C_{\mu\nu} H^{\mu\nu} + o(\eta^2)
                $
                more than ODE's and SDE's.
                The deviation from SDE due to non-Gaussian noise is
                $
                    - (T/6) S_{\mu\nu\lambda} J^{\mu\nu\lambda} 
                    + o(\eta^3)
                $.\footnote{
                    This is Example \ref{exm:first}'s more exact
                    expression for $\eta \ll 1$:
                    they agree to leading order in $\eta$.
                }
            \end{cor}
            %
%==============================================================================
%    EXPERIMENTS AND APPLICATIONS
%==============================================================================

\section{Applying the Theory}

\subsection{Experiments}

    Our experiments' rejection of the null hypothesis is sometimes drastic.
    E.g., in Figure \ref{fig:vanilla}\ofsix{4}, \citep{ch18}
    predicts a velocity of $0$ while we predict a velocity of $\eta^2/6$.  
    %
    \texttt{I} bars and \texttt{+} signs to mark a $95\%$ confidence
    interval based on the standard error of the mean.  Appendix
    \ref{sect:landscape} lists architectures, procedure, and further tests.

    \subsubsection{Training time and batch size}
        %----------------------------------------------------------------------
        %       Vanilla SGD; Epochs and Overfitting         
        %----------------------------------------------------------------------
        We test Theorem \ref{thm:resum}'s order $\eta^3$ truncation on smooth
        convnets for CIFAR-10 and Fashion-MNIST.  Theory agrees with experiment
        through on timescales long enough for accuracy to increase by $0.5\%$
        (Figure \ref{fig:vanilla}\ofsix{0},\ofsix{1}).
        %----------------------------------------------------------------------
        %       Emulating Small Batches with Large Ones     
        %----------------------------------------------------------------------
        Figure \ref{fig:vanilla}\ofsix{2} tests Corollary \ref{cor:batch}'s
        claim that high-$C$ regions repel SGD more than GD.  This is
        significant because $C$ controls the rate at which the gen\. gap (test
        minus train loss) grows (Corollary \ref{cor:overfit}, Figure
        \ref{fig:vanilla}\ofsix{0}).
      
        \begin{figure}[h!] 
            \centering
            \pmoo{3.0cm}{new-test-0}        \pmoo{3.0cm}{new-big-bm-new}          \pmoo{3.0cm}{new-thermo-linear-screw}
            \pmoo{3.0cm}{rebut-test-1-T100} \pmoo{3.0cm}{rebut-gen-cifar-lenet-4} \pmoo{3.0cm}{new-tak}
            \caption{
                {\bf Left: Perturbation models SGD for small $\eta T$.}
                Fashion-MNIST convnet's test loss vs learning rate;
                un-re-summed predictions.
                %
                \protect\ofsix{0}: For all init.s tested ($1$ shown,
                $11$ unshown), the order $3$ prediction agrees with experiment
                through $\eta T \approx 10^0$, corresponding to a decrease
                in $0\mbox{-}1$ error of $\approx 10^{-3}$.
                %
                \protect\ofsix{1}: For large $\eta T$, our predictions
                break down.  Here, the order-$3$ prediction holds until the
                $0\mbox{-}1$ error improves by $5\cdot 10^{-3}$.
                %%%
                %%%
                \newline
                {\bf Center: $C$ controls gen.\ gap and distinguishes GD
                from SGD.}
                %
                With equal-scaled axes, \protect\ofsix{2} shows that
                GDC matches SGD (small vertical varianec) better than GD
                matches SGD (large horizontal variance) in test loss for a
                range of $\eta$ ($\approx 10^{-3}-10^{-1}$) and
                init.s\ (zero and several Xavier-Glorot trials) for
                logistic regression and convnets.  Here, $T=10$. 
                %
                \protect\ofsix{3}: CIFAR-10 generalization gaps.  For all
                init.s tested ($1$ shown, $11$ unshown), the
                degree-$2$ prediction agrees with experiment through $\eta T
                \approx 5\cdot 10^{-1}$.
                %%%
                %%%
                \newline
                {\bf Right: Predictions near minima excel even for large $\eta T$.}
                %
                \protect\ofsix{4}: On \Archimedes, SGD travels the valley of
                global minima in the positive $z$ direction.  $H$ and $C$
                are bounded, and the effect appears for all small $\eta$, so
                the effect is not a pathology of well-chosen learning rate or
                divergent noise. 
                %
                \protect\ofsix{5}: For \MeanEstimation\ with fixed $C$ and a
                range of $H$s, initialized at the truth, the test losses after
                fixed-$T$ optimization are smallest for very small and very
                large curvatures.  Both sharp and flat minima overfit less.
            }
            \label{fig:vanilla}
        %    \label{fig:batchandgen}
        %    \label{fig:thermoandtak}
        \end{figure}


    %--------------------------------------------------------------------------
    %           Thermodynamic Engine                        
    %--------------------------------------------------------------------------

    \subsubsection{Minima that are flat with respect to $C$ attract SGD} \label{subsect:entropic}
        To test Corollary \ref{cor:entropic}, we construct a counter-intuitive
        loss landscape wherein SGD steadily moves in a direction of $0$ test
        gradient.  Our mechanism differs from that of \cite{ch18}'s approximate
        analysis, which in this case predicts a velocity of $0$.%
        \footnote{
            Indeed, our velocity is $\eta$-perpendicular to the image of
            $(\eta C)^\mu_\nu$.
        }
        Note that for fixed $z$, $H$ is quadratic and $S$ is linear.  Also,
        since $x \sim \Nn(0,1)$, $x S(\theta)$ has expectation $0$.
        %
        \Archimedes\ thus has valley of global minima on the line $x=y=0$. 
        For SGD initialized at $\theta=0$, Corollary \ref{cor:entropic}
        predicts a $z$-velocity of $+\eta^2/6$ per timestep.  The prediction
        agrees with experiment even as the net displacement exceeds the 
        the landscape's natural length scale of $2\pi$
        (Figure \ref{fig:vanilla}\ofsix{4}).

    %--------------------------------------------------------------------------
    %           Sharp vs Flat Minima                        
    %--------------------------------------------------------------------------

    \subsubsection{Sharp and flat minima both overfit less than medium minima} \label{subsect:overfit}

        Prior work finds both that \emph{sharp} minima overfit less (for, $l^2$
        regularization sharpens minima) or that \emph{flat} minima overfit less
        (for, flat minima are robust to small displacements).  In fact,
        generalization's relationship to curvature depends on the landscape's
        noise structure (Corollary \ref{cor:overfit}, Figure
        \ref{fig:vanilla}\ofsix{5}).
        %
        To combat overfitting, we may add Corollary \ref{cor:overfit}'s
        expression for gen.\ gap to $l$.  Unlike AIC, which it subsumes, this
        regularizer is continous and thus liable to descent.  We call this
        regularizer \emph{STIC} ({\color{red} APPENDIX}).  By descending on
        STIC, we may tune smooth hyperparameters such as $l_2$ regularization
        coefficients in the noisy, small-$N$ regime $H \ll C/N$.  Since matrix
        exponentiation takes time cubic in dimension, exact STIC is most useful
        for small models.

%==============================================================================
%    CONCLUSION      
%==============================================================================

\subsection{Conclusion} \label{sect:concl}

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Summarize Contributions  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    We presented a diagram-based method for studying stochastic optimization on
    short timescales or near minima.
        Corollaries \ref{cor:entropic} and \ref{cor:overfit} together offer
        insight into SGD's success in training deep networks: SGD senses
        curvature, and curvature controls generalization.
    %
    Analyzing $\sdia{c(01-2)(02-12)}$, we proved that \textbf{flat and sharp
    minima both overfit} less than medium minima.  Intuitively, flat minima are
    robust to vector noise, sharp minima are robust to covector noise, and
    medium minima robust to neither.  We thus propose a
    a smooth analogue of AIC enabling gradient-based hyperparameter tuning.
    %
    Inspecting $\sdia{c(01-2-3)(02-12-23)}$, we extended \cite{we19b} to
    nonconstant, nonisotropic covariance to reveal that \textbf{SGD descends on
    a landscape smoothed by the current covariance $C$}.  As $C$ evolves, the
    smoothed landscape evolves, resulting in non-conservative dynamics.
    %
    Examining $\sdia{c(01-2)(01-12)}$, we showed that \textbf{GD may emulate
    SGD}, as conjectured by \cite{ro18}.  This is significant because, while
    small batch sizes can lead to better generalization \citep{bo91}, modern
    infrastructure increasingly rewards large batch sizes \citep{go18}.  

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Anticipate Criticism of Limitations  ~~~~~~~~~~~~~~~~~~~~~~~~~~

    %\subsection{Consequences}

        Since our predictions depend only on loss data near initialization,
        they break down after the weight moves far from initialization.  Our
        theory thus best applies to small-movement contexts, whether for long
        times (large $\eta T$) near an isolated minimum or for short times
        (small $\eta T$) in general.  E.g., our theory might especially
        illuminate meta-learners that are based on fine-tuning (e.g.\
        \cite{fi17}'s MAML).

        Much as meteorologists understand how warm and cold fronts interact
        despite long-term forecasting's intractability, we quantify the
        counter-intuitive dynamics governing each short-term interval of SGD's
        trajectory.  Equipped with our theory, practitioners may now
        refine intuitions (e.g.\ that SGD descends on the train loss) to
        account for noise.
       
%==============================================================================
%    CODA               
%==============================================================================

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Broader Impacts  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\section*{Broader Impacts}

    Though machine learning has the long-term potential for vast improvements
    in world-wide quality of life, it is today a source of enormous carbon
    emissions \citep{st19}.  Our analysis of SGD may lead to a reduced carbon
    footprint in three ways. 
     
    First, Section \ref{subsect:epochs-batch} shows how to modify the loss
    landscape so that large-batch GD enjoys the stochastic regularizing
    properties of small-batch SGD, or (symmetrically) so that small-batch SGD
    enjoys the stability of large-batch GD.  By unchaining the effective batch
    size from the actual batch size, we raise the possibility of training
    neural networks on a wider range of hardware than currently practical.  For
    example, asynchronous concurrent small-batch SGD (e.g., \cite{ni11}) might
    require less inter-GPU communication and therefore less power.
     
    Second, Section \ref{sect:concl} discusses an application to meta-learning,
    which has the potential to decrease the per-task sample complexity and
    hence carbon footprint of modern machine learning.
     
    Third, the generalization of AIC developed in  Sections
    \ref{subsect:curvature-and-overfitting} and \ref{subsect:overfit} permits
    certain forms of model selection by gradient descent rather than brute
    force search.  This might drastically reduce the energy consumed during
    model selection.

    That said, insofar as our theory furthers practice, it may instead
    contribute to the rapidly growing popularity of GPU-intensive learning,
    thus negating the aforementioned benefits and accelerating climate change.

    More broadly, this paper analyzes optimization in the face of uncertainty.
    As ML systems deployed today must increasingly address \emph{user privacy},
    \emph{pedestrian safety}, and \emph{dataset diversity}, it becomes
    important to recognize that training sets and test sets
    differ.  Toward this end, theoretical work relating to non-Gaussian noise
    may assist practitioners in building provably non-discriminatory, safe, or
    private models (e.g., \cite{dw06}).  By quantifying how correlated,
    non-Gaussian gradient noise affects descent-based learning, this paper
    contributes to such broader theory.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Acknowledgements  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\section*{Acknowledgements}

    %\begin{ack}
        We feel deep gratitude to
            \textsc{Sho Yaida},
            \textsc{Dan A. Roberts}, and
            \textsc{Josh Tenenbaum}
        for posing some of the problems this work resolves and for their
        patient guidance.  We appreciate the generosity of
            \textsc{Andy Banburski},
            \textsc{Ben R. Bray},
            \textsc{Jeff Lagarias}, and
            \textsc{Wenli Zhao}
        in critiquing our drafts.
        Without the encouragement of
            \textsc{Jason Corso},
            \textsc{Chloe Kleinfeldt},
            \textsc{Alex Lew}, 
            \textsc{Ari Morcos}, and
            \textsc{David Schwab},
        this paper would not be.
        Finally, we thank our anonymous reviewers for inspiring an improved
        presentation.
        %
        This work was funded in part by MIT's Jacobs Presidential Fellowship
        and in part by Facebook AI Research.
    %\end{ack}
        
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  References  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%\section*{References}
    \bibliographystyle{plainnat}
    \bibliography{perturb}

%==============================================================================
%    APPENDICES      
%==============================================================================

\clearpage
\newpage
\renewcommand{\thesection}{\Alph{section}}
\setcounter{section}{0}

\section*{Organization of Appendices}
    These three appendices respectively serve three functions:
    \begin{itemize}
        \item to explain how to calculate using diagrams;
        \item to precisely state and prove our results, then pose a conjecture;
        \item to specify our experimental methods and results.
    \end{itemize}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Tutorial  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\section{How to Calculate Expected Test Losses}
    Our work introduces a novel technique for calculating the expected learning
    curves of SGD in terms of statistics of the loss landscape near
    initialization.  Here, we explain this technique.  There are {\bf four
    steps} to computing the expected test loss after a specific number of
    gradient updates: 
    \begin{itemize}
        \item Based on the chosen optimization hyperparameters (namely, batch
              size, training set size, and number of epochs):
              {\bf draw the spacetime grid} that encodes these hyperparameters.
        \item Based on our desired level of precision,
              {\bf draw all the relevant embeddings} of diagrams into the
              spacetime.
        \item {\bf Evaluate each diagram embedding}.
        \item {\bf Sum the embeddings' values} to obtain the quantity of
              interest as a function of the learning rate.
    \end{itemize}

    After presenting a small, complete example calculation that follows these
    four steps, we explain how to perform each of these steps in its own
    sub-section.  We then discuss how diagrams often offer intuition as well as
    calculational help.  Though we focus on the computation of expected test
    losses, we explain how a small change in the above four steps allows for
    the computation also of variances (instead of expectations) and of train
    losses (instead of test losses).  We conclude by comparing direct
    calculation based on our Key Lemma to the diagram method; we point out when
    and why diagrams streamline computation.

    \subsection{An example calculation}                             \label{appendix:example}
    \subsection{How to identify the relevant space-time}            \label{appendix:draw-spacetime}
    \subsection{How to identify the relevant diagram embeddings}    \label{appendix:draw-embeddings}
    \subsection{How to evaluate each embedding}                     \label{appendix:evaluate-embeddings}
    \subsection{How to sum the embeddings' values}                  \label{appendix:sum-embeddings}
    \subsection{Interpreting diagrams to build intuition}           \label{appendix:interpret-diagrams}
    \subsection{How to solve variant problems}                      \label{appendix:solve-variants}
    \subsection{Do diagrams streamline computation?}                \label{appendix:diagrams-streamline}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Math  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\section{Assumptions, Proofs, and Future Topics}
    \subsection{Setup and assumptions}                              \label{appendix:assumptions}
    \subsection{Statements of results}                              \label{appendix:statements}  
    \subsection{Proof of the Dyson Lemma}                           \label{appendix:key-lemma}
    \subsection{From Dyson to diagrams}                             \label{appendix:toward-diagrams}
    \subsection{On M\"obius inversion}                              \label{appendix:mobius}
    \subsection{Proof of Theorems}                                  \label{appendix:re-summation}
    \subsection{Proofs of Corollaries}                              \label{appendix:corollaries}
    \subsection{Proofs of miscellaneous claims}                     \label{appendix:claims}
    \subsection{Future Topics}                                      \label{appendix:future}

        The diagram method opens the door to exploration of Lagrangian
        formalisms and curved backgrounds\footnote{
            \cite{la60, la51} introduce these concepts.
        }:
        \begin{quest}
            Does some least-action principle govern SGD; if not, what is an
            essential obstacle to this characterization?
        \end{quest}
        Lagrange's least-action formalism intimately intertwines with the
        diagrams of physics.  Together, they afford a modular framework for
        introducing new interactions as new terms or diagram nodes.  In fact,
        we find that some \emph{higher-order} methods --- such as the
        Hessian-based update
        $
            \theta \leftsquigarrow
            \theta -
            (\eta^{-1} + \lambda \nabla \nabla l_t(\theta))^{-1}
            \nabla l_t(\theta)
        $
        parameterized by small $\eta, \lambda$ --- admit diagrammatic analysis
        when we represent the $\lambda$ term as a second type of diagram node.
        Though diagrams suffice for computation, it is Lagrangians that most
        deeply illuminate scaling and conservation laws.
        \begin{conj}[Riemann Curvature Regularizes]
            For small $\eta$, SGD's gen. gap decreases as sectional curvature
            grows.
        \end{conj}
        Though our work assumes a flat metric $\eta^{\mu\nu}$, it might
        generalize to curved weight spaces\footnote{
            One may represent the affine connection as a node, thus giving
            rise to non-tensorial and hence gauge-dependent diagrams.
        }.
        Curvature finds concrete application in the \emph{learning on
        manifolds} paradigm of \cite{ab07, zh16}, notably specialized to
        \cite{am98}'s \emph{natural gradient descent} and \cite{ni17}'s
        \emph{hyperbolic embeddings}.  We are optimistic our formalism may
        resolve conjectures such as above.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Experiments  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\section{Exerimental Methods and Results}

    \subsection{What artificial landscapes did we use?}             \label{appendix:artificial}

        We define three artificial landscapes, evocatively called
        \Gauss, \Archimedes, and \MeanEstimation.

        \subsubsection*{\Gauss}
            The \Gauss\, landscape is a simple case of fitting a gaussian to
            data.  In particular, it is a probability distribution $\Dd$ over
            functions $l_x:\RR^1\to \RR$ on $1$-dimensional weight space,
            indexed by standard-normally distributed $1$-dimensional datapoints
            $x$ and defined by the expression:
            $$
                l_x(h)
                \triangleq
                \frac{1}{2} \wrap{h + x^2 \exp(-h)}
            $$
            To measure overfitting, we initialize at the true test minimum
            $h=0$, then train and see how much the test loss increases.
            
        \subsubsection*{\Archimedes}
            The \Archimedes\, landscape has chirality, much like the ancient
            screw of Archimedes.  
            Specifically, the \Archimedes\ landscape has
            %
            weights     $\theta = (u,v,z) \in \RR^3$,
            %
            data points $x \sim \Nn(0, 1)$,
            %
            and loss:
            %
            $$
                l_x(w)
                \triangleq
                \frac{1}{2} H(\theta) + x \cdot S(\theta)
            $$
            %
            Here, $$H(\theta) = u^2 + v^2 + (\cos(z) u + \sin(z) v)^2$$
            %
            and   $$S(\theta) = \cos(z-\pi/4) u + \sin(z-\pi/4) v$$
            We note that the landscape 
            has a three-dimensional continuous screw symmetry consisting of
            translation along $z$ and simulteous rotation in $u-v$.
            Our experiments initialize at $x=y=z=0$, which lies within a valley
            of global minima defined by $x=y=0$.  
            
        \subsubsection*{\MeanEstimation}
            The \MeanEstimation\, family of landscapes has $1$ dimensional
            weights $1$-dimensional datapoints $x$ and defined by the
            expression:
            $$
                l_x(w)
                \triangleq
                \frac{1}{2} H w^2 + x S w
            $$
            Here, $H, S$ are positive reals parameterizing the family; they
            give the hessian and (square root of) gradient covariance,
            respectively.

            For our hyperparameter-selection experiment {\color{red} FIGURE},
            we introduce an $l_2$ term $\lambda$ as follows:
            $$
                l_x(w, \lambda)
                \triangleq
                \frac{1}{2} (H + \lambda) w^2 + x S w
            $$
            Here, we constrain $\lambda\geq 0$ during optimization using
            projections; we found similar results when parameterizing
            $\lambda = \exp(h)$, which obviates the need for projection but
            necessitates a non-canonical choice of initialization.  We
            initialize $\lambda=0$.

    \subsection{What image-classification landscapes did we use?}   \label{appendix:natural}

        \subsubsection*{Architectures}
            In addition to the artificial loss landscapes
            \Gauss, \Archimedes, and \MeanEstimation, 
            we tested our predictions on logistic linear regression
            and simple convolutional networks (2 convolutional weight layers
            each with kernel $5$, stride $2$, and $10$ channels, followed by
            two dense weight layers with hidden dimension $10$) for the
            CIFAR-10 \cite{kr09} and Fashion-MNIST datasets \cite{xi17}.  The
            convolutional architectures used $\tanh$ activations and Gaussian
            Xavier initialization.  To set a standard distance scale on weight
            space, we parameterized the model so that the
            Gaussian-Xavier initialization of the linear maps in each layer
            differentially pulls back to standard normal initializations of the
            parameters.
            
        \subsubsection*{Datasets}
            For image classification landscapes, we regard the finite amount of
            available data as the true (sum of diracs) distribution $\Dd$ from
            which we sample test and training sets in i.i.d.\ manner (and hence
            ``with replacement'').  We do this to gain practical access to a
            ground truth against which we may compare our predictions.  One
            might object that this sampling procedure would cause test and
            training sets to overlap, hence biasing test loss measurements.  In
            fact, test and training sets overlap only in reference, not in
            sense: the situation is analogous to a text prediction task in
            which two training points culled from different corpora happen to
            record the same sequence of words, say, ``Thank you!''.  In any
            case, all of our experiments focus on the limited-data regime, e.g.
            $10^1$ datapoints out of $\sim 10^{4.5}$ dirac masses, so overlaps
            are rare.

    \subsection{Measurement process}                                \label{appendix:measure}

        \subsubsection*{Diagram evaluation on real landscapes}
            We implemented the formulae of Appendix \ref{sect:bessel} in order
            to estimate diagram values from real data measured at
            initialization from batch averages of products of derivatives.

        \subsubsection*{Descent simulations}
            We recorded test and train losses for each of the trials below.  To
            improve our estimation of average differences, when we compared two
            optimizers, we gave them the same random seed (and hence the same
            training sets).

            We ran $2 \cdot 10^5$ trials of \Gauss\, with SDE and SGD,
            initialized at the test minimum with $T=1$ and $\eta$ ranging from
            $5\cdot 10^{-2}$ to $2.5\cdot 10^{-1}$.
            We ran $5 \cdot 10^1$ trials of \Archimedes with SGD with $T=10^4$
            and $\eta$ ranging from $10^{-2}$ to $10^{-1}$.
            We ran $10^3$ trials of \MeanEstimation with GD and STIC
            with $T=10^2$, $H$ ranging from $10^{-4}$ to $4 \cdot 10^0$,
            a covariance of gradients of $10^2$, and the true mean $0$ or
            $10$ units away from initialization.

            We ran $5 \cdot 10^4$ trials of the CIFAR-10 convnet on each of $6$
            Glorot-Xavier initializations we fixed once and for all through
            these experiments for the optimizers SGD, GD, and GDC, with $T=10$
            and $\eta$ between $10^{-3}$ and $2.5 \cdot 10^{-2}$.  We did
            likewise for the linear logistic model on the one initialization of
            $0$.

            We ran $4 \cdot 10^4$ trials of the Fashion-MNIST convnet on each
            of $6$ Glorot-Xavier initializations we fixed once and for all
            through these experiments for the optimizers SGD, GD, and GDC with
            $T=10$ and $\eta$ between $10^{-3}$ and $2.5 \cdot 10^{-2}$.  We
            did likewise for the linear logistic model on the one
            initialization of $0$. 

    \subsection{Implementing optimizers}                            \label{appendix:optimizers}
        We approximated SDE by refining time discretization by a factor of
        $16$, scaling learning rate down by a factor of $16$, and introducing
        additional noise in the shape of the covariance in proportion as
        prescribed by the Wiener process scaling.

        Our GDC regularizer was implemented using the unbiased estimator
        $\hat{C} \triangleq (l_x - l_y)_\mu {l_x}_\nu / 2$.
        
        For our tests of regularization based on Corollary \ref{cor:overfit},
        we exploited the low-dimensional special structure of the artificial
        landscape in order to avoid diagonalizing to perform the matrix
        exponentiation: precisely, we used that, even on training landscapes,
        the covariance of gradients would be degenerate in all but one
        direction, and so we need only exponentiate a scalar.

    \subsection{Software frameworks and hardware}                   \label{appendix:frameworks}

        All code and data-wrangling scripts can be found on
        {\color{mooteal}github.com/???????/perturb}.  This link will be made
        available after the period of double-blind review.

        Our code uses PyTorch 0.4.0 \cite{pa19} on Python 3.6.7; there are no
        other substantive dependencies.  The code's randomness is parameterized
        by random seeds and hence reproducible.

        We ran experiments on a Lenovo laptop and on our institution's
        clusters; we consumed about $100$ GPU-hours.

    \subsection{Unbiased estimators of landscape statistics}        \label{appendix:bessel-factors}
        %
        We use the following method, well known to some of our colleagues but
        hard to find writings on, to obtain unbiased estimates for various
        statistics of the loss landscape.  The method is merely an elaboration
        of Bessel's factor \citep{ga23}.  For completeness, we explain it here. 
        
        Given samples from a joint probability space $\prod_{0\leq d<D} X_d$,
        we seek unbiased estimates of multipoint correlators (i.e. products of
        expectations of products) such as $\wang{x_0 x_1 x_2}\wang{x_3}$.  For
        example, say $D=2$ and from $2S$ samples we'd like to estimate
        $\wang{x_0 x_1}$.  Most simply, we could use $\Avg_{0\leq s<2S}
        x_0^{(s)} x_1^{(s)}$, where $\Avg$ denotes averaging.  In fact, the
        following also works:
        %
        \begin{equation} \label{eq:bessel}
            S
            \wrap{\Avg_{0\leq s< S} x_0^{(s)}}
            \wrap{\Avg_{0\leq s< S} x_1^{(s)}}
            +
            (1-S)
            \wrap{\Avg_{0\leq s< S} x_0^{(s)}}
            \wrap{\Avg_{S\leq s<2S} x_1^{(s)}}
        \end{equation}
        %
        When multiplication is expensive (e.g. when each $x_d^{(s)}$ is a
        tensor and multiplication is tensor contraction), we prefer the latter,
        since it uses $O(1)$ rather than $O(S)$ multiplications.  This in turn
        allows more efficient use of large-batch computations on GPUs.  We now
        generalize this estimator to higher-point correlators (and $D\cdot S$
        samples).

        For uniform notation, we assume without loss that each of the $D$
        factors appears exactly once in the multipoint expression of interest;
        such expressions then correspond to partitions on $D$ elements, which
        we represent as maps $\mu:\wasq{D}\to \wasq{D}$ with $\mu(d)\leq d$ and
        $\mu\circ \mu=\mu$.  Note that $\wabs{\mu} \coloneqq \wabs{im(\mu)}$
        counts $\mu$'s parts.  We then define the statistic
        %
        $$
            \wurl{x}_\mu
            \triangleq
            \prod_{0\leq d<D} \Avg_{0\leq s<S} x_d^{(\mu(d)\cdot S + s)}
        $$
        %
        and the correlator $\wang{x}_\mu$ we define to be the expectation of 
        $\wurl{x}_\mu$ when $S=1$.  In this notation, \ref{eq:bessel} says: 
        $$
            \wang{x}_{\partitionbox{0}\partitionbox{1}}
            =
            \expct{
                S       \cdot \wurl{x}_{\partitionbox{0 1}} +
                (1-S)   \cdot \wurl{x}_{\partitionbox{0}\partitionbox{1}}
            }
        $$
        %
        Here, the boxes indicate partitions of $\wasq{D}=\wasq{2}=\{0,1\}$.
        Now, for general $\mu$, we have:
        %
        \begin{equation} \label{eq:newbessel}
            \expct{S^D \wurl{x}_\mu}
            =
            \sum_{\tau\leq \mu} \wrap{
                \prod_{0\leq d<D}
                    \frac{S!}{\wrap{S-\wabs{\tau(\mu^{-1}(d))}}!}
            }
            \wang{x}_\tau
        \end{equation}
        %
        where `$\tau \leq \mu$' ranges through partitions \emph{finer} than 
        $\mu$, i.e. maps $\tau$ through which $\mu$ factors.   
        In smaller steps, \ref{eq:newbessel} holds because
        %
        \begin{align*}
            \expct{S^D \wurl{x}_\mu}
            &=
            \expct{
                \sum_{(0\leq s_d<S) \in \wasq{S}^D}
                \prod_{0\leq d<D}
                x_d^{\wrap{\mu(d)\cdot S + s_d}}
            }\\
            &=
            \sum_{\substack{(0\leq s_d<S) \\ \in \wasq{S}^D}}
            \expct{
                \prod_{0\leq d<D}
                x_d^{\wrap{\min \wurl{
                    \tilde{d}~:~\mu(\tilde{d})\cdot S+s_{\tilde{d}} = \mu(d)\cdot S+s_d
                }}}
            }\\
            &=
            \sum_{\tau} \wabs{\wurl{\substack{
                (0\leq s_d<S)~\in~[S]^D~: \\
                \wrap{\substack{
                    \mu(d)=\mu(\tilde{d}) \\
                    \wedge~s_d=s_{\tilde{d}}
                }}
                \Leftrightarrow
                \tau(d)=\tau(\tilde{d})
            }}}
            \wang{x}_\tau \\
            &=
            \sum_{\tau\leq \mu} \wrap{
                \prod_{0\leq d<D}
                    \frac{S!}{\wrap{S-\wabs{\tau(\mu^{-1}(d))}}!}
            }
            \wang{x}_\tau
        \end{align*}

        Solving \ref{eq:newbessel} for $\wang{x}_\mu$, we find:
        %
        \begin{equation*}
            \text{\fbox{$
            \wang{x}_\mu
            =
            \frac{S^D}{S^{\wabs{\mu}}}
            \expct{
                \wurl{x}_\mu
            }
            -
            \sum_{\tau < \mu} \wrap{
                \prod_{d\in im(\mu)}
                \frac{\wrap{S-1}!}{\wrap{S-\wabs{\tau(\mu^{-1}(d))}}!}
            }
            \wang{x}_\tau
            $}}
        \end{equation*}
        %
        This expresses $\wang{x}_\mu$ in terms of the batch-friendly estimator
        $\wurl{x}_\mu$ as well as correlators $\wang{x}_\tau$ for $\tau$
        \emph{strictly} finer than $\mu$.  We may thus (use dynamic programming
        to) obtain unbiased estimators $\wang{x}_\mu$ for all partitions $\mu$.
        Symmetries of the joint distribution and of the multilinear
        multiplication may further streamline estimation by turning a sum over
        $\tau$ into a multiplication by a combinatorial factor.  For example,
        in the case of complete symmetry:
        %
        $$
            \wang{x}_{\partitionbox{012}}
            =
            S^2
            \wurl{x}_{\partitionbox{012}}
            -
            \frac{(S-1)!}{(S-3)!}
            \wurl{x}_{\partitionbox{0}\partitionbox{1}\partitionbox{2}}
            -
            3\frac{(S-1)!}{(S-2)!}
            \wurl{x}_{\partitionbox{0}\partitionbox{12}}
        $$

    \subsection{Additional figures}                                 \label{appendix:figures}
        \begin{figure}[H] 
            \centering
            \pmoo{3.0cm}{multi-fashion-logistic-0}
            \pmoo{3.0cm}{vs-sde}
            \pmoo{3.0cm}{tak-reg}
            \caption{
                \textbf{Further experimental results}.
                %
                \textbf{Left}: SGD with $2, 3, 5, 8$ epochs incurs greater test
                loss than one-epoch SGD (difference shown in I bars) by the
                predicted amounts (predictions shaded) for a range of learning
                rates.  Here, all SGD runs have $N=10$; we scale the learning
                rate for $E$-epoch SGD by $1/E$ to isolate the effect of
                inter-epoch correlations away from the effect of larger $\eta
                T$.
                %
                \textbf{Center}: SGD's difference from SDE after $\eta T
                \approx 10^{-1}$ with maximal coarseness on the Gaussian-fit
                problem.  Two effects not modeled by SDE ---
                time-discretization and non-Gaussian noise oppose on this
                landscape but do not completely cancel.  Our theory
                approximates the above curve with a correct sign and order of
                magnitude; we expect that the fourth order corrections would
                improve it further.
                %
                \textbf{Right}: Blue intervals regularization using Corollary
                \ref{cor:overfit}.  When the blue intervals fall below the
                black bar, this proposed method outperforms plain GD.  For
                \MeanEstimation with fixed $C$ and a range of $H$s, initialized
                a fixed distance \emph{away} from the true minimum, descent on
                an $l_2$ penalty coefficient $\lambda$ improves on plain GD for
                most Hessians.  The new method does not always outperform GD,
                because $\lambda$ is not perfectly tuned according to STIC but
                instead descended on for finite $\eta T$.
            }
            \label{fig:takreg}
        \end{figure}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  History of SGD  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\section{History of SGD}
    We were surprised to learn of gradient descent's long history:

    It was \cite{ki52} who, in uniting gradient descent \citep{ca47} with
    stochastic approximation \citep{ro51}, invented SGD.  Since the development
    of back-propagation for efficient differentiation \citep{we74}, SGD has
    been used to train connectionist models including neural networks
    \citep{bo91}, in recent years to remarkable success \citep{le15}.

\end{document}
