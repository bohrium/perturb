%   author: samtenka
%   change: 2020-06-02
%   create: 2020-05-29
%   descrp: LaTeX source for perturb project
%   to use: compile along with perturb.bib and diagram and plot directories

%==============================================================================
%=====  LATEX PREAMBLE  =======================================================
%==============================================================================

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Document Styling  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}      

%\usepackage[nonatbib]{neurips_2020}
\usepackage{natbib}
%\usepackage[final]{neurips_2020}

%---------------------  mathematics  ------------------------------------------

\usepackage{amsmath, amssymb, amsthm, amsfonts}
\usepackage{mathtools, nicefrac, xstring}

\usepackage{setspace}
%---------------------  tables  ----------------------------------------------- 

\usepackage{booktabs}
\usepackage{array}
\newcolumntype{L}{>{$}l<{$}}

%---------------------  graphics and figures  ---------------------------------

\usepackage{graphicx}
\usepackage{wrapfig, float, subfigure}
\usepackage{hanging, txfonts, ifthen}

\newcommand{\ofsix}[1]{
    {\tiny \raisebox{0.04cm}{$\substack{
        \ifthenelse{\equal{#1}{0}}{{\color{moor}\blacksquare}}{\square}
        \ifthenelse{\equal{#1}{2}}{{\color{moor}\blacksquare}}{\square}    
        \ifthenelse{\equal{#1}{4}}{{\color{moor}\blacksquare}}{\square} \\
        \ifthenelse{\equal{#1}{1}}{{\color{moor}\blacksquare}}{\square}    
        \ifthenelse{\equal{#1}{3}}{{\color{moor}\blacksquare}}{\square}
        \ifthenelse{\equal{#1}{5}}{{\color{moor}\blacksquare}}{\square}
    }$}}%
}

\newcommand{\ofthreemoo}[1]{
    {\tiny
        \raisebox{-0.025cm}{\color{gray}\scalebox{2.5}{$\substack{
            \ifthenelse{\equal{#1}{0}}{{\color{moor}\blacksquare}}{\square} 
        }$}}%
        \raisebox{0.04cm}{$\substack{
            \IfSubStr{#1}{1}{{\color{moor}\blacksquare}}{\square}   
            \IfSubStr{#1}{1}{{\color{moor}\blacksquare}}{\square} \\
            \IfSubStr{#1}{2}{{\color{moor}\blacksquare}}{\square}    
            \IfSubStr{#1}{2}{{\color{moor}\blacksquare}}{\square}    
        }$}%
    }%
}

%---------------------  colors  -----------------------------------------------

\usepackage{xcolor, framed}
\definecolor{moolime}{rgb}{0.90,1.00,0.90}
\definecolor{moosky}{rgb}{0.90,0.90,1.00}
\definecolor{moopink}{rgb}{1.00,0.90,0.90}
\definecolor{moor}{rgb}{0.8,0.2,0.2}
\definecolor{moog}{rgb}{0.2,0.8,0.2}
\definecolor{moob}{rgb}{0.2,0.2,0.8}
\definecolor{mooteal}{rgb}{0.1,0.6,0.4}

%---------------------  intertext: footnotes and hyperlinks  ------------------ 

\usepackage[perpage]{footmisc}
\renewcommand*{\thefootnote}{
    \color{red}
    \arabic{footnote}
    %\fnsymbol{footnote}
} 

\usepackage{hyperref}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Theorem Environments  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  mathematical results  ---------------------------------

\theoremstyle{plain}
    \newtheorem*{klem*}{Key Lemma}
    \newtheorem{thm}{Theorem}
    \newtheorem*{thm*}{Theorem}
    \newtheorem{cor}{Corollary}
    \newtheorem{prop}{Proposition}

%---------------------  mathematical questions  -------------------------------

    \newtheorem{conj}{Conjecture}
    \newtheorem{quest}{Question}
    \newtheorem*{quest*}{Question}
    \newtheorem*{quests*}{Questions}

%---------------------  definitions, answers, remarks  ------------------------

\theoremstyle{definition}
    \newtheorem{defn}{Definition}
    \newtheorem*{answ*}{Answer}
    \newtheorem{rmk}{Remark}
    \newtheorem*{midea*}{Main Idea}
    \newtheorem*{rmk*}{Remark}
    \newtheorem{exm}{Example}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Custom Math Commands  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  expanding containers  ---------------------------------

\newcommand{\wrap}[1]{\left(#1\right)}
\newcommand{\wasq}[1]{\left[#1\right]}
\newcommand{\wang}[1]{\left\langle#1\right\rangle}
\newcommand{\wive}[1]{\left\llbracket#1\right\rrbracket}
\newcommand{\worm}[1]{\left\|#1\right\|}
\newcommand{\wabs}[1]{\left|#1\right|}
\newcommand{\wurl}[1]{\left\{#1\right\}}

\newcommand{\partitionbox}[1]{
    \text{
        \fboxsep=0.5pt
        \tiny
        \fbox{#1}
    }
}

%---------------------  special named objects  --------------------------------

\newcommand{\Free}{\mathcal{F}}
\newcommand{\Forg}{\mathcal{G}}
\newcommand{\Mod}{\mathcal{M}}
\newcommand{\Hom}{\text{\textnormal{Hom}}}
\newcommand{\Aut}{\text{\textnormal{Aut}}}
\newcommand{\image}{\text{\textnormal{im}}}
\newcommand{\uvalue}{\text{\textnormal{uvalue}}}
\newcommand{\rvalue}{\text{\textnormal{rvalue}}}
\newcommand{\edges}{\text{\textnormal{edges}}}
\newcommand{\ords}{\text{\textnormal{ords}}}
\newcommand{\parts}{\text{\textnormal{parts}}}
\newcommand{\SGD}{\text{\textnormal{SGD}}}
\DeclareMathOperator*{\Avg}{\text{\sffamily A}}
\newcommand{\expc}{\mathbb{E}}
\newcommand{\expct}[1]{\mathbb{E}\left[#1\right]}

%---------------------  fancy letters  ----------------------------------------

\newcommand{\Aa}{\mathcal{A}}
\newcommand{\Bb}{\mathcal{B}}
\newcommand{\Cc}{\mathcal{C}}   \newcommand{\CC}{\mathbb{C}}
\newcommand{\Dd}{\mathcal{D}}
\newcommand{\Ee}{\mathcal{E}}
\newcommand{\Ff}{\mathcal{F}}
\newcommand{\Gg}{\mathcal{G}}
\newcommand{\Hh}{\mathcal{H}}
\newcommand{\Ll}{\mathcal{L}}
\newcommand{\Mm}{\mathcal{M}}
\newcommand{\Nn}{\mathcal{N}}   \newcommand{\NN}{\mathbb{N}}
\newcommand{\Oo}{\mathcal{O}}
\newcommand{\Pp}{\mathcal{P}}
\newcommand{\Qq}{\mathcal{Q}}   \newcommand{\QQ}{\mathbb{Q}}
\newcommand{\Rr}{\mathcal{R}}   \newcommand{\RR}{\mathbb{R}}
\newcommand{\Ss}{\mathcal{S}}
\newcommand{\Tt}{\mathcal{T}}
\newcommand{\Uu}{\mathcal{U}}
\newcommand{\Vv}{\mathcal{V}}
\newcommand{\Ww}{\mathcal{W}}
\newcommand{\Xx}{\mathcal{X}}
\newcommand{\Yy}{\mathcal{Y}}
\newcommand{\Zz}{\mathcal{Z}}   \newcommand{\ZZ}{\mathbb{Z}}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Pictures  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  pictures with specified width or height  --------------

\newcommand{\plotmoow}[3]{\includegraphics[width=#2          ]{../#1}}
\newcommand{\plotmooh}[3]{\includegraphics[         height=#3]{../#1}}
\newcommand{\pmoo}[2]{\includegraphics[height=#1]{../plots/#2}}

%---------------------  inline diagrams of various sizes  ---------------------

\newcommand{\sizeddia}[2]{
    \begin{gathered}
        \includegraphics[scale=#2]{../diagrams/#1.png}
    \end{gathered}
}
\newcommand{\bdia}[1]{\protect \sizeddia{#1}{0.22}}
\newcommand{\dia} [1]{\protect \sizeddia{#1}{0.18}}
\newcommand{\mdia}[1]{\protect \sizeddia{#1}{0.14}}
\newcommand{\sdia}[1]{\protect \sizeddia{#1}{0.10}}

\newcommand{\mend}{\hfill $\Diamond$}

\newcommand{\Gauss}{\textsc{Gauss}}
\newcommand{\Archimedes}{\textsc{Archimedes}}
\newcommand{\MeanEstimation}{\textsc{Mean Estimation}}

%==============================================================================
%=====  FRONT MATTER  =========================================================
%==============================================================================

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Title and Author  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\title{%
    A Perturbative Analysis of Stochastic Descent
}

\author{%
    \textbf{Samuel C.~Tenka} \\
    Computer Science and AI Lab \\
    Massachusetts Institute of Technology \\
    Cambridge, MA 02139 \\
    \texttt{coli{\tiny@}mit.edu}
}

\begin{document}

    \maketitle

    
    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Abstract  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    
    \begin{abstract}
        %
        %-------------  hammer and general nail  ------------------------------
        %
        We analyze stochastic gradient descent (SGD) at small learning rates.
        Unlike prior analyses based on stochastic differential equations, our
        theory models discrete time and hence non-Gaussian noise.
        %
        %-------------  applications  -----------------------------------------
        %
        We prove that gradient noise systematically pushes SGD toward flatter
        minima.  We characterize when and why flat minima overfit less than
        other minima.  We generalize the Akaike information criterion (AIC) to
        a smooth estimator of overfitting, hence enabling gradient-based model
        selection.  We show how non-stochastic GD with a modified loss function
        may emulate SGD.
        %
        %-------------  mention of experiments  -------------------------------
        %
        We verify our predictions on convnets for CIFAR-10 and Fashion-MNIST.
    \end{abstract}
    
%==============================================================================
%=====  INTRODUCTION  =========================================================
%==============================================================================

\section{Introduction}

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Orienting Invitation  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    %-----------------  object of study  --------------------------------------

    Practitioners benefit from the intuition that stochastic gradient descent
    (SGD) approximates noiseless gradient descent (GD) \citep{bo91}.  In this
    paper, we refine that intuition by showing how gradient noise biases
    learning toward certain areas of weight space.
    %
    %-----------------  vs ODE and SDE  ---------------------------------------
    %
    Departing from prior work, we model discrete time and hence non-Gaussian
    noise.  Indeed, we derive corrections to continuous-time, Gaussian-noise
    approximations such as ordinary and stochastic differential equations (ODE,
    SDE).
    For example, we construct a loss landscape on which SGD eternally cycles
    counterclockwise, a phenomenon impossible with ODEs. 
    %
    %-----------------  organization Plan  ------------------------------------
    %
    Leaving the rigorous development of our general theory to
    \S\ref{appendix:math}, our paper body highlights our theory's intuition and
    main corollaries.

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Soft Benefits: Physical Intuition and Further Applications  ~~~

    %-----------------  retrospective  ----------------------------------------
    %
    Our work offers a novel theoretical viewpoint: of SGD as a sum of many
    concurrent interactions between weights and data.  Diagrams such as
    $\sdia{c(01-2-3)(02-12-23)}$, analogous to those of \cite{fe49} and
    \cite{pe71}, depict these interactions. 
    %
    %-----------------  prospective  ------------------------------------------
    %
    \S\ref{appendix:future} discusses this bridge to physics --- and its
    relation to Hessian methods and natural GD --- as topics for future
    research.  We also discuss how this work may lessen the energy footprint
    required to train machine learning models.  More broadly, our work adds to
    the body of theory on optimization in the face of uncertainty, theory that
    may one day inform solutions to emerging issues in user privacy and
    pedestrian safety.

    \subsection{Example of diagram-based computation of SGD's test loss} \label{subsect:example}

        \newcommand{\nb} { \nabla }
        \newcommand{\lx} { l_x(\theta) }
        \newcommand{\teq} { \triangleq }
        \newcommand{\ex}[1] { \expc_x \wasq{#1} }

        If we run SGD for $T$ gradient steps with learning rate $\eta$ starting
        at weight $\theta_0$, then by Taylor expansion we may express the
        expected test loss of the final weight $\theta_T$ in terms of
        statistics of the loss landscape evaluated at $\theta_0$.  Our
        technical contribution is to organize the computation of this Taylor
        series via combinatorial objects we call
        \emph{diagrams}:
        \begin{midea*}[Informal]
            We can enumerate all diagrams, and assign to each diagram a number
            depending on $\eta, T$, such that summing these numbers over all
            diagrams yields SGD's expected test loss.  Restricting to 
            diagrams with $\leq d$ edges leads to $o(\eta^d)$ error.
        \end{midea*}

        Deferring details, we illustrate this work flow.  First, let
        $l_x(\theta)$ be weight $\theta$'s loss on datapoint $x$.
        We define a dictionary between tensors relating to this loss
        landscape, and diagram fragments that we will soon assemble:
        \begin{center}
            \begin{tabular}{ll}
                $G \teq \ex{\nb\lx}       \teq \mdia{MOO(0)(0)}     $ &                                                             \\
                $H \teq \ex{\nb\nb\lx}    \teq \mdia{MOO(0)(0-0)}   $ & $ C \teq \ex{(\nb\lx - G)^2} \teq \mdia{MOOc(01)(0-1)}    $ \\
                $J \teq \ex{\nb\nb\nb\lx} \teq \mdia{MOO(0)(0-0-0)} $ & $ S \teq \ex{(\nb\lx - G)^3} \teq \mdia{MOOc(012)(0-1-2)} $ 
            \end{tabular}
        \end{center}
        Here, $G, H, J$ denote the loss's derivatives w.r.t.\
        $\theta$, and $G, C, S$ denote the gradient's 
        cumulants w.r.t.\ the randomness in $x$.
        Each $\nabla^d l_x$ corresponds to a degree-$d$ node, and
        fuzzy outlines group nodes that occur within the same expectation.  

        We may pair together the loose ends of the above (and of
        analogues with more edges) to obtain
        \emph{diagrams}.\footnote{
            A diagram's colors and geometric layout lack meaning: we
            {\color{moor} color} only for convenient reference, e.g.\ to
            a diagram's ``green nodes''.  Only the topology of a diagram
            --- not its size or angles --- appear in our theory.
        }
        E.g., we may join
        $
            C = \sdia{MOOc(01)(0-1)}
        $
        with
        $
            H = \sdia{MOO(0)(0-0)}
        $
        to get
        $
            \sdia{c(01-2)(02-12)}
        $.
        As another example, we may join two copies of
        $
            G = \sdia{MOO(0)(0)}
        $
        with two copies of
        $
            H = \sdia{MOO(0)(0-0)}
        $
        to get
        $
            \sdia{c(0-1-2-3)(01-12-23)} 
        $.
        Intuitively, each diagram represents the interaction of its parts: of
        gradients ($G$), noise ($C, S, \cdots$) and curvature ($H, J, \cdots$). 
        In fact, \S\ref{appendix:interpret-diagrams} interprets 
        information as flowing along edges.
        %
        \begin{exm} \label{exm:first}
            Does non-Gaussian noise affect SGD?
            Specifically, since the skewness $S$ measures non-gaussianity,
            let's compute how $S$ affects test loss. The recipe is to identify
            the fewest-edged diagrams containing $S = \sdia{MOOc(012)(0-1-2)}$.
            In this case, there is one fewest-edged diagram ---
            $\sdia{c(012-3)(03-13-23)}$; it results from joining $S$ with
            $J=\sdia{MOO(0)(0-0-0)}$.  To evaluate a diagram, we multiply its
            components (here, $S, J$) with exponentiated $\eta H$'s, one for
            each edge:
            \begin{align*} %\label{eqn:nongauss}
                -\frac{\eta^3}{3!}
                \sum_{\mu\nu\lambda}
                    S_{\mu\nu\lambda}
                    \frac{
                        1 - \exp(-T\eta (H_{\mu\mu} + H_{\nu\nu} + H_{\lambda\lambda}))
                    }{
                        \eta (H_{\mu\mu} + H_{\nu\nu} + H_{\lambda\lambda})
                    }
                    J_{\mu\nu\lambda}
            \end{align*}
            This is $S$'s leading order contribution to SGD's test loss
            written in an eigenbasis of $\eta H$.
        \end{exm}
        \begin{rmk}
            When $\eta H = \|\eta H\|_2 I$, the large $T$ limit is 
            $
                - (\eta^3/3!)
                \sum_{\mu\nu\lambda}
                    S_{\mu\nu\lambda} J_{\mu\nu\lambda} / 3 \|\eta H\|_2
            $.
            Since $J = \nabla H$, $J / \|\eta H\|_2$ measures the relative
            change in curvature $H$ w.r.t.\ $\theta$.  So non-gaussian noise
            affects SGD proportion to the logarithmic derivative of curvature.
        \end{rmk}

%==============================================================================
%=====  BACKGROUND AND NOTATION  ==============================================
%==============================================================================

\subsection{Background, notation, and assumptions} \label{sect:background}
       
    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Tensor Conventions  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    %\subsection{Tensor conventions}
        Let $G, H, J, C, S$ be as in \S \ref{subsect:example}.
        We implicitly sum repeated Greek indices: if a covector $A$ and a
        vector $B$\footnote{
            Vectors/covectors are also called column/row vectors.
        } have coefficients $A_\mu, B^\mu$, then 
        $
            A_\mu B^\mu
            \triangleq
            \sum_\mu A_\mu \cdot B^\mu
        $.
        %To expedite dimensional analysis,
        We regard the learning rate as an
        inverse metric $\eta^{\mu\nu}$ that converts gradient covectors to
        displacement vectors \citep{bo13}.  We use the learning rate
        $\eta$ to raise indices: e.g.,
        $
            H^{\mu}_{\lambda}
            \triangleq
            \eta^{\mu\nu} H_{\nu\lambda}
        $ and
        $
            C^{\mu}_{\mu}
            \triangleq
            \sum_{\mu \nu} \eta^{\mu\nu} \cdot C_{\nu\mu}
        $.
        Though $\eta$ is a tensor, we may still define $o(\eta^d)$: a quantity
        $q$ \emph{vanishes to order $\eta^d$} when $\lim_{\eta\to 0} q/p(\eta)
        = 0$ for some homogeneous degree-$d$ polynomial $p$.

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  The Loss Landscape  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    %\subsection{Loss landscape}

        %-------------  the landscape  ----------------------------------------

        We fix a loss function $l:\Mm\to\RR$ on a space $\Mm$ of weights.  We
        fix a distribution $\Dd$ from which unbiased estimates of $l$ are
        drawn.  We write $l_x$ for a generic sample from $\Dd$ and $(l_n: 0\leq
        n<N)$ for a training sequence drawn i.i.d.\ from $\Dd$.  We refer both
        to $n$ and to $l_n$ as \emph{training points}.  We assume
        \S\ref{appendix:assumptions}'s hypotheses, e.g.\ that $l, l_x$ are
        analytic and that all moments exist.
        %
        %-------------  specialization to a common case  ----------------------
        %
        E.g., our theory models $\tanh$ networks with cross entropy loss on
        bounded data --- with weight sharing, skip connections, dropout, and
        weight decay.
        
    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Names of SGD Parameters  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    %\subsection{SGD terminology}
        %SGD performs $\eta$-steepest descent on the estimates $l_n$.
        %
        Our general theory describes SGD with any number
             $N$ of training points,
             $T$ of updates, and 
             $B$ of points per batch.
        SGD then runs $T$ many updates (i.e. $E=TN/B$ epochs, i.e. $M=T/N$
        updates per point) of the form
        $
            \theta^\mu
            \coloneqq
            \theta^\mu -
            \eta^{\mu\nu} \nabla_\nu
                \sum_{n\in \Bb_t} l_n(\theta) / B
        $,
        where $\Bb_t$ is the $t$th batch.  Our paper's body --- but not
        appendices --- will assume unless otherwise stated that \textbf{SGD has
        $E=B=1$ and GD has $T=B=N$}.

%==============================================================================
%    RELATED WORK    
%==============================================================================

\subsection{Related work} \label{sect:related}

    %--------------------------------------------------------------------------
    %           Analyzing Overfitting; Relevance of Optimization; SDE Errs  
    %--------------------------------------------------------------------------

    Several lines of work treat the overfitting of SGD-trained networks
    \citep{ne17a}.  E.g., \cite{ba17} controls the Rademacher complexity of
    deep hypothesis classes, leading to optimizer-agnostic generalization
    bounds.  Yet SGD-trained networks generalize despite their ability to
    shatter large sets \citep{zh17}, so generalization must arise from not only
    architecture but also optimization \citep{ne17b}.  Others approximate
    SGD by SDE to analyze implicit regularization (e.g.\ \cite{ch18}), but,
    per \cite{ya19a}, such continuous-time analyses cannot treat SGD noise
    correctly.
    %
    %%--------------------------------------------------------------------------
    %%           We Extend Dan's Approach                     
    %%--------------------------------------------------------------------------
    %
    We avoid these pitfalls by Taylor expanding around $\eta=0$ as in
    \cite{ro18}; unlike that work, we generalize beyond order $\eta^1$ and
    $T=2$.
    
    %--------------------------------------------------------------------------
    %           Phenomenology of Rademacher Correlates such as Hessians
    %--------------------------------------------------------------------------

    Our predictions are vacuous for large $\eta$.  Other analyses treat
    large-$\eta$ learning phenomenologically, whether by finding empirical
    correlates of gen.\ gap \citep{li18}, by showing that \emph{flat} minima
    generalize (\cite{ho17}, \cite{ke17}, \cite{wa18}), or by showing that
    \emph{sharp} minima generalize (\cite{st56}, \cite{di17}, \cite{wu18}).
    Our theory reconciles these clashing claims.
    
    %--------------------------------------------------------------------------
    %           Our Work vs Other Perturbative Approaches            
    %--------------------------------------------------------------------------

    Prior work analyzes SGD perturbatively: \cite{dy19} perturb in inverse
    network width, using 't Hooft diagrams to correct the Gaussian Process
    approximation for specific deep nets.  Perturbing to order $\eta^2$,
    \cite{ch18} and \cite{li17} assume uncorrelated Gaussian noise, so they
    cannot describe SGD's gen.\ gap.  By contrast, we use Penrose diagrams to
    compute test losses to arbitrary order in the learning rate.  We allow
    correlated, non-Gaussian noise and thus \emph{any} smooth architecture.
    E.g., we do not assume information-geometric relationships between $C$ and
    $H$,%
    \footnote{
        Disagreement of $C$ and $H$ is typical in modern learning \citep{ro12,
        ku19}.
    }
    so we may model VAEs. 

%==============================================================================
%=====  DIAGRAM CALCULUS FOR SGD  =============================================
%==============================================================================

\section{Theory, specialized to $E=B=1$ SGD's test loss} \label{sect:calculus}

    %\subsection{Diagrams, embeddings, and re-summed values}
        \begin{wraptable}{r}{5cm}
            \begin{spacing}{0.8}
            \begin{tabular}{p{5cm}}
                \textbf{Examples}:
                The diagrams
                $\sdia{c(0-1)(01)}$, $\sdia{c(012-3)(03-13-23)}$ each have $2$
                parts; $\sdia{c(0-12-3)(03-13-23)}$, $\sdia{c(01-2-3)(02-13-23)}$
                have $3$.
                %
                Corollaries \ref{cor:overfit}, \ref{cor:epochs},
                \ref{cor:batch} have $E\neq 1 \neq B$, so they feature
                $\sdia{c(01)(01)}$ and $\sdia{c(01-2)(01-12)}$, generalized
                diagrams that violate the path condition. 
                %
                Diagrams $\sdia{c(0-1-2)(02-12)}$, $\sdia{c(01-2)(01-12)}$
                are irreducible; due to their green nodes,
                $\sdia{c(0-1-2)(01-12)}$, $\sdia{c(01-2-3)(03-12-23)}$ are not.
                %
                For all $f$,
                $|\Aut_f(\sdia{c(01-2-3)(02-13-23)})|=1$ and
                $|\Aut_f(\sdia{c(01-2-3)(02-12-23)})|=2$.
            \end{tabular}
            \end{spacing}
        \end{wraptable}

        A \emph{diagram} is a finite rooted tree equipped with a partition
        of its nodes that obeys the \emph{path condition}: no path from leaf to
        root may encounter any part more than once.
        We specify the root by drawing it rightmost.  We draw the parts of 
        the partition by grouping each part's nodes inside fuzzy outlines. 
        %
        A diagram is \emph{irreducible} when each of its degree-$2$ nodes is in
        a part of size one.
        %
        An \emph{embedding} $f$ of a diagram $D$ is an injection from the
        $D$'s parts to (integer) times $0 \leq t \leq T$ that sends the
        root to $T$ and such that, for each path from leaf to root, the
        corresponding sequence of times is increasing.  E.g., $f$ might send
        $\sdia{c(01-2-3)(02-13-23)}$'s red part to $t=3$ and its green part to
        $t=4$, but not vice versa.
        %
        Let $\wabs{\Aut(D)}$ count automorphisms of $D$ that preserve $f$.

        %%%%%%%%%

        Up to unbiasing terms,\footnote{
            E.g., we actually define $\sdia{MOOc(01)(0-1)}$ to be the cumulant
            $C = \ex{(\nb\lx - G)^2}$, not the moment $\ex{(\nb\lx)^2}$.
            This centering is routine (see {\color{red} APPENDIX}), tedious to
            keep notating, and un-germane, so we ignore it.
        }
        the \emph{re-summed value} $\rvalue_f(D)$ is constructed as follows.
        %
        \textbf{Node rule}: insert a factor a $\nabla^d l_x$for each degree $d$
        node. 
        %
        \textbf{Outline rule}: group the nodes in each part within expectation
        brackets $\expc_x{}$.
        %
        \textbf{Edge rule}: for each edge whose endpoints $f$ sends to times
        $t, t^\prime$, insert a factor of $K^{\wabs{t^\prime-t}-1} \eta$
        where $K \triangleq (I-\eta H)$.
        %
        E.g., if $f$ maps $\sdia{c(012-3)(03-13-23)}$'s red part to time $t =
        T-\Delta t$, then (the red part gives $S$; the green part, $J$):
        $$
            \rvalue_f\wrap{\sdia{c(012-3)(03-13-23)}} = 
            S_{\mu\lambda\rho}
                (K^{\Delta t-1}\eta)^{\mu\nu}
                (K^{\Delta t-1}\eta)^{\lambda\sigma}
                (K^{\Delta t-1}\eta)^{\rho\pi}
            J_{\nu\sigma\pi}
        $$
        In fact, we may integrate this expression per Remark
        \ref{rmk:integrate} to recover Example \ref{exm:first}.

    \subsection{Main result}

    %\subsection{Recipe for SGD's expected test loss}
        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        %~~~~~  Recipe for Test Loss  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        Theorem \ref{thm:resum} expresses SGD's test loss as a sum over
        diagrams.  A diagram with $d$ edges scales as $O(\eta^d)$, so the
        following is a series in $\eta$.  We later truncate the series to small
        $d$, thus focusing on few-edged diagrams and simplifying the
        combinatorics of embeddings.
        \begin{thm}[Special Case] \label{thm:resum}
            For any $T$: for $\eta$ small enough, SGD has expected test loss
            \begin{equation*} \label{eq:resum}
                \sum_{\substack{D~\text{an irreducible} \\ \text{diagram}}}
                \sum_{\substack{f~\text{an embedding} \\ \text{of}~D}}
                \frac{(-1)^{|\edges(D)|}}{\wabs{\Aut_f(D)}}
                \,
                {\rvalue_f}(D)
            \end{equation*}
        \end{thm}

        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        %~~~~~  Convergence  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
        \begin{thm} \label{thm:converge}
            If $\theta_\star$ locally minimizes $l$ and $H$ is strictly
            positive, then for SGD initialized sufficiently close to
            $\theta_\star$, the $d$th-order truncation of Theorem
            \ref{thm:resum} converges as $T\to \infty$.
        \end{thm}

        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        %~~~~~  Simplifications  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
        \begin{rmk} \label{rmk:integrate}
            We may approximate sums by integrals and $(I-\eta H)^t$ by $\exp(-
            \eta H t)$, reducing to a routine integration of exponentials
            at the cost of an error factor $1 + o(\eta)$.
        \end{rmk}
      
    \subsection{Insights from the formalism}
    
        \subsubsection{SGD descends on a $C$-smoothed landscape and prefers
        minima flat w.r.t.\ $C$.}
    
            \begin{cor}[Computed from $\sdia{c(01-2-3)(02-12-23)}$] \label{cor:entropic}
                Initialized at a test minimum, and run for long times $T \gg
                1/\eta H$, SGD drifts with an expected time-averaged velocity
                of
                $$
                    v^\lambda
                    =
                    \frac{\eta^3}{T}
                    \sum_{\mu\nu}
                        C_{\mu\nu}
                        \frac{1}{\eta (H_{\mu\mu} + H_{\nu\nu})}
                        J_{\mu\nu\lambda}
                        \frac{1}{H_{\lambda\lambda}}
                    + o(\eta^2)
                    ~~~~~
                    ~~~~~
                    ~~~~~
                    \text{in an eigenbasis for}~\eta H
                $$
            \end{cor}
            
            Intuitively, $D = \sdia{c(01-2-3)(02-12-23)}$
            contains a subdiagram $\sdia{c(01-2)(02-12)} = (K\eta)^2 CH$.
            By a routine check, $CH+o(\eta^2)$ is the loss increase upon
            convolving $l$ with a $C$-shaped Gaussian.  Since
            $D$ connects the subdiagram to {\color{red} to the test
            measurement} via $1$ edge, it couples $CH$ to $l$'s linear part, so
            it represents a displacement of $\theta$ away from high $CH$.  In
            short, \emph{SGD descends on a covariance-smoothed landscape}.
            That is, in a valley of minima, SGD moves toward minima that are
            flat w.r.t.\ $C$ (Figure \ref{fig:cubicandspring}\ofthreemoo{0}).
    
            \cite{ya19b} reports a small-$T$ version of this result that
            scales with $\eta^3$.  Meanwhile, Corollary \ref{cor:entropic}'s
            large-$T$ analysis scales with $\eta^2$.  Our analysis integrates
            the noise over many updates, hence amplifying the contribution of
            $C$, and experiments verify this scaling law.
            %
            We do not make \cite{we19b}'s assumptions of thermal equilibrium,
            fast-slow mode separation, or constant covariance.  This generality
            reveals that SGD's velocity field is generically non-conservative
            (Section \ref{subsect:entropic}).
      
        \subsubsection{Both flat and sharp minima overfit less} \label{subsect:curvature-and-overfitting}
            We quantify Figure \ref{fig:cubicandspring}\ofthreemoo{12}'s
            intuition for how noise and curvature affect overfitting:
            \begin{cor}[from $\sdia{c(01-2)(02-12)}$, $\sdia{c(01)(01)}$]\label{cor:overfit}
                Initialize GD at a test minimum.  The test-loss-increase and the
                gen.\ gap (test minus train loss) due to training are,
                with errors $o(\eta^2)$ and $o(\eta^1)$:
                $$
                    \frac{C_{\mu\nu}}{2N} ~
                        \wrap{(I - \exp(-\eta T H))^{\otimes 2}}^{\mu\nu}_{\rho\lambda}
                        \wrap{H^{-1}}^{\rho\lambda}
                    ~~~~~ \text{and} ~~~~~
                    \frac{C_{\mu\nu}}{N} ~
                        \wrap{I - \exp(-\eta T H)}^{\nu}_{\lambda}
                        \wrap{H^{-1}}^{\lambda\mu}
                $$
            \end{cor}
            This gen.\ gap tends  
            to $C_{\mu\nu}(H^{-1})^{\mu\nu}/N$ as $T\to\infty$.  For maximum
            likelihood (ML) estimation in well-specified models near the ``true''
            minimum, $C=H$ is the Fisher metric, so we recover AIC:
            $(\textnormal{number of parameters})/N$.  Unlike AIC, our more general
            expression is descendably smooth, may be used with MAP or ELBO tasks
            instead of just ML, and does not assume a well-specified model.
    
            \begin{figure}[h!]
                \centering
                \plotmooh{diagrams/entropic-force-diagram}{}{0.32\columnwidth} 
                \plotmooh{diagrams/sharp}{}{0.31\columnwidth}
                \caption{%
                    \textbf{Geometric intuition for curvature-noise interactions.}
                    \textbf{Left}:
                        Gradient noise pushes SGD toward minima that are flat.
                        \emph{with respect to the covariance} (Corollary
                        \ref{cor:entropic}).  Our theory does not assume
                        \cite{we19b}'s mode separation, but we label
                        ``fast/slow modes'' to ease comparison. 
                        In red are typical $\theta$s in each cross-section of
                        the valley.  $J = \nabla H$ measures the change in
                        curvature across the valley.
                    \textbf{\bf Right}:
                      Both noise structure and curvature affect overfitting. 
                        \protect\ofthreemoo{1}: small $H$s are favored in
                        \emph{vector}-perturbed landscapes.
                        \protect\ofthreemoo{2}: large $H$s are favored in
                        \emph{covector}-perturbed landscapes.  SGD's implicit
                        regularization interpolates between these rows
                        (Corollary \ref{cor:overfit}).
                }
                \label{fig:cubicandspring}
            \end{figure}
    
        \subsubsection{High-$C$ regions more strongly repel small-$E,B$ SGD than large-$E,B$ SGD}
            \label{subsect:epochs-batch}
    
            \begin{cor}[$\sdia{c(01-2)(01-12)}$] \label{cor:epochs}
                $(M=1, \eta=\eta_0)$ SGD's weight avoids 
                high-$C$ regions more than $(M=M_0, \eta=\eta_0/M_0)$ SGD's:
                $
                    \expct{\theta_{M=M_0} - \theta_{M=1}}^\mu
                        =
                    \wrap{\frac{M_0-1}{M_0}}\wrap{\frac{B+1}{B}}\wrap{\frac{N}{2}}
                    \wrap{\nabla^\mu C^{\nu}_{\nu}} / 2
                    + o(\eta^2)
                $.
            \end{cor}
        
            \begin{cor}[$\sdia{c(01-2)(01-12)}$] \label{cor:batch}
                $(M=M_0, B=N)$ GD's test loss exceeds $(M=M_0, B=1)$ SGD's test
                loss by
                $
                      \frac{M_0(N-1)}{2} ~
                      \wrap{\nabla_\mu C^{\nu}_{\nu}} G^\mu / 2
                      +o(\eta^2)
                $.
                If $\hat{C}$ is a smooth unbiased estimator of $C$, then GD on
                a modified loss 
                $
                    \tilde l_n = l_n +
                        \frac{N-1}{4N} ~
                        \hat{C}_\nu^\nu(\theta)
                $
                has an expected test loss that agrees with SGD's to order
                $\eta^2$.  We call this method GDC.
            \end{cor}
    
        \subsubsection{Non-Gaussian noise affects SGD but not SDE}
    
            Stochastic Differential Equations (SDE: see \cite{li18}) are a popular
            theoretical approximation to SGD, but SDE and SGD differ in several
            ways.  For instance, the inter-epoch noise correlations in multi-epoch
            SGD measurably affect SGD's final test loss (Corollary
            \ref{cor:epochs}), but SDE assumes uncorrelated gradient updates.  Even
            if we restrict to single-epoch SDE, differences arise due to time
            discretization and non-gaussian noise. 
            %
            \begin{cor}[$\sdia{c(01-2)(02-12)}$, $\sdia{c(012-3)(03-13-23)}$] \label{cor:vsode}
                SGD's test loss is
                $
                    \frac{T}{2} ~ C_{\mu\nu} H^{\mu\nu} + o(\eta^2)
                $
                more than ODE's and SDE's.
                The deviation from SDE due to non-Gaussian noise is
                $
                    - (T/6) S_{\mu\nu\lambda} J^{\mu\nu\lambda} 
                    + o(\eta^3)
                $.\footnote{
                    This is Example \ref{exm:first}'s more exact
                    expression for $\eta \ll 1$:
                    they agree to leading order in $\eta$.
                }
            \end{cor}
            %
%==============================================================================
%    EXPERIMENTS AND APPLICATIONS
%==============================================================================

\section{Applying the theory}

\subsection{Experiments}

    Despite the convergence results in Theorems \ref{thm:resum} and
    \ref{thm:converge}, we have no theoretical bounds for the domain and
    \emph{rate} of convergence.  Instead, we thus test our predictions by
    experiment.  We find support for our theory in drastic rejections of the
    null hypothesis.  E.g., in Figure \ref{fig:vanilla}\ofsix{4},
    \citep{ch18} predicts a velocity of $0$ while we predict a velocity of
    $\eta^2/6$.  
    %
    Here, \texttt{I} bars and \texttt{+} signs mark $95\%$ confidence
    intervals based on the standard error of the mean.
    \S\ref{appendix:experiments} lists architectures, procedures, and further
    plots.

    \subsubsection{Training time, epochs, and batch size; $C$ repels SGD more than GD}
        %----------------------------------------------------------------------
        %       Vanilla SGD                                 
        %----------------------------------------------------------------------
        We test Theorem \ref{thm:resum}'s order $\eta^3$ truncation on smooth
        convnets for CIFAR-10 and Fashion-MNIST.  Theory agrees with experiment
        through on timescales long enough for accuracy to increase by $0.5\%$
        (Figure \ref{fig:vanilla}\ofsix{0},\ofsix{1}).
        %----------------------------------------------------------------------
        %       Epochs and Overfitting                      
        %----------------------------------------------------------------------
        \S\ref{appendix:figures} supports Corollary \ref{cor:epochs}'s
        predictions relating to epoch number.
        %----------------------------------------------------------------------
        %       Emulating Small Batches with Large Ones     
        %----------------------------------------------------------------------
        Figure \ref{fig:vanilla}\ofsix{2} tests Corollary \ref{cor:batch}'s
        claim that (relative to GD's motion) high-$C$ regions \emph{repel} SGD.
        This is significant because $C$ controls the rate at which the gen.\
        gap (test minus train loss) grows (Corollary \ref{cor:overfit}, Figure
        \ref{fig:vanilla}\ofsix{3}).
      
        \begin{figure}[h!] 
            \centering
            \pmoo{3.0cm}{new-test-0}        \pmoo{3.0cm}{new-big-bm-new}          \pmoo{3.0cm}{new-thermo-linear-screw}
            \pmoo{3.0cm}{rebut-test-1-T100} \pmoo{3.0cm}{rebut-gen-cifar-lenet-4} \pmoo{3.0cm}{new-tak}
            \caption{
                {\bf Left: Perturbation models SGD for small $\eta T$.}
                Fashion-MNIST convnet's test loss vs learning rate;
                un-re-summed predictions.
                %
                \protect\ofsix{0}: For all init.s tested ($1$ shown,
                $11$ unshown), the order $3$ prediction agrees with experiment
                through $\eta T \approx 10^0$, corresponding to a decrease
                in $0\mbox{-}1$ error of $\approx 10^{-3}$.
                %
                \protect\ofsix{1}: For large $\eta T$, our predictions
                break down.  Here, the order-$3$ prediction holds until the
                $0\mbox{-}1$ error improves by $5\cdot 10^{-3}$.
                %%%
                %%%
                \newline
                {\bf Center: $C$ controls gen.\ gap and distinguishes GD
                from SGD.}
                %
                With equal-scaled axes, \protect\ofsix{2} shows that
                GDC matches SGD (small vertical varianec) better than GD
                matches SGD (large horizontal variance) in test loss for a
                range of $\eta$ ($\approx 10^{-3}-10^{-1}$) and
                init.s\ (zero and several Xavier-Glorot trials) for
                logistic regression and convnets.  Here, $T=10$. 
                %
                \protect\ofsix{3}: CIFAR-10 generalization gaps.  For all
                init.s tested ($1$ shown, $11$ unshown), the
                degree-$2$ prediction agrees with experiment through $\eta T
                \approx 5\cdot 10^{-1}$.
                %%%
                %%%
                \newline
                {\bf Right: Predictions near minima excel for large $\eta T$.}%
                \protect\ofsix{4}: SGD travels \Archimedes' valley of
                global minima in the positive $z$ direction.
                Note: $H$ and $C$ are bounded across the valley, we see drift
                for all small $\eta$, and we see displacement exceeding the
                landscape's period of $2\pi$.  So: the effect is not a
                pathology of well-chosen learning rate, divergent noise, or
                ephemeral initial conditions.  SGD's motion depends smoothly on
                the landscape, so our special case shows non-conservativity is
                generic.    
                %
                \protect\ofsix{5}: For \MeanEstimation\ with fixed $C$ and a
                range of $H$s, initialized at the truth, the test losses after
                fixed-$T$ GD are smallest for very small and very
                large $H$.  Both sharp and flat minima overfit less.
                Our result improves on Takeuchi information and hence on AIC
                \citep{di18}, especially near $H=0$.
            }
            \label{fig:vanilla}
        %    \label{fig:batchandgen}
        %    \label{fig:thermoandtak}
        \end{figure}


    %--------------------------------------------------------------------------
    %           Thermodynamic Engine                        
    %--------------------------------------------------------------------------

    \subsubsection{Minima that are flat with respect to $C$ attract SGD} \label{subsect:entropic}
        \S\ref{appendix:artificial}
        constructs a counter-intuitive loss landscape, \Archimedes, wherein SGD
        steadily travels a valley of global minima.
        %
        \Archimedes\ has valley of global minima on the line $u=v=0$;
        Corollary \ref{cor:entropic} predicts that SGD travels this valley
        with a $z$-velocity of $+\eta^2/6$ per timestep.  By contrast, \cite{ch18}'s
        approximate analysis predicts a velocity of $0$.\footnote{
            Indeed, \Archimedes' velocity is $\eta$-perpendicular to the image
            of $(\eta C)^\mu_\nu$ in tangent space.
        }
        Our prediction agrees with experiment, even for large $T$ (Figure
        \ref{fig:vanilla}\ofsix{4}).
        %

    %--------------------------------------------------------------------------
    %           Sharp vs Flat Minima                        
    %--------------------------------------------------------------------------

    \subsubsection{Sharp and flat minima both overfit less than medium minima} \label{subsect:overfit}

        Prior work finds both that \emph{sharp} minima overfit less (for, $l^2$
        regularization sharpens minima) or that \emph{flat} minima overfit less
        (for, flat minima are robust to small displacements).  In fact, both
        phenomena occur, and noise structure determines which dominates
        (Corollary \ref{cor:overfit}).  This effect appears even in
        \MeanEstimation\, (\S\ref{appendix:artificial}): Figure
        \ref{fig:vanilla}\ofsix{5}.
        %
        To combat overfitting, we may add Corollary \ref{cor:overfit}'s
        expression for gen.\ gap to $l$.  By descending on this regularized
        loss, we may tune smooth hyperparameters such as $l_2$ regularization
        coefficients in the noisy, small-$N$ regime $H \ll C/N$
        \S\ref{appendix:figures}.  Since matrix exponentiation takes time cubic
        in dimension, this regularizer is most useful for small models.

%==============================================================================
%    CONCLUSION      
%==============================================================================

\subsection{Conclusion} \label{sect:concl}

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Summarize Contributions  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    We presented a diagram-based method for studying stochastic optimization on
    short timescales or near minima.
        Corollaries \ref{cor:entropic} and \ref{cor:overfit} together offer
        insight into SGD's success in training deep networks: SGD senses
        curvature, and curvature controls generalization.
    %
    Analyzing $\sdia{c(01-2)(02-12)}$, we proved that \textbf{flat and sharp
    minima both overfit less} than medium minima.  Intuitively, flat minima are
    robust to vector noise, sharp minima are robust to covector noise, and
    medium minima robust to neither.  We thus proposed a regularizer enabling
    gradient-based hyperparameter tuning.
    %
    Inspecting $\sdia{c(01-2-3)(02-12-23)}$, we extended \cite{we19b} to
    nonconstant, nonisotropic covariance to reveal that \textbf{SGD descends on
    a landscape smoothed by the current covariance $C$}.  As $C$ evolves, the
    smoothed landscape evolves, resulting in non-conservative dynamics.
    %
    Examining $\sdia{c(01-2)(01-12)}$, we showed that \textbf{GD may emulate
    SGD}, as conjectured by \cite{ro18}.  This is significant because, while
    small batch sizes can lead to better generalization \citep{bo91}, modern
    infrastructure increasingly rewards large batch sizes \citep{go18}.  

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Anticipate Criticism of Limitations  ~~~~~~~~~~~~~~~~~~~~~~~~~~

    %\subsection{Consequences}

        Since our predictions depend only on loss data near initialization,
        they break down after the weight moves far from initialization.  Our
        theory thus best applies to small-movement contexts, whether for long
        times (large $\eta T$) near an isolated minimum or for short times
        (small $\eta T$) in general.  E.g., our theory might especially
        illuminate meta-learners that are based on fine-tuning (e.g.\
        \cite{fi17}'s MAML).

        Much as meteorologists understand how warm and cold fronts interact
        despite long-term forecasting's intractability, we quantify the
        counter-intuitive dynamics governing each short-term interval of SGD's
        trajectory.  Equipped with our theory, practitioners may now
        refine intuitions (e.g.\ that SGD descends on the train loss) to
        account for noise.
       
%==============================================================================
%    CODA               
%==============================================================================

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Broader Impacts  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\section*{Broader impacts}

    Though machine learning has the long-term potential for vast improvements
    in world-wide quality of life, it is today a source of enormous carbon
    emissions \citep{st19}.  Our analysis of SGD may lead to a reduced carbon
    footprint in three ways. 
     
    First, Section \ref{subsect:epochs-batch} shows how to modify the loss
    landscape so that large-batch GD enjoys the stochastic regularizing
    properties of small-batch SGD, or (symmetrically) so that small-batch SGD
    enjoys the stability of large-batch GD.  By unchaining the effective batch
    size from the actual batch size, we raise the possibility of training
    neural networks on a wider range of hardware than currently practical.  For
    example, asynchronous concurrent SGD (e.g., \cite{ni11}) might
    require less inter-GPU communication and therefore less power.
     
    Second, Section \ref{sect:concl} discusses an application to meta-learning,
    which has the potential to decrease the per-task sample complexity and
    hence carbon footprint of modern machine learning.
     
    Third, the generalization of AIC developed in  Sections
    \ref{subsect:curvature-and-overfitting} and \ref{subsect:overfit} permits
    certain forms of model selection by gradient descent rather than brute
    force search.  This might drastically reduce the energy consumed during
    model selection.

    That said, insofar as our theory furthers practice, it may instead
    contribute to the rapidly growing popularity of GPU-intensive learning,
    thus negating the aforementioned benefits and accelerating climate change.

    More broadly, this paper analyzes optimization in the face of uncertainty.
    As ML systems deployed today must increasingly address \emph{user privacy},
    \emph{pedestrian safety}, and \emph{dataset diversity}, it becomes
    important to recognize that training sets and test sets
    differ.  Toward this end, theoretical work relating to non-Gaussian noise
    may assist practitioners in building provably non-discriminatory, safe, or
    private models (e.g., \cite{dw06}).  By quantifying how correlated,
    non-Gaussian gradient noise affects descent-based learning, this paper
    contributes to such broader theory.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Acknowledgements  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\section*{Acknowledgements}

    %\begin{ack}
        We would like to thank
            \textsc{Sho Yaida},
            \textsc{Dan A. Roberts}, and
            \textsc{Josh Tenenbaum}
        for
        %posing some of the problems this work resolves and for
        their patient guidance.  We appreciate the generosity of
            \textsc{Andy Banburski},
            \textsc{Ben R. Bray},
            \textsc{Jeff Lagarias}, and
            \textsc{Wenli Zhao}
        in critiquing our drafts.
        Without the encouragement of
            \textsc{Jason Corso},
            \textsc{Chloe Kleinfeldt},
            \textsc{Alex Lew}, 
            \textsc{Ari Morcos}, and
            \textsc{David Schwab},
        this paper would not be.
        Finally, we thank our anonymous reviewers for inspiring an improved
        presentation.
        %
        This work was funded in part by MIT's Jacobs Presidential Fellowship
        and in part by Facebook AI Research.
    %\end{ack}
        
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  References  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\newpage
%\section*{References}
    \bibliographystyle{plainnat}
    \bibliography{perturb}

%==============================================================================
%    APPENDICES      
%==============================================================================

\clearpage
\newpage
\renewcommand{\thesection}{\Alph{section}}
\setcounter{section}{0}

\section*{Organization of the appendices}
    The following three appendices serve three respective functions:
    \begin{itemize}
        \item to explain how to calculate using diagrams;
        \item to precisely state and prove our results, then pose a conjecture;
        \item to specify our experimental methods and results.
    \end{itemize}
    We end with a historical note.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Tutorial  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\section{How to calculate expected test losses}\label{appendix:tutorial}
    Our work introduces a novel technique for calculating the expected learning
    curves of SGD in terms of statistics of the loss landscape near
    initialization.  Here, we explain this technique.  There are {\bf four
    steps} to computing the expected test loss after a specific number of
    gradient updates: 
    \begin{itemize}
        \item Based on the chosen optimization hyperparameters (namely, batch
              size, training set size, and number of epochs):
              {\bf draw the spacetime grid} that encodes these hyperparameters.
        \item Based on our desired level of precision,
              {\bf draw all the relevant embeddings} of diagrams into the
              spacetime.
        \item {\bf Evaluate each diagram embedding}.
        \item {\bf Sum the embeddings' values} to obtain the quantity of
              interest as a function of the learning rate.
    \end{itemize}

    After presenting a small, complete example calculation that follows these
    four steps, we explain how to perform each of these steps in its own
    sub-section.  We then discuss how diagrams often offer intuition as well as
    calculational help.  Though we focus on the computation of expected test
    losses, we explain how a small change in the above four steps allows for
    the computation also of variances (instead of expectations) and of train
    losses (instead of test losses).  We conclude by comparing direct
    calculation based on our Key Lemma to the diagram method; we point out when
    and why diagrams streamline computation.

    \subsection{An example calculation}                             \label{appendix:example}
    \subsection{How to identify the relevant space-time}            \label{appendix:draw-spacetime}
    \subsection{How to identify the relevant diagram embeddings}    \label{appendix:draw-embeddings}
    \subsection{How to evaluate each embedding}                     \label{appendix:evaluate-embeddings}
    \subsection{How to sum the embeddings' values}                  \label{appendix:sum-embeddings}
    \subsection{Interpreting diagrams to build intuition}           \label{appendix:interpret-diagrams}
    \subsection{How to solve variant problems}                      \label{appendix:solve-variants}
    \subsection{Do diagrams streamline computation?}                \label{appendix:diagrams-streamline}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Math  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\section{Assumptions, results, proofs, and future topics}\label{appendix:math}
    \subsection{Setup and assumptions}                              \label{appendix:assumptions}

    \subsection{A key lemma \`a la Dyson}                           \label{appendix:key-lemma}

        Suppose $s$ is an analytic function defined on the space of weights.
        The following Lemma, reminiscent of \cite{dy49a}, helps us track
        $s(\theta)$ as SGD updates $\theta$:
        \begin{klem*} \label{lem:dyson}
            For all $T$: for $\eta$ sufficiently small, $s(\theta_T)$ is a sum
            over tuples of natural numbers:
            \begin{equation}\label{eq:dyson}
                \sum_{(d_t: 0\leq t<T) \in \NN^T}
                (-\eta)^{\sum_t d_t}
                \wrap{
                    \prod_{0 \leq t < T}
                        \wrap{\left.
                            \frac{(g \nabla)^{d_t}}{d_t!}
                        \right|_{g = \sum_{n\in \Bb_t} \nabla l_n(\theta) / B}}
                }(s) (\theta_0)
            \end{equation}
            Moreover, the expectation symbol (over training sets) commutes with
            the sum over $d$s.
        \end{klem*}
        Here, we consider each $(g \nabla)^{d_t}$ as a higher order function
        that takes in a function $f$ defined on weight space and outputs a
        function equal to the $d_t$th derivative of $f$, times $g^{d_t}$.
        The above product then indicates composition of $(g \nabla)^{d_t}$'s
        across the different $t$'s.  In total, that product takes the function
        $s$ as input and outputs a function equal to some polynomial of $s$'s
        derivatives.

        \begin{proof}[Proof of the Key Lemma]%
            We work in a neighborhood of the initialization so that the tangent
            space of weight space is a trivial bundle.  For convenience, we fix
            a  coordinate system, and with it the induced flat,
            non-degenerate inverse metric $\tilde\eta$; the benefit is that we
            may compare our varying $\eta$ against one fixed $\tilde\eta$.
            Henceforth, a ``ball'' unless otherwise specified will mean a ball
            with respect to $\tilde\eta$ around the initialization $\theta_0$.
            Since $s$ is analytic, its Taylor series converges to $s$ within
            some positive radius $\rho$ ball.  By assumption, every $l_t$ is
            also analytic with radius of convergence around $\theta_0$ at least
            some $\rho>0$.  Since gradients are $x$-uniformly
            bounded by a continuous function of $\theta$, and since in finite
            dimensions the closed $\rho$-ball is compact, we have a strict
            gradient bound $b$ uniform in both $x$ and $\theta$ on gradient
            norms within that closed ball.  When
            \begin{equation} \label{eq:smalleta}
                2 \eta T b < \rho \tilde\eta
            \end{equation}
            as norms, SGD after $T$ steps on any train set
            will necessarily stay within the $\rho$-ball.\footnote{
                In fact, the factor of $2$ helps ensure that SGD initialized at
                any point within a $\rho/2$ ball will necessarily stay within
                the $\rho$-ball.
            } We note that the above condition on $\eta$ is weak enough to
            permit all $\eta$ within some open neighborhood of $\eta=0$.  

            Condition \ref{eq:smalleta} together with analyticity of $s$ then
            implies that
            $
                \wrap{\exp(-\eta g \nabla) s}(\theta) = s(\theta - \eta g)
            $
            when $\theta$ lies in the $\tilde\eta$ ball (of radius $\rho$) and
            its $\eta$-distance from that $\tilde\eta$ ball's boundary exceeds
            $b$, and that both sides are analytic in $\eta, \theta$ on the same
            domain --- and \emph{a fortiori} when $\theta$ lies in the ball of
            radius $\rho (1 - 1/(2T))$.  Likewise, a routine induction through
            $T$ gives the value of $s$ (after doing $T$ gradient steps from an
            initialization $\theta$) as
            $$
                \wrap{
                    \prod_{0\leq t<T}
                        \left.
                            \exp(-\eta g \nabla)
                        \right|_{g=\nabla l_t(\theta)}
                }
                (s)(\theta)
            $$
            for any $\theta$ in the $\rho (1-T/(2T)$-ball (that is, the
            $\rho/2$-ball), and that both sides are analytic in $\eta, \theta$
            on that same domain.  Note that in each exponential, the
            $\nabla_\nu$ does not act on the $\nabla_\mu l(\theta)$ with which
            it pairs.  

            Now we use the standard expansion of $\exp$.  Because (by
            analyticity) the order $d$ coeffients of $l_t, s$ are bounded by
            some exponential decay in $d$ that has by assumption an $x$-uniform
            rate, we have absolute convergence and may rearrange sums.  We
            choose to group by total degree:
            \begin{equation} \label{eq:expansion}
                \cdots 
                =
                \sum_{0\leq d < \infty} (-\eta)^d
                \sum_{\substack{(d_t: 0\leq t<T) \\ \sum_t d_t = d}}
                \wrap{
                    \prod_{0 \leq t < T} \left.
                        \frac{(g \nabla)^{d_t}}{d_t!}
                    \right|_{g=\nabla l_t(\theta)}
                } s (\theta)
            \end{equation}
            The first part of the Key Lemma is proved.  It remains to show that
            expectations over train sets commute with the above summation.

            We will apply Fubini's Theorem.  To do so, it suffices to show that   
            $$
                \wabs{c_d((l_t: 0\leq t<T))} 
                \triangleq
                \wabs{
                    \sum_{\substack{(d_t: 0\leq t<T) \\ \sum_t d_t = d}}
                    \wrap{
                        \prod_{0 \leq t < T} \left.
                            \frac{(g \nabla)^{d_t}}{d_t!}
                        \right|_{g=\nabla l_t(\theta)}
                    } s (\theta)
                }
            $$
            has an expectation that decays exponentially with $d$.  The symbol
            $c_d$ we introduce purely for convenience; that its value depends
            on the train set we emphasize using function application
            notation.  Crucially, no matter the train set, we have shown
            that the expansion \ref{eq:expansion} (that features $c_d$ appear
            as coefficients) converges to an analytic function for all $\eta$
            bounded as in condition \ref{eq:smalleta}.  The uniformity of this
            demanded bound on $\eta$ implies by the standard relation between
            radii of convergence and decay of coefficients that $\wabs{c_d}$
            decays exponentially in $d$ at a rate uniform over train sets.
            If the expectation of $\wabs{c_d}$ exists at all, then, it will
            likewise decay at that same shared rate.
            
            Finally, $\wabs{c_d}$ indeed has a well-defined expected value, for
            $\wabs{c_d}$ is a bounded continuous function of a
            (finite-dimensional) space of $T$-tuples (each of whose entries can
            specify the first $d$ derivatives of an $l_t$) and because the
            latter space enjoys a joint distribution.  So Fubini's Theorem
            applies.  The Key Lemma follows.   
        \end{proof}

    \subsection{From Dyson to diagrams}                             \label{appendix:toward-diagrams}

        We now describe the terms that appear in the Key Lemma.  The following
        result looks like Theorem \ref{thm:resum}, except it has $\uvalue(D)$
        instead of $\uvalue_f(D)$, and the sum is over all diagrams, not just
        irreducible ones.  In fact, we will use Theorem \ref{thm:sgdcoef} to
        prove Theorem \ref{thm:resum}.

        \begin{thm}[Test Loss as a Path Integral] \label{thm:sgdcoef}
            For all $T$: for $\eta$ sufficiently small, SGD's expected test
            loss is
            \begin{equation*}\label{eq:sgdcoef}
                \sum_{D}
                %\wrap{
                    \sum_{\text{embeddings}~f}
                    \frac{1}{\wabs{\Aut_f(D)}}
                %}
                \frac{\uvalue(D)}{(-B)^{|\edges(D)|}}
            \end{equation*}
            Here, $D$ is a diagram whose root $r$ does not participate in
            any fuzzy edge, $f$ is an embedding of $D$ into spacetime, and
            $\wabs{\Aut_f(D)}$ counts the graph-automorphisms of $D$ that
            preserve $f$'s assignment of nodes to cells.
            %
            If we replace $D$ by 
            $
                \wrap{-\sum_{p \in \parts(D)} (D_{rp} - D)/N}
            $, where $r$ is $D$'s root,
            we obtain the expected generalization gap (test minus train loss).
        \end{thm}

        Theorem \ref{thm:sgdcoef} describe the terms that appear in the Key
        Lemma by matching each term to an embedding of a diagram in spacetime,
        so that the infinite sum becomes a sum over all diagram spacetime
        configurations.  The main idea is that the combinatorics of diagrams
        parallels the combinatorics of repeated applications of the product
        rule for derivatives applied to the expression in the Key Lemma.
        Balancing against this combinatorial explosion are factorial-style
        denominators, again from the Key Lemma, that we summarize in terms of
        the sizes of automorphism groups.

        \begin{proof}[Proof of Theorem \ref{thm:sgdcoef}]
            We first prove the statement about test losses.
            Due to the analyticity property established in our proof of the
            Key Lemma, it suffices to show agreement at each degree $d$ and
            train set individually.  That is, it suffices to show --- for
            each train set $(l_n: 0\leq n<N)$, spacetime $S$, function $\pi:
            S\to [N]$ that induces $\sim$, and natural $d$ --- that
            \begin{align} \label{eq:toprove}
                (-\eta)^d
                \sum_{\substack{
                    (d_t: 0\leq t<T) \\
                    \sum_t d_t = d
                }}
                \wrap{
                    \prod_{0 \leq t < T} \left.
                        \frac{(g \nabla)^{d_t}}{d_t!}
                    \right|_{g=\nabla l_t(\theta)}
                } l (\theta)
                = \nonumber \\
                \sum_{\substack{
                    D \in \image(\Free) \\
                    \textnormal{with $d$ edges}
                }}
                \wrap{
                    \sum_{f: D\to\Free(S)}
                    \frac{1}{\wabs{\Aut_f(D)}}
                }
                \frac{\uvalue_\pi(D, f)}{B^{d}}
            \end{align}
            Here, $\uvalue_\pi$ is the value of a diagram embedding before
            taking expectations over train sets.  We have for all $f$ that
            $\expct{\uvalue_\pi(D, f)} = \uvalue(D)$.
            Observe that both sides of \ref{eq:toprove} are finitary sums.

            \begin{rmk}[Differentiating Products] \label{rmk:leibniz}
                The product rule of Leibniz easily generalizes to higher
                derivatives of finitary products:
                $$
                    \nabla^{\wabs{M}} \prod_{k \in K} p_k
                    = 
                    \sum_{\nu:M\to K} \prod_{k\in K} \wrap{
                        \nabla^{\wabs{\nu^{-1}(k)}} p_k
                    }
                $$
                The above has $\wabs{K}^{\wabs{M}}$ many term indexed by
                functions to $K$ from $M$.
            \end{rmk}

            We proceed by joint induction on $d$ and $S$.  The base cases
            wherein $S$ is empty or $d=0$ both follow immediately from the Key
            Lemma, for then the only embedding is the unique embedding of the
            one-node diagram $\sdia{(0)()}$.  For the induction step, suppose
            $S$ is a sequence of $\Mm = \min S \subseteq S$ followed by a
            strictly smaller $S$ and that the result is proven for $(\tilde d,
            \tilde S)$ for every $\tilde d \leq d$.  Let us group by $d_0$ the
            terms on the left hand side of desideratum \ref{eq:toprove}.
            Applying the induction hypothesis with $\tilde d = d - d_0$, we
            find that that left hand side is:
            \begin{align*}
                \sum_{\substack{
                    0 \leq d_0 \leq d
                }}
                \sum_{\substack{
                    \tilde D \in \image(\Free) \\
                    \textnormal{with $d-d_0$ edges}
                }}
                \frac{1}{d_0!}
                \sum_{\tilde f: \tilde D\to\Free(\tilde S)} \wrap{
                    \frac{1}{\wabs{\Aut_{\tilde f}(\tilde D)}}
                }
                ~\cdot~
                \\ %---------------------------------------------
                (-\eta)^{d_0}
                \left.
                    (g \nabla)^{d_0}
                \right|_{g=\nabla l_0(\theta)}
                \frac{\uvalue_\pi(\tilde D, \tilde f)}{B^{d-d_0}}
            \end{align*}
            Since $\uvalue_\pi(\tilde D, \tilde f)$ is a multilinear product of
            $d-d_0+1$ many tensors, the product rule for derivatives tells us
            that $(g \nabla)^{d_0}$ acts on $\uvalue_\pi(\tilde D, \tilde f)$
            to produce $(d-d_0+1)^{d_0}$ terms.  In fact,
            $
                g = \sum_{m\in \Mm} \nabla l_m(\theta) / B
            $ 
            expands to
            $B^{d_0}(d-d_0+1)^{d_0}$ terms, each conveniently indexed
            by a pair of functions $\beta:[d_0]\to \Mm$ and $\nu:[d_0]\to
            \tilde D$.  The $(\beta, \nu)$-term corresponds to an embedding
            $f$ of a larger diagram $D$ in the sense that it contributes
            $\uvalue_\pi(D, f)/B^{d_0}$ to the sum.  Here, $(f, D)$ is $(\tilde
            f, \tilde D)$ with $\wabs{\wrap{\beta \times \nu}^{-1}(n, v)}$ many
            additional edges from the cell of datapoint $n$ at time $0$ to the
            $v$th node of $\tilde D$ as embedded by $\tilde f$.

            By the Leibniz rule of Remark \label{rmk:leibniz}, this $(\beta,
            \nu)$-indexed sum by corresponds to a sum over embeddings $f$ that
            restrict to $\tilde f$, whose terms are multiples of the value of
            the corresponding embedding of $D$.  Together with the sum over
            $\tilde f$, this gives a sum over all embeddings $f$.  So we now
            only need to check that the coefficients for each $f:D\to S$ are as
            claimed.

            We note that the $(\beta, \nu)$ diagram (and its value) agrees with
            the $(\beta \circ \sigma, \nu \circ \sigma)$ diagram (and its
            value) for any permutation $\sigma$ of $[d_0]$.  The corresponding
            orbit has size
            \begin{align*}
                \frac{d_0!}{
                    \prod_{(m, i) \in \Mm \times \tilde D}
                        \wabs{(\beta \times \nu)^{-1}(m, i)}!
                }
            \end{align*}
            by the Orbit Stabilizer Theorem of elementary group theory.   

            It is thus enough to show that
            \begin{align*} \label{eqn:countclaim}
                \wabs{\Aut_f(D)} = 
                \wabs{\Aut_{\tilde f}(D)}
                \prod_{(m, i) \in \Mm \times \tilde D}
                    \wabs{(\beta \times \nu)^{-1}(m, i)}!
            \end{align*}
            We will show this by a direct bijection.  First, observe that
            $
                f = \beta \sqcup \tilde f:
                    [d_0] \sqcup \tilde D \to \Mm \sqcup \tilde S
            $. 
            So each automorphism $\phi: D\to D$ that commutes with $f$ induces
            both an automorphism
            $
                \Aa = \phi|_{\tilde D}: \tilde D\to \tilde D
            $
            that commutes with $\tilde f$ together with the data of a map
            $
                \Bb = \phi_{[d_0]}: [d_0] \to [d_0] 
            $
            that both commutes with $\beta$.  However, not every such pair of
            maps arises from a $\phi$.  For, in order for $\Aa \sqcup \Bb: D
            \to D$ to be an automorphism, it must respect the order structure
            of $D$.  In particular, if $x\leq_D y$ with $x \in [d_0]$ and $y
            \in \tilde D$, then we need
            $$
                \Bb(x) \leq_D \Aa(y)
            $$
            as well.  The
            pairs $(\Aa, \Bb)$ that thusly preserve order are in bijection with
            the $\phi \in \Aut_f(D)$.  There are $\wabs{\Aut_{\tilde f}(\tilde
            D)}$ many $\Aa$.  For each $\Aa$, there are as many $\Bb$ as there
            are sequences $(\sigma_i: i \in \tilde D)$ of permutations on
            $
                \{j\in [d_0]: j\leq_D i\} \subseteq [d_0]
            $ 
            that commute with $\Bb$.  These permutations may be chosen
            independently; there are 
            $
                \prod_{m\in \Mm}
                    \wabs{(\beta \times \nu)^{-1}(m, i)}!
            $
            many choices for $\sigma_i$.  Claim \ref{eqn:countclaim} follows,
            and with it the correctness of coefficients.
 
            The argument for generalization gaps parallels the above when we
            use $l-\sum_n l_n/N$ instead of $l$ as the value for $s$. 
            Theorem \ref{thm:sgdcoef} is proved.
        \end{proof}

        \begin{rmk}[The Case of $E=B=1$ SGD]
            The spacetime of $E=B=1$ SGD permits all and only those
            embeddings that assign to each part of a diagram's partition  a
            distinct cell.  Such embeddings factor through a diagram
            ordering and are thus easily counted using factorials per
            Proposition \ref{prop:vanilla}.  That proposition immediately
            follows from the now-proven Theorem \ref{thm:sgdcoef}.
        \end{rmk}

        \begin{prop} \label{prop:vanilla}
            The order $\eta^d$ contribution to the expected test loss of
            one-epoch SGD with singleton batches is:
            \begin{equation*}\label{eq:sgdbasiccoef}
                \frac{(-1)^d}{d!} \sum_{D} 
                |\ords(D)| {N \choose P-1} {d \choose d_0,\cdots,d_{P-1}}
                \uvalue(D)
            \end{equation*}
            where $D$ ranges over $d$-edged diagrams.  Here, $D$'s parts have
            sizes $d_p: 0\leq p\leq P$, and $|\ords(D)|$ counts the total
            orderings of $D$ s.t.\ children precede parents and parts are
            contiguous.
        \end{prop}

    \subsection{Theorems \ref{thm:resum} and \ref{thm:converge}}    \label{appendix:re-summation}

        The diagrams summed in Theorem \ref{thm:resum} and \ref{thm:converge}
        may be grouped by their geometric realizations.  Each nonempty class of
        diagrams with a given geometric realization has a unique element with
        minimally many edges, and in this way all and only irreducible diagrams
        arise. 

        We encounter two complications: on one hand, that the sizes of
        automorphism groups might not be uniform among the class of diagrams
        with a given geometric realization.  On the other hand, that the
        embeddings of a specific member of that class might be hard to count.
        The first we handle using Orbit-Stabilizer.  The second we address as
        described by \label{subsubsect:mobius} via M\"obius sums.
           
        \begin{proof}[Proof of Theorem \ref{thm:resum}]
            We focus on test loss instead of generalization gap; the proofs are
            similar. The difference in loss from the noiseless case is given by
            all the diagram embeddings with at least one fuzzy tie, where the
            fuzzy tie pattern is actually replaced by a difference between
            noisy and noiseless cases as prescribed by the preceding discussion
            on M\"obius Sums.  Beware that even relatively noiseless embeddings
            may have illegal collisions of non-fuzzily-tied nodes within a
            single spacetime (data) row.  Throughout the rest of this proof, we
            permit such illegal embeddings of the fuzz-less diagrams that arise
            from the aforementioned decomposition.  

            Because the Taylor series for analytic functions converge
            absolutely in the interior of the disk of convergence, the
            rearrangement of terms corresponding to a grouping by geometric
            realizations preserves the convergence result of Theorem
            \ref{thm:sgdcoef}.  

            Let us then focus on those diagrams $\sigma$ with a given geometric
            realization represented by an irreducible diagram $\rho$.  By
            Theorem \ref{thm:sgdcoef}, it suffices to show that
            \begin{equation} \label{eq:hard}
                \sum_{f:\rho\to S}
                \sum_{\substack{
                    \tilde f:\sigma\to S \\
                    \exists i_\star: f=\tilde f \circ i_\star
                }}
                \frac{1}{\wabs{\Aut_{\tilde f}(\sigma)}}
                =
                \sum_{f:\rho\to S}
                \sum_{\substack{
                    \tilde f:\sigma\to S \\
                    \exists i_\star: f=\tilde f \circ i_\star
                }}
                \sum_{\substack{
                    i:\rho\to\sigma \\
                    f = \tilde f \circ i
                }}
                \frac{1}{\wabs{\Aut_{f}(\rho)}}
            \end{equation}
            Here, $f$ is considered up to an equivalence defined by
            precomposition with an automorphism of $\rho$.  We likewise
            consider $\tilde f$ up to automorphisms of $\sigma$.  And above,
            $i$ ranges through maps that induce isomorphisms of geometric
            realizations, where $i$ is considered equivalent to $\hat i$ when
            for some automorphism $\phi \in \Aut_{\tilde f}(\sigma)$, we have
            $\hat i = i \circ \phi$.  Name as $X$ the set of all such $i$s
            under this equivalence relation.

            In equation \ref{eq:hard}, we have introduced
            redundant sums to structurally align the two expressions on the
            page; besides this rewriting, we see that equation \ref{eq:hard}'s
            left hand side matches Theorem \ref{thm:sgdcoef} resulting formula
            and tgat its right hand side is the desired formula of Theorem
            \ref{thm:resum}. 

            To prove equation \ref{eq:hard}, it suffices to show (for any
            $f, \tilde f, i$ as above) that
            $$
                \wabs{\Aut_f(\rho)}
                =
                \wabs{\Aut_{\tilde f}(\sigma)}
                \cdot
                \wabs{X}
            $$
            We will prove this using the Orbit Stabilizer Theorem by presenting
            an action of $\Aut_f(\rho)$ on $X$.  We simply use precomposition
            so that $\psi\in \Aut_f(\rho)$ sends $i\in X$ to $i\circ \psi$.
            Since $f\circ\psi = f$, $i\circ \psi \in X$.  Moreover, the action
            is well-defined, because if $i\sim \hat i$ by $\phi$, then $i \circ
            \psi \sim \hat i \circ \psi$ also by $\phi$.
            
            The stabilizer of $i$ has size $\wabs{\Aut_{\tilde f}(\rho)}$.
            For, when $i \sim i \circ \psi$ via $\phi \in \Aut_{\tilde
            f}(\rho)$, we have $i\circ \psi = \phi \circ i$.  This relation in
            fact induces a bijective correspondence: \emph{every} $\phi$
            induces a $\psi$ via $\psi = i^{-1} \circ \phi \circ i$, so we have
            a map $\text{stabilizer}(i) \hookleftarrow \Aut_{\tilde f}(\rho)$
            seen to be well-defined and injective because structure set
            morphisms are by definition strictly increasing and because $i$s
            must induce isomorphisms of geometric realizations.  Conversely,
            every $\psi$ that stabilizes enjoys \emph{only} one $\phi$ via
            which $i \sim i \circ \phi$, again by the same (isomorphism and
            strict increase) properties.  So the stabilizer has the claimed
            size.

            Meanwhile, the orbit is all of $\wabs{X}$.  Indeed, suppose $i_A,
            i_B \in X$.  We will present $\psi \in \Aut_f(\rho)$ such that $i_B
            \sim i_A \circ \psi$ by $\phi=\text{identity}$.  We simply define
            $\psi = i_A^{-1} \circ i_B$, well-defined by the aforementioned
            (isomorphisms and strict increase) properties.  It is then routine
            to verify that
            $
                f \circ \psi
                =
                \tilde f \circ i_A \circ i_A^{-1} \circ i_B
                =
                \tilde f \circ i_B
                = f.
            $
            So the orbit has the claimed size, and by the Orbit Stabilizer
            Theorem, the coefficients in the expansions of Theorems 
            \ref{thm:resum} and \ref{thm:sgdcoef} match.
        \end{proof}

        \begin{proof}[Proof of Theorem \ref{thm:converge}]
            Since we assumed hessians are positive: for any $m$, the propagator
            $K^t = \wrap{(I-\eta H)^{\otimes m}}^t$ exponentially decays to $0$
            (at a rate dependent on $m$).  Since up to degree $d$ only a finite
            number of diagrams exist and hence only a finite number of possible
            $m$s, the exponential rates are bounded away from $0$.  Moreover,
            for any fixed $t_{\text{big}}$, the number of diagrams ---
            involving no exponent $t$ exceeding $t_{\text{big}}$ --- is
            eventually constant as $T$ grows.  Meanwhile, the number involving
            at least one exponent $t$ exceeding that threshold grows
            polynomially in $T$ (with degree $d$).  The exponential decay of
            each term overwhelms the polynomial growth in the number of terms,
            and the convergence statement follows.
        \end{proof}

    \subsection{Proofs of corollaries}                              \label{appendix:corollaries}

        \subsubsection{Corollary \ref{cor:entropic}}

            \begin{proof}
                The relevant irreducible diagram is $\sdia{c(01-2-3)(02-12-23)}$
                {color{red} (amputated as in the previous subsubsection)}.   
                An embedding of this diagram into $E=B=1$ SGD's spacetime
                is determined by two durations --- 
                $t$ from {\color{moor}red} to {\color{moog}green} and
                $\tilde t$ from {\color{moog}green} to {\color{moob}blue} ---
                obeying $t+\tilde t \leq T$.
                The automorphism group of each embedding has size $2$: identity
                or switch the {\color{moor}red} nodes.  So the answer is: 
                $$
                    C_{\mu \nu}
                    J^{\rho\lambda}_{\sigma}
                    \wrap{\int_{t+\tilde t\leq T}
                        \wrap{\exp(-t \eta H) \eta}^{\mu\rho}
                        \wrap{\exp(-t \eta H) \eta}^{\nu\lambda}
                        \wrap{\exp(-\tilde t \eta H) \eta}^{\sigma\pi}
                    }
                $$
                Standard calculus then gives the desired result.
            \end{proof}

        \subsubsection{Corollary \ref{cor:overfit}'s first part}

            \begin{proof}[Proof.]
                The relevant irreducible diagram is $\sdia{(01-2)(02-12)}$
                (which equals $\sdia{c(01-2)(02-12)}$ because we are at a test
                minimum).  This diagram has one embedding for each pair of
                same-row shaded cells, potentially identical, in spacetime; for
                GD, the spacetime has every cell shaded, so each
                \emph{non-decreasing} pair of durations in $[0,T]^2$ is
                represented; the symmetry factor for the case where the cells
                is identical is $1/2$, so we lose no precision by interpreting
                a automorphism-weighted sum over the \emph{non-decreasing}
                pairs as half of a sum over all pairs.  Each of these may embed
                into $N$ many rows, hence the factor below of $N$.  The two
                integration variables (say, $t, \tilde t$) separate, and we
                have:
                $$
                    \frac{N}{B^{\text{degree}}}
                    \frac{C_{\mu\nu}}{2}
                    \int_t \wrap{\exp(-t \eta H)}^\mu_\lambda
                    \int_{\tilde t} \wrap{\exp(-\tilde t \eta H)}^\nu_\rho
                    \eta^{\lambda\sigma}
                    \eta^{\rho\pi}
                    H_{\sigma\pi}
                $$
                Since for GD we have $N=B$ and we are working to degree $2$,
                the prefactor is $1/N$.  Since $\int_t \exp(a t) = (I-\exp(-a
                T))/a$, the desired result follows. 
            \end{proof}

        \subsubsection{Corollary \ref{cor:overfit}'s second part}

            We apply the generalization gap modification {\color{red}
            described in Theorem \ref{thm:sgdcoef}} to Theorem
            \ref{thm:resum}'s result about test losses; this is licensed,
            as seen by inspecting their proofs.  

            \begin{proof}[Proof]
                The relevant irreducible diagram is $\sdia{c(01)(01)}$.  This
                diagram has one embedding for each shaded cell of spacetime;
                for GD, the spacetime has every cell shaded, so each duration
                from $0$ to $T$ is represented.  So the generalization gap is,
                to leading order,
                $$
                    + \frac{C_{\mu\nu}}{N}
                    \int_t \wrap{\exp(-t \eta H)}^\mu_\lambda
                    \eta^{\lambda\nu}
                $$
                Here, the minus sign from the gen-gap modification canceled
                with the minus sign from the odd power of $-\eta$.  Integration
                finishes the proof.
            \end{proof}
 
        \subsubsection{Corollaries \ref{cor:epochs} and \ref{cor:batch}}

            Corollary \ref{cor:epochs} and the first part of Corollary
            \ref{cor:batch} follow from plugging appropriate values of $M,
            N, B$ into the following proposition.

            \begin{prop}\label{prop:ordtwo}
                To order $\eta^2$, the test loss of SGD --- on $N$
                samples for $M$ epochs with batch size $B$ dividing $N$ and with any
                shuffling scheme --- has expectation
                {\small
                \begin{align*}
                                                            l              
                    &- MN                                   G_\mu G^\mu       
                     + MN\wrap{MN - \frac{1}{2}}            G_\mu H^{\mu}_{\nu} G^\nu \\
                    &+ MN\wrap{\frac{M}{2}}                 C_{\mu \nu} H^{\mu \nu}
                     + MN\wrap{\frac{M-\frac{1}{B}}{2}}     \wrap{\nabla_\mu C^{\nu}_{\nu}} G^\mu / 2
                \end{align*}
                }
            \end{prop}

            \begin{proof}[of Proposition \ref{prop:ordtwo}]
                To prove Proposition \ref{prop:ordtwo}, we simply count
                the embeddings of the diagrams, noting that the automorphism groups
                are all of size $1$ or $2$.  Since we use fuzzy outlines instead of
                fuzzy ties, we allow untied nodes to occupy the same row, since the
                excess will be canceled out by the term subtract in the definition of
                fuzzy outlines.  See Table \ref{tbl:ordtwo}.
                \begin{table}[H]
                    \centering
                    \begin{tabular}{cll}
                        diagram                 & embed.s w/ $\wabs{\Aut_f}=1$  & embed.s w/ $\wabs{\Aut_f}=2$   \\ \hline
                        $\sdia{(0)()}$          & $1$                           & $0$                            \\  
                        $\sdia{(0-1)(01)}$      & $MNB$                         & $0$                            \\                  
                        $\sdia{(0-1-2)(01-12)}$ & ${MNB\choose 2}$              & $0$                            \\
                        $\sdia{c(01-2)(01-12)}$ & $N{MB\choose 2}$              & $0$                            \\
                        $\sdia{(0-1-2)(02-12)}$ & ${MNB\choose 2}$              & $0$                            \\
                        $\sdia{c(01-2)(02-12)}$ & $N{MB\choose 2}$              & $MNB$                             
                    \end{tabular}
                    \label{tbl:ordtwo}
                \end{table}
            \end{proof}

            \begin{proof}[Proof of Corollary \ref{cor:batch}'s second part]
                {\color{red} FILL IN}
            \end{proof}

        \subsubsection{Corollary \ref{cor:vsode}}

            The corollary's first part follows immediately from {\color{red}
            Remark
            \ref{rmk:vsode} in the case that $d=2$, $P=2$, and $(\eta N)^d$ is
            considered fixed while $N^{P-d-1}$ is considered changing.}

            \begin{proof}[Proof of second part]
                Because $\expct{\nabla l}$ vanishes at initialization, all
                diagrams with a degree-one vertex that is a singleton vanish.
                Because we work at order $\eta^3$, we consider $3$-edged
                diagrams.  Finally, because all first and second moments match
                between the two landscapes, we consider only diagrams with at
                least one partition of size at least $3$.  The only such test
                diagram is $\sdia{c(012-3)(03-13-23)}$.  This embeds in $T$
                ways (one for each spacetime cell) and has
                symmetry factor $1/3!$ for a total of
                $$
                    \frac{T \eta^3 }{6}
                    \expct{\nabla^3 l}
                    \expct{\nabla l_{n_{t_a}} \nabla l_{n_{t_b}} \nabla l_{n_{t_c}}}
                $$
            \end{proof}

    \subsection{Proofs of miscellaneous claims}                     \label{appendix:claims}
    \subsection{Future Topics}                                      \label{appendix:future}

        The diagram method opens the door to exploration of Lagrangian
        formalisms and curved backgrounds\footnote{
            \cite{la60, la51} introduce these concepts.
        }:
        \begin{quest}
            Does some least-action principle govern SGD; if not, what is an
            essential obstacle to this characterization?
        \end{quest}
        Lagrange's least-action formalism intimately intertwines with the
        diagrams of physics.  Together, they afford a modular framework for
        introducing new interactions as new terms or diagram nodes.  In fact,
        we find that some \emph{higher-order} methods --- such as the
        Hessian-based update
        $
            \theta \leftsquigarrow
            \theta -
            (\eta^{-1} + \lambda \nabla \nabla l_t(\theta))^{-1}
            \nabla l_t(\theta)
        $
        parameterized by small $\eta, \lambda$ --- admit diagrammatic analysis
        when we represent the $\lambda$ term as a second type of diagram node.
        Though diagrams suffice for computation, it is Lagrangians that most
        deeply illuminate scaling and conservation laws.
        %
        \begin{conj}[Riemann Curvature Regularizes]
            SGD's gen.\ gap decreases as sectional curvature grows. 
        \end{conj}
        Though our work assumes a flat metric $\eta^{\mu\nu}$, it might
        generalize to curved weight spaces\footnote{
            One may represent the affine connection as a node, thus giving
            rise to non-tensorial and hence gauge-dependent diagrams.
        }.
        Curvature finds concrete application in the \emph{learning on
        manifolds} paradigm of \cite{ab07, zh16}, notably specialized to
        \cite{am98}'s \emph{natural gradient descent} and \cite{ni17}'s
        \emph{hyperbolic embeddings}.  We are optimistic our formalism may
        resolve conjectures such as above.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Experiments  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\section{Exerimental methods}\label{appendix:experiments}

    \subsection{What artificial landscapes did we use?}             \label{appendix:artificial}

        We define three artificial landscapes, evocatively called
        \Gauss, \Archimedes, and \MeanEstimation.

        \subsubsection*{\Gauss}
            The \Gauss\, landscape is a simple case of fitting a gaussian to
            data.  In particular, it is a probability distribution $\Dd$ over
            functions $l_x:\RR^1\to \RR$ on $1$-dimensional weight space,
            indexed by standard-normally distributed $1$-dimensional datapoints
            $x$ and defined by the expression:
            $$
                l_x(h)
                \triangleq
                \frac{1}{2} \wrap{h + x^2 \exp(-h)}
            $$
            To measure overfitting, we initialize at the true test minimum
            $h=0$, then train and see how much the test loss increases.
            
        \subsubsection*{\Archimedes}
            The \Archimedes\, landscape has chirality, much like the ancient
            screw of Archimedes.  
            Specifically, the \Archimedes\ landscape has
            %
            weights     $\theta = (u,v,z) \in \RR^3$,
            %
            data points $x \sim \Nn(0, 1)$,
            %
            and loss:
            %
            $$
                l_x(w)
                \triangleq
                \frac{1}{2} H(\theta) + x \cdot S(\theta)
            $$
            %
            Here, $$H(\theta) = u^2 + v^2 + (\cos(z) u + \sin(z) v)^2$$
            %
            and   $$S(\theta) = \cos(z-\pi/4) u + \sin(z-\pi/4) v$$
            Note that for fixed $z$, $H$ is quadratic and $S$ is linear.  Also,
            since $x \sim \Nn(0,1)$, $x S(\theta)$ has expectation $0$.
 
            In fact, the landscape has a three-dimensional continuous screw
            symmetry consisting of translation along $z$ and simulateous
            rotation in the $u-v$ plane.  Our experiments are initialized at
            $u=v=z=0$, which lies within a valley of global minima defined by
            $u=v=0$.  

            The paper body showed that SGD travels in \Archimedes' $+z$
            direction.  By topologically quotienting the weight space, we may
            turn the the line-shaped valley into a circle-shaped valley.  Then
            SGD eternally travels counterclockwise in this circle.
           
        \subsubsection*{\MeanEstimation}
            The \MeanEstimation\, family of landscapes has $1$ dimensional
            weights $1$-dimensional datapoints $x$ and defined by the
            expression:
            $$
                l_x(w)
                \triangleq
                \frac{1}{2} H w^2 + x S w
            $$
            Here, $H, S$ are positive reals parameterizing the family; they
            give the hessian and (square root of) gradient covariance,
            respectively.

            For our hyperparameter-selection experiment {\color{red} FIGURE},
            we introduce an $l_2$ term $\lambda$ as follows:
            $$
                l_x(w, \lambda)
                \triangleq
                \frac{1}{2} (H + \lambda) w^2 + x S w
            $$
            Here, we constrain $\lambda\geq 0$ during optimization using
            projections; we found similar results when parameterizing
            $\lambda = \exp(h)$, which obviates the need for projection but
            necessitates a non-canonical choice of initialization.  We
            initialize $\lambda=0$.

    \subsection{What image-classification landscapes did we use?}   \label{appendix:natural}

        \subsubsection*{Architectures}
            In addition to the artificial loss landscapes
            \Gauss, \Archimedes, and \MeanEstimation, 
            we tested our predictions on logistic linear regression
            and simple convolutional networks (2 convolutional weight layers
            each with kernel $5$, stride $2$, and $10$ channels, followed by
            two dense weight layers with hidden dimension $10$) for the
            CIFAR-10 \cite{kr09} and Fashion-MNIST datasets \cite{xi17}.  The
            convolutional architectures used $\tanh$ activations and Gaussian
            Xavier initialization.  To set a standard distance scale on weight
            space, we parameterized the model so that the
            Gaussian-Xavier initialization of the linear maps in each layer
            differentially pulls back to standard normal initializations of the
            parameters.
            
        \subsubsection*{Datasets}
            For image classification landscapes, we regard the finite amount of
            available data as the true (sum of diracs) distribution $\Dd$ from
            which we sample test and training sets in i.i.d.\ manner (and hence
            ``with replacement'').  We do this to gain practical access to a
            ground truth against which we may compare our predictions.  One
            might object that this sampling procedure would cause test and
            training sets to overlap, hence biasing test loss measurements.  In
            fact, test and training sets overlap only in reference, not in
            sense: the situation is analogous to a text prediction task in
            which two training points culled from different corpora happen to
            record the same sequence of words, say, ``Thank you!''.  In any
            case, all of our experiments focus on the limited-data regime, e.g.
            $10^1$ datapoints out of $\sim 10^{4.5}$ dirac masses, so overlaps
            are rare.

    \subsection{Measurement process}                                \label{appendix:measure}

        \subsubsection*{Diagram evaluation on real landscapes}
            We implemented the formulae of Appendix \ref{sect:bessel} in order
            to estimate diagram values from real data measured at
            initialization from batch averages of products of derivatives.

        \subsubsection*{Descent simulations}
            We recorded test and train losses for each of the trials below.  To
            improve our estimation of average differences, when we compared two
            optimizers, we gave them the same random seed (and hence the same
            training sets).

            We ran $2 \cdot 10^5$ trials of \Gauss\, with SDE and SGD,
            initialized at the test minimum with $T=1$ and $\eta$ ranging from
            $5\cdot 10^{-2}$ to $2.5\cdot 10^{-1}$.
            We ran $5 \cdot 10^1$ trials of \Archimedes with SGD with $T=10^4$
            and $\eta$ ranging from $10^{-2}$ to $10^{-1}$.
            We ran $10^3$ trials of \MeanEstimation with GD and STIC
            with $T=10^2$, $H$ ranging from $10^{-4}$ to $4 \cdot 10^0$,
            a covariance of gradients of $10^2$, and the true mean $0$ or
            $10$ units away from initialization.

            We ran $5 \cdot 10^4$ trials of the CIFAR-10 convnet on each of $6$
            Glorot-Xavier initializations we fixed once and for all through
            these experiments for the optimizers SGD, GD, and GDC, with $T=10$
            and $\eta$ between $10^{-3}$ and $2.5 \cdot 10^{-2}$.  We did
            likewise for the linear logistic model on the one initialization of
            $0$.

            We ran $4 \cdot 10^4$ trials of the Fashion-MNIST convnet on each
            of $6$ Glorot-Xavier initializations we fixed once and for all
            through these experiments for the optimizers SGD, GD, and GDC with
            $T=10$ and $\eta$ between $10^{-3}$ and $2.5 \cdot 10^{-2}$.  We
            did likewise for the linear logistic model on the one
            initialization of $0$. 

    \subsection{Implementing optimizers}                            \label{appendix:optimizers}
        We approximated SDE by refining time discretization by a factor of
        $16$, scaling learning rate down by a factor of $16$, and introducing
        additional noise in the shape of the covariance in proportion as
        prescribed by the Wiener process scaling.

        Our GDC regularizer was implemented using the unbiased estimator
        $$
            \hat{C} \triangleq (l_x - l_y)_\mu {l_x}_\nu / 2
        $$
        
        For our tests of regularization based on Corollary \ref{cor:overfit},
        we exploited the low-dimensional special structure of the artificial
        landscape in order to avoid diagonalizing to perform the matrix
        exponentiation: precisely, we used that, even on training landscapes,
        the covariance of gradients would be degenerate in all but one
        direction, and so we need only exponentiate a scalar.

    \subsection{Software frameworks and hardware}                   \label{appendix:frameworks}

        All code and data-wrangling scripts can be found on
        {\color{mooteal}github.com/???????/perturb}.  This link will be made
        available after the period of double-blind review.

        Our code uses PyTorch 0.4.0 \cite{pa19} on Python 3.6.7; there are no
        other substantive dependencies.  The code's randomness is parameterized
        by random seeds and hence reproducible.

        We ran experiments on a Lenovo laptop and on our institution's
        clusters; we consumed about $100$ GPU-hours.

    \subsection{Unbiased estimators of landscape statistics}        \label{appendix:bessel-factors}
        %
        We use the following method --- familiar to some of our colleagues but
        hard to find writings on --- for obtaining unbiased estimates for
        various statistics of the loss landscape.  The method is merely an
        elaboration of Bessel's factor \citep{ga23}.  For completeness, we
        explain it here. 
        
        Given samples from a joint probability space $\prod_{0\leq d<D} X_d$,
        we seek unbiased estimates of \emph{multipoint correlators} (i.e.\ products of
        expectations of products) such as $\wang{x_0 x_1 x_2}\wang{x_3}$.  Here,
        angle brackets denote expectations over the population. 
        For
        example, say $D=2$ and from $2S$ samples we'd like to estimate
        $\wang{x_0 x_1}$.  Most simply, we could use $\Avg_{0\leq s<2S}
        x_0^{(s)} x_1^{(s)}$, where $\Avg$ denotes averaging over the sample.  In fact, the
        following also works:
        %
        \begin{equation} \label{eq:bessel}
            S
            \wrap{\Avg_{0\leq s< S} x_0^{(s)}}
            \wrap{\Avg_{0\leq s< S} x_1^{(s)}}
            +
            (1-S)
            \wrap{\Avg_{0\leq s< S} x_0^{(s)}}
            \wrap{\Avg_{S\leq s<2S} x_1^{(s)}}
        \end{equation}
        %
        When multiplication is expensive (e.g. when each $x_d^{(s)}$ is a
        tensor and multiplication is tensor contraction), we prefer the latter,
        since it uses $O(1)$ rather than $O(S)$ multiplications.  This in turn
        allows more efficient use of batch computations on GPUs.  We now
        generalize this estimator to higher-point correlators (and $D\cdot S$
        samples).

        For uniform notation, we assume without loss that each of the $D$
        factors appears exactly once in the multipoint expression of interest;
        such expressions then correspond to partitions on $D$ elements, which
        we represent as maps $\mu:\wasq{D}\to \wasq{D}$ with $\mu(d)\leq d$ and
        $\mu\circ \mu=\mu$.  Note that $\wabs{\mu} \coloneqq \wabs{im(\mu)}$
        counts $\mu$'s parts.  We then define the statistic
        %
        $$
            \wurl{x}_\mu
            \triangleq
            \prod_{0\leq d<D} \Avg_{0\leq s<S} x_d^{(\mu(d)\cdot S + s)}
        $$
        %
        and the correlator $\wang{x}_\mu$ we define to be the expectation of 
        $\wurl{x}_\mu$ when $S=1$.  In this notation, \ref{eq:bessel} says: 
        $$
            \wang{x}_{\partitionbox{0}\partitionbox{1}}
            =
            \expct{
                S       \cdot \wurl{x}_{\partitionbox{0 1}} +
                (1-S)   \cdot \wurl{x}_{\partitionbox{0}\partitionbox{1}}
            }
        $$
        %
        Here, the boxes indicate partitions of $\wasq{D}=\wasq{2}=\{0,1\}$.
        Now, for general $\mu$, we have:
        %
        \begin{equation} \label{eq:newbessel}
            \expct{S^D \wurl{x}_\mu}
            =
            \sum_{\tau\leq \mu} \wrap{
                \prod_{0\leq d<D}
                    \frac{S!}{\wrap{S-\wabs{\tau(\mu^{-1}(d))}}!}
            }
            \wang{x}_\tau
        \end{equation}
        %
        where `$\tau \leq \mu$' ranges through partitions \emph{finer} than 
        $\mu$, i.e. maps $\tau$ through which $\mu$ factors.   
        In smaller steps, \ref{eq:newbessel} holds because
        %
        \begin{align*}
            \expct{S^D \wurl{x}_\mu}
            &=
            \expct{
                \sum_{(0\leq s_d<S) \in \wasq{S}^D}
                \prod_{0\leq d<D}
                x_d^{\wrap{\mu(d)\cdot S + s_d}}
            }\\
            &=
            \sum_{\substack{(0\leq s_d<S) \\ \in \wasq{S}^D}}
            \expct{
                \prod_{0\leq d<D}
                x_d^{\wrap{\min \wurl{
                    \tilde{d}~:~\mu(\tilde{d})\cdot S+s_{\tilde{d}} = \mu(d)\cdot S+s_d
                }}}
            }\\
            &=
            \sum_{\tau} \wabs{\wurl{\substack{
                (0\leq s_d<S)~\in~[S]^D~: \\
                \wrap{\substack{
                    \mu(d)=\mu(\tilde{d}) \\
                    \wedge~s_d=s_{\tilde{d}}
                }}
                \Leftrightarrow
                \tau(d)=\tau(\tilde{d})
            }}}
            \wang{x}_\tau \\
            &=
            \sum_{\tau\leq \mu} \wrap{
                \prod_{0\leq d<D}
                    \frac{S!}{\wrap{S-\wabs{\tau(\mu^{-1}(d))}}!}
            }
            \wang{x}_\tau
        \end{align*}

        Solving \ref{eq:newbessel} for $\wang{x}_\mu$, we find:
        %
        \begin{equation*}
            \text{\fbox{$
            \wang{x}_\mu
            =
            \frac{S^D}{S^{\wabs{\mu}}}
            \expct{
                \wurl{x}_\mu
            }
            -
            \sum_{\tau < \mu} \wrap{
                \prod_{d\in im(\mu)}
                \frac{\wrap{S-1}!}{\wrap{S-\wabs{\tau(\mu^{-1}(d))}}!}
            }
            \wang{x}_\tau
            $}}
        \end{equation*}
        %
        This expresses $\wang{x}_\mu$ in terms of the batch-friendly estimator
        $\wurl{x}_\mu$ as well as correlators $\wang{x}_\tau$ for $\tau$
        \emph{strictly} finer than $\mu$.  We may thus (use dynamic programming
        to) obtain unbiased estimators $\wang{x}_\mu$ for all partitions $\mu$.
        Symmetries of the joint distribution and of the multilinear
        multiplication may further streamline estimation by turning a sum over
        $\tau$ into a multiplication by a combinatorial factor.  For example,
        in the case of complete symmetry:
        %
        $$
            \wang{x}_{\partitionbox{012}}
            =
            S^2
            \wurl{x}_{\partitionbox{012}}
            -
            \frac{(S-1)!}{(S-3)!}
            \wurl{x}_{\partitionbox{0}\partitionbox{1}\partitionbox{2}}
            -
            3\frac{(S-1)!}{(S-2)!}
            \wurl{x}_{\partitionbox{0}\partitionbox{12}}
        $$

    \subsection{Additional figures}                                 \label{appendix:figures}
        \begin{figure}[H] 
            \centering
            \pmoo{3.0cm}{multi-fashion-logistic-0}
            \pmoo{3.0cm}{vs-sde}
            \pmoo{3.0cm}{tak-reg}
            \caption{
                \textbf{Further experimental results}.
                %
                \textbf{Left}: SGD with $2, 3, 5, 8$ epochs incurs greater test
                loss than one-epoch SGD (difference shown in I bars) by the
                predicted amounts (predictions shaded) for a range of learning
                rates.  Here, all SGD runs have $N=10$; we scale the learning
                rate for $E$-epoch SGD by $1/E$ to isolate the effect of
                inter-epoch correlations away from the effect of larger $\eta
                T$.
                %
                \textbf{Center}: SGD's difference from SDE after $\eta T
                \approx 10^{-1}$ with maximal coarseness on the Gaussian-fit
                problem.  Two effects not modeled by SDE ---
                time-discretization and non-Gaussian noise oppose on this
                landscape but do not completely cancel.  Our theory
                approximates the above curve with a correct sign and order of
                magnitude; we expect that the fourth order corrections would
                improve it further.
                %
                \textbf{Right}: Blue intervals regularization using Corollary
                \ref{cor:overfit}.  When the blue intervals fall below the
                black bar, this proposed method outperforms plain GD.  For
                \MeanEstimation with fixed $C$ and a range of $H$s, initialized
                a fixed distance \emph{away} from the true minimum, descent on
                an $l_2$ penalty coefficient $\lambda$ improves on plain GD for
                most Hessians.  The new method does not always outperform GD,
                because $\lambda$ is not perfectly tuned according to STIC but
                instead descended on for finite $\eta T$.
            }
            \label{fig:takreg}
        \end{figure}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  History of SGD  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\section{History of SGD}
    We were surprised to learn of gradient descent's pre-silicon history:

    It was \cite{ki52} who, in uniting gradient descent \citep{ca47} with
    stochastic approximation \citep{ro51}, invented SGD.  Since the development
    of back-propagation for efficient differentiation \citep{we74}, SGD has
    been used to train connectionist models including neural networks
    \citep{bo91}, in recent years to remarkable success \citep{le15}.

\end{document}
