%   author: samtenka
%   change: 2020-06-01
%   create: 2020-05-29
%   descrp: LaTeX source for perturb project
%   to use: compile along with perturb.bib and diagram and plot directories

%==============================================================================
%=====  LATEX PREAMBLE  =======================================================
%==============================================================================

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Document Styling  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}      

%\usepackage[nonatbib]{neurips_2020}
\usepackage{natbib}
%\usepackage[final]{neurips_2020}

%---------------------  mathematics  ------------------------------------------

\usepackage{amsmath, amssymb, amsthm, amsfonts}
\usepackage{mathtools, nicefrac}

\usepackage{setspace}
%---------------------  tables  ----------------------------------------------- 

\usepackage{booktabs}
\usepackage{array}
\newcolumntype{L}{>{$}l<{$}}

%---------------------  graphics and figures  ---------------------------------

\usepackage{graphicx}
\usepackage{wrapfig, float, subfigure}
\usepackage{hanging, txfonts, ifthen}

\newcommand{\ofsix}[1]{
    {\tiny \raisebox{0.04cm}{$\substack{
        \ifthenelse{\equal{#1}{0}}{{\color{moor}\blacksquare}}{\square}
        \ifthenelse{\equal{#1}{2}}{{\color{moor}\blacksquare}}{\square}    
        \ifthenelse{\equal{#1}{4}}{{\color{moor}\blacksquare}}{\square} \\
        \ifthenelse{\equal{#1}{1}}{{\color{moor}\blacksquare}}{\square}    
        \ifthenelse{\equal{#1}{3}}{{\color{moor}\blacksquare}}{\square}
        \ifthenelse{\equal{#1}{5}}{{\color{moor}\blacksquare}}{\square}
    }$}}
}

%---------------------  colors  -----------------------------------------------

\usepackage{xcolor, framed}
\definecolor{moolime}{rgb}{0.90,1.00,0.90}
\definecolor{moosky}{rgb}{0.90,0.90,1.00}
\definecolor{moopink}{rgb}{1.00,0.90,0.90}
\definecolor{moor}{rgb}{0.8,0.2,0.2}
\definecolor{moog}{rgb}{0.2,0.8,0.2}
\definecolor{moob}{rgb}{0.2,0.2,0.8}
\definecolor{mooteal}{rgb}{0.1,0.6,0.4}

%---------------------  intertext: footnotes and hyperlinks  ------------------ 

\usepackage[perpage]{footmisc}
\renewcommand*{\thefootnote}{
    \color{red}
    \arabic{footnote}
    %\fnsymbol{footnote}
} 

\usepackage{hyperref}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Theorem Environments  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  mathematical results  ---------------------------------

\theoremstyle{plain}
    \newtheorem*{klem*}{Key Lemma}
    \newtheorem{thm}{Theorem}
    \newtheorem*{thm*}{Theorem}
    \newtheorem{cor}{Corollary}
    \newtheorem{prop}{Proposition}

%---------------------  mathematical questions  -------------------------------

    \newtheorem{conj}{Conjecture}
    \newtheorem{quest}{Question}
    \newtheorem*{quest*}{Question}
    \newtheorem*{quests*}{Questions}

%---------------------  definitions, answers, remarks  ------------------------

\theoremstyle{definition}
    \newtheorem{defn}{Definition}
    \newtheorem*{answ*}{Answer}
    \newtheorem{rmk}{Remark}
    \newtheorem*{midea*}{Main Idea}
    \newtheorem*{rmk*}{Remark}
    \newtheorem{exm}{Example}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Custom Math Commands  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  expanding containers  ---------------------------------

\newcommand{\wrap}[1]{\left(#1\right)}
\newcommand{\wasq}[1]{\left[#1\right]}
\newcommand{\wang}[1]{\left\langle#1\right\rangle}
\newcommand{\wive}[1]{\left\llbracket#1\right\rrbracket}
\newcommand{\worm}[1]{\left\|#1\right\|}
\newcommand{\wabs}[1]{\left|#1\right|}
\newcommand{\wurl}[1]{\left\{#1\right\}}

\newcommand{\partitionbox}[1]{
    \text{
        \fboxsep=0.5pt
        \tiny
        \fbox{#1}
    }
}

%---------------------  special named objects  --------------------------------

\newcommand{\Free}{\mathcal{F}}
\newcommand{\Forg}{\mathcal{G}}
\newcommand{\Mod}{\mathcal{M}}
\newcommand{\Hom}{\text{\textnormal{Hom}}}
\newcommand{\Aut}{\text{\textnormal{Aut}}}
\newcommand{\image}{\text{\textnormal{im}}}
\newcommand{\uvalue}{\text{\textnormal{uvalue}}}
\newcommand{\rvalue}{\text{\textnormal{rvalue}}}
\newcommand{\edges}{\text{\textnormal{edges}}}
\newcommand{\ords}{\text{\textnormal{ords}}}
\newcommand{\parts}{\text{\textnormal{parts}}}
\newcommand{\SGD}{\text{\textnormal{SGD}}}
\DeclareMathOperator*{\Avg}{\text{\sffamily A}}
\newcommand{\expc}{\mathbb{E}}
\newcommand{\expct}[1]{\mathbb{E}\left[#1\right]}

%---------------------  fancy letters  ----------------------------------------

\newcommand{\Aa}{\mathcal{A}}
\newcommand{\Bb}{\mathcal{B}}
\newcommand{\Cc}{\mathcal{C}}   \newcommand{\CC}{\mathbb{C}}
\newcommand{\Dd}{\mathcal{D}}
\newcommand{\Ee}{\mathcal{E}}
\newcommand{\Ff}{\mathcal{F}}
\newcommand{\Gg}{\mathcal{G}}
\newcommand{\Hh}{\mathcal{H}}
\newcommand{\Ll}{\mathcal{L}}
\newcommand{\Mm}{\mathcal{M}}
\newcommand{\Nn}{\mathcal{N}}   \newcommand{\NN}{\mathbb{N}}
\newcommand{\Oo}{\mathcal{O}}
\newcommand{\Pp}{\mathcal{P}}
\newcommand{\Qq}{\mathcal{Q}}   \newcommand{\QQ}{\mathbb{Q}}
\newcommand{\Rr}{\mathcal{R}}   \newcommand{\RR}{\mathbb{R}}
\newcommand{\Ss}{\mathcal{S}}
\newcommand{\Tt}{\mathcal{T}}
\newcommand{\Uu}{\mathcal{U}}
\newcommand{\Vv}{\mathcal{V}}
\newcommand{\Ww}{\mathcal{W}}
\newcommand{\Xx}{\mathcal{X}}
\newcommand{\Yy}{\mathcal{Y}}
\newcommand{\Zz}{\mathcal{Z}}   \newcommand{\ZZ}{\mathbb{Z}}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Pictures  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  pictures with specified width or height  --------------

\newcommand{\plotmoow}[3]{\includegraphics[width=#2          ]{../#1}}
\newcommand{\plotmooh}[3]{\includegraphics[         height=#3]{../#1}}
\newcommand{\pmoo}[2]{\includegraphics[height=#1]{../plots/#2}}

%---------------------  inline diagrams of various sizes  ---------------------

\newcommand{\sizeddia}[2]{
    \begin{gathered}
        \includegraphics[scale=#2]{../diagrams/#1.png}
    \end{gathered}
}
\newcommand{\bdia}[1]{\protect \sizeddia{#1}{0.22}}
\newcommand{\dia} [1]{\protect \sizeddia{#1}{0.18}}
\newcommand{\mdia}[1]{\protect \sizeddia{#1}{0.14}}
\newcommand{\sdia}[1]{\protect \sizeddia{#1}{0.10}}

\newcommand{\mend}{\hfill $\Diamond$}

\newcommand{\Archimedes}{\textsc{Archimedes}}
\newcommand{\MeanEstimation}{\textsc{Mean Estimation}}

%==============================================================================
%=====  FRONT MATTER  =========================================================
%==============================================================================

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Title and Author  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\title{%
    A Perturbative Analysis of Stochastic Descent
}

\author{%
    Samuel C.~Tenka \\
    Computer Science and AI Lab \\
    Massachusetts Institute of Technology \\
    Cambridge, MA 02139 \\
    \texttt{colimit@mit.edu}
}

\begin{document}

    \maketitle
    
    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Abstract  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    
    \begin{abstract}
        %
        %-------------  hammer and general nail  ------------------------------
        %
        We analyze Stochastic Gradient Descent (SGD) at small learning rates.
        Unlike prior analyses based on stochastic differential equations, our
        theory models discrete time and hence non-Gaussian noise.
        %
        %-------------  applications  -----------------------------------------
        %
        We prove that gradient noise systematically pushes SGD toward flatter
        minima.  We characterize when and why flat minima overfit less than
        sharp minima.  We generalize the Akaike Info.\ Criterion (AIC) to a
        smooth estimator of overfitting, hence enabling gradient-based model
        selection.  We show how non-stochastic GD with a modified loss function
        may emulate SGD.
        %
        %-------------  mention of experiments  -------------------------------
        %
        We verify our predictions on convnets for CIFAR-10 and Fashion-MNIST.
    \end{abstract}
    
%==============================================================================
%=====  INTRODUCTION  =========================================================
%==============================================================================

\section{Introduction}

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Orienting Invitation  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    %-----------------  object of study  --------------------------------------

    Practitioners benefit from the intuition that SGD approximates noiseless GD
    \cite{bo91}.  In this paper, we refine that intuition by showing how
    gradient noise biases learning toward certain areas of weight space.
    %
    %-----------------  vs ODE and SDE  ---------------------------------------
    %
    Departing from prior work, we model discrete time and hence non-Gaussian
    noise.  Indeed, we derive corrections to continuous-time, Gaussian-noise
    approximations such as ordinary and stochastic differential equations (ODE,
    SDE).
    For example, we construct a loss landscape on which SGD eternally cycles
    counterclockwise, a phenomenon impossible with ODEs. 
    %
    %-----------------  organization Plan  ------------------------------------
    %
    Leaving the rigorous development of our general theory to {\color{red}
    APPENDIX}, our paper body highlights and intuitively discusses the theory's
    main corollaries.

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Soft Benefits: Physical Intuition and Further Applications  ~~~

    %-----------------  retrospective  ----------------------------------------
    %
    Our work offers a novel viewpoint of SGD as many concurrent interactions
    between weights and data.  Diagrams such as $\sdia{c(01-2-3)(02-12-23)}$,
    analogous to those of \cite{fe49, pe71}, depict these interactions. 
    %
    %-----------------  prospective  ------------------------------------------
    %
    In the appendix, we discuss this bridge to physics --- and its relation to
    Hessian methods and natural GD --- as topics for future research.  We also
    discuss how this work may ameliorate or exacerbate the learning community's
    disproportionate contribution to climate change.  More broadly, our work
    adds to the body of theory on optimization in the face of uncertainty,
    theory that may one day inform solutions to emerging issues in user privacy
    and pedestrian safety.

    \subsection{Example of diagram-based computation of SGD's test loss}

        \newcommand{\nb} { \nabla }
        \newcommand{\lx} { l_x(\theta) }
        \newcommand{\teq} { \triangleq }
        \newcommand{\ex}[1] { \expc_x \wasq{#1} }

        If we run SGD for $T$ gradient steps with learning rate $\eta$ starting
        at weight $\theta_0$, then by Taylor expansion we may express the
        expected test loss of the final weight $\theta_T$ in terms of
        statistics of the loss landscape evaluated at $\theta_0$.  Our
        technical contribution is to organize the computation of this Taylor
        series via combinatorial objects we call
        \emph{diagrams}:
        \begin{midea*}[Informal]
            We can enumerate all diagrams, and assign to each diagram a number
            depending on $\eta, T$, such that summing these numbers over all
            diagrams yields SGD's expected test loss.  Restricting to 
            diagrams with $\leq d$ edges leads to $o(\eta^d)$ error.
        \end{midea*}

        Deferring details to later sections and appendices, we illustrate this
        work flow.  First, let $l_x(\theta)$ be weight $\theta$'s loss on
        datapoint $x$.  We define a tensor$\,\leftrightarrow\,$diagram
        dictionary:
        \begin{center}
            \begin{tabular}{ll}
                $G \teq \ex{\nb\lx}       \teq \mdia{MOO(0)(0)}     $ &                                                             \\
                $H \teq \ex{\nb\nb\lx}    \teq \mdia{MOO(0)(0-0)}   $ & $ C \teq \ex{(\nb\lx - G)^2} \teq \mdia{MOOc(01)(0-1)}    $ \\
                $J \teq \ex{\nb\nb\nb\lx} \teq \mdia{MOO(0)(0-0-0)} $ & $ S \teq \ex{(\nb\lx - G)^3} \teq \mdia{MOOc(012)(0-1-2)} $ 
            \end{tabular}
        \end{center}

        Here, $G, H, J$ denote the loss's derivatives w.r.t.\
        $\theta$, and $G, C, S$ denote the gradient's 
        cumulants w.r.t.\ the randomness in $x$.
        Each $\nabla^d l_x$ corresponds to a node with $d$ edges emanating, and
        fuzzy outlines group nodes that occur within the same expectation.  

        We may pair together the loose ends of the above (and higher-degree
        analogues) to obtain
        \emph{diagrams}.\footnote{
            A diagram's colors and geometric layout lack meaning: we
            {\color{moor} color} only for convenient reference, e.g.\ to
            a diagram's ``green nodes''.  Only the topology of a diagram
            --- not its size or angles --- appear in our theory.
        }
        E.g., we may join
        $
            C = \sdia{MOOc(01)(0-1)}
        $
        with
        $
            H = \sdia{MOO(0)(0-0)}
        $
        to get
        $
            \sdia{c(01-2)(02-12)}
        $.
        As another example, we may join two copies of
        $
            G = \sdia{MOO(0)(0)}
        $
        with two copies of
        $
            H = \sdia{MOO(0)(0-0)}
        $
        to get
        $
            \sdia{c(0-1-2-3)(01-12-23)} 
        $.
        
        %
        \begin{exm} \label{exm:first}
            Does non-Gaussian noise affect SGD?
            Specifically, since the skew $S$ measures non-gaussianity, let's
            compute how $S$ affects test loss. The recipe is to identify the
            fewest-edged diagrams containing $S = \sdia{MOOc(012)(0-1-2)}$.  In
            this case, there is one fewest-edged diagram ---
            $\sdia{c(012-3)(03-13-23)}$; it results from joining $S$ with
            $J=\sdia{MOO(0)(0-0-0)}$.  To evaluate a diagram, we multiply its
            components (here, $S, J$) with exponentiated $\eta H$'s, one for
            each edge:
            \begin{align} \label{eqn:nongauss}
                -\frac{\eta^3}{3!}
                \sum_{\mu\nu\lambda}
                    S_{\mu\nu\lambda}
                    \frac{
                        1 - \exp(-T\eta (H_{\mu\mu} + H_{\nu\nu} + H_{\lambda\lambda}))
                    }{
                        \eta (H_{\mu\mu} + H_{\nu\nu} + H_{\lambda\lambda})
                    }
                    J_{\mu\nu\lambda}
            \end{align}
            This is $S$'s leading order contribution to SGD's test loss
            written in an eigenbasis of $\eta H$.
        \end{exm}
        \begin{rmk}
            For large $T$ and isotropic $\eta H$, (\ref{eqn:nongauss}) becomes
            $
                - (\eta^3/3!)
                \sum_{\mu\nu\lambda}
                    S_{\mu\nu\lambda} J_{\mu\nu\lambda} / 3 \eta \wabs{H}
            $.
            Since $J = \nabla H$, $J / \wabs{H}$ measures the relative change
            in curvature $H$ w.r.t.\ $\theta$.  So non-gaussian noise affects
            SGD proportion to the logarithmic derivative of curvature.
        \end{rmk}
        In general, each diagram intuitively represents the net effect of a
        certain combination of gradients ($G$), noise ($C, S, \cdots$) and
        curvature ($H, J, \cdots$). 

%==============================================================================
%=====  BACKGROUND AND NOTATION  ==============================================
%==============================================================================

\subsection{Background, Notation, and Assumptions} \label{sect:background}
       
    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Tensor Conventions  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    %\subsection{Tensor conventions}
        We sometimes implicitly sum repeated Greek indices: if a covector $A$
        and a vector $B$\footnote{
            Vectors/covectors are also called column/row vectors.
        } have coefficients $A_\mu, B^\mu$, then 
        $
            A_\mu B^\mu
            \triangleq
            \sum_\mu A_\mu \cdot B^\mu
        $.
        %To expedite dimensional analysis,
        We regard the learning rate as an
        inverse metric $\eta^{\mu\nu}$ that converts gradient covectors to
        displacement vectors \citep{bo13}.  We use the learning rate
        $\eta$ to raise indices: e.g.,
        $
            H^{\mu}_{\lambda}
            \triangleq
            \eta^{\mu\nu} H_{\nu\lambda}
        $ and
        $
            C^{\mu}_{\mu}
            \triangleq
            \sum_{\mu \nu} \eta^{\mu\nu} \cdot C_{\nu\mu}
        $.
        Though $\eta$ is a tensor, we may still define $o(\eta^d)$: a quantity
        $q$ \emph{vanishes to order $\eta^d$} when $\lim_{\eta\to 0} q/p(\eta)
        = 0$ for some homogeneous degree-$d$ polynomial $p$.

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  The Loss Landscape  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    %\subsection{Loss landscape}

        %-------------  the landscape  ----------------------------------------

        We fix a loss function $l:\Mm\to\RR$ on a space $\Mm$ of weights.  We
        fix a distribution $\Dd$ from which unbiased estimates of $l$ are
        drawn.  We write $l_x$ for a generic sample from $\Dd$ and $(l_n: 0\leq
        n<N)$ for a training sequence drawn i.i.d.\ from $\Dd$.  We refer both
        to $n$ and to $l_n$ as \emph{training points}.  We assume Appendix
        {\color{red} FILL IN}'s hypotheses, e.g.\ that $l, l_x$ are
        analytic and that moments exist.
        %
        %-------------  specialization to a common case  ----------------------
        %
        E.g., our theory models $\tanh$ networks with cross entropy loss on
        bounded data --- with arbitrary weight sharing, skip connections, soft
        attention, dropout, and weight decay.
        
    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Names of SGD Parameters  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    %\subsection{SGD terminology}
        %SGD performs $\eta$-steepest descent on the estimates $l_n$.
        %
        Our general theory describes SGD with any number
             $N$ of training points,
             $T$ of updates, and 
             $B$ of points per batch.
        SGD then runs $T$ many updates (i.e. $E=TN/B$ epochs, i.e. $M=T/N$
        updates per point)
        $
            \theta^\mu
            \coloneqq
            \theta^\mu -
            \eta^{\mu\nu} \nabla_\nu
                \sum_{n\in \Bb_t} l_n(\theta) / B
        $,
        where $\Bb_t$ is the $t$th batch.  Our paper's body --- but not
        appendices --- will assume \textbf{SGD has $E=B=1$ and GD has $T=B=N$}
        unless otherwise stated.

%==============================================================================
%    RELATED WORK    
%==============================================================================

\subsection{Related Work} \label{sect:related}

    %--------------------------------------------------------------------------
    %           Analyzing Overfitting; Relevance of Optimization; SDE Errs  
    %--------------------------------------------------------------------------

    Several research programs treat the overfitting of SGD-trained networks
    \citep{ne17a}.  E.g., \cite{ba17} controls the Rademacher complexity of
    deep hypothesis classes, leading to optimizer-agnostic generalization
    bounds.  Yet SGD-trained networks generalize despite their ability to
    shatter large sets \citep{zh17}, so generalization must arise from not only
    architecture but also optimization \citep{ne17b}.  Others approximate
    SGD by SDE to analyze implicit regularization (e.g.\ \cite{ch18}), but,
    per \cite{ya19a}, such continuous-time analyses cannot treat SGD noise
    correctly.
    %
    %%--------------------------------------------------------------------------
    %%           We Extend Dan's Approach                     
    %%--------------------------------------------------------------------------
    %
    We avoid these pitfalls by Taylor expanding around $\eta=0$ as in
    \cite{ro18}; unlike that work, we generalize beyond order $\eta^1$ and
    $T=2$.
    
    %--------------------------------------------------------------------------
    %           Phenomenology of Rademacher Correlates such as Hessians
    %--------------------------------------------------------------------------

    Our theory is vacuous for large $\eta$.  Other analyses treat
    large-$\eta$ learning phenomenologically, whether by finding empirical
    correlates of gen.\ gap \citep{li18}, by showing that \emph{flat} minima
    generalize (\cite{ho17}, \cite{ke17}, \cite{wa18}), or by showing that
    \emph{sharp} minima generalize (\cite{st56}, \cite{di17}, \cite{wu18}).
    Our theory reconciles these clashing claims.
    
    %--------------------------------------------------------------------------
    %           Our Work vs Other Perturbative Approaches            
    %--------------------------------------------------------------------------

    Prior work analyzes SGD perturbatively: \cite{dy19} perturb in inverse
    network width, using 't Hooft diagrams to correct the Gaussian Process
    approximation for specific deep nets.  Perturbing to order $\eta^2$,
    \cite{ch18} and \cite{li17} assume uncorrelated Gaussian noise, so they
    cannot describe SGD's gen\. gap.  We use Penrose diagrams to compute test
    losses to arbitrary order $\eta$.  We allow for correlated, non-Gaussian
    noiseand thus \emph{any} smooth architecture.  E.g., we do not assume
    information-geometric relationships between $C$ and
    $H$,
    \footnote{
        Disagreement of $C$ and $H$ is typical in modern learning \citep{ro12,
        ku19}.
    }
    so we may model VAEs. 

%==============================================================================
%=====  DIAGRAM CALCULUS FOR SGD  =============================================
%==============================================================================

\section{Theory, Specialized to $E=B=1$ SGD's Test Loss} \label{sect:calculus}

    %\subsection{Diagrams, embeddings, and re-summed values}
        \begin{wraptable}{r}{5cm}
            \begin{spacing}{0.8}
            \begin{tabular}{p{5cm}}
                \textbf{Examples}:
                The diagrams
                $\sdia{c(0-1)(01)}$, $\sdia{c(012-3)(03-13-23)}$ each have $2$
                parts; $\sdia{c(0-12-3)(03-13-23)}$, $\sdia{c(01-2-3)(02-13-23)}$
                each have $3$.
                %
                Corollaries \ref{cor:overfit}, \ref{cor:epochs},
                \ref{cor:batch} do not assume $E=B=1$, so they feature
                $\sdia{c(01)(01)}$ and $\sdia{c(01-2)(01-12)}$, generalized
                diagrams that violate the path condition. 
                %
                The diagrams $\sdia{c(0-1-2)(02-12)}$, $\sdia{c(01-2)(01-12)}$
                are irreducible; due to their green nodes,
                $\sdia{c(0-1-2)(01-12)}$, $\sdia{c(01-2-3)(03-12-23)}$ are not.
            \end{tabular}
            \end{spacing}
        \end{wraptable}

        A \emph{diagram} is a finite rooted tree equipped with a partition
        of its nodes obeying the \emph{path condition}: no path from leaf to
        root may encounter any part more than once.
        We specify the root by drawing it rightmost.  We draw the parts of 
        the partition by grouping the nodes within each part via fuzzy
        outlines. 
        %
        A diagram is \emph{irreducible} when each of its degree-$2$ nodes is in
        a part of size one.
        %
        An \emph{embedding} $f$ of a diagram $D$ is an injection from the
        diagram's parts to (integer) times $0 \leq t \leq T$ that sends the
        root to $T$ and such that, for each path from leaf to root, the
        corresponding sequence of times is increase.  E.g., $f$ might send
        $\sdia{c(01-2-3)(02-13-23)}$'s red part to $t=3$ and its green part to
        $t=4$, but not vice versa.
        %
        {\color{red} Let $\wabs{\Aut_f(D)}$ count the graph automorphisms of
        $D$ that commute with $f$.}

        %%%%%%%%%

        Up to unbiasing terms,\footnote{
            E.g., we actually define $\sdia{MOOc(01)(0-1)}$ to be the cumulant
            $C = \ex{(\nb\lx - G)^2}$, not the moment $\ex{(\nb\lx)^2}$.
            This centering is routine (see {\color{red} APPENDIX}), tedious to
            keep notating, and un-germane, so we ignore it.
        }
        the \emph{re-summed value} $\rvalue_f(D)$ is constructed as follows.
        %
        \textbf{Node rule}: insert a factor a $\nabla^d l_x$for each degree $d$
        node. 
        %
        \textbf{Edge rule}: for each edge whose endpoints $f$ sends to times
        $t, t^\prime$, insert a factor of $K^{\wabs{t^\prime-t}-1} \eta$
        where $K \triangleq (I-\eta H)$.
        %
        \textbf{Outline rule}: group the nodes in each part within expectation
        brackets $\expc_x{}$.
        %
        %
        E.g., if $f$ maps $\sdia{c(012-3)(03-13-23)}$'s red part to time $t =
        T-\Delta t$, then (the red part gives $S$; the green part, $J$):
        $$
            \rvalue_f\wrap{\sdia{c(012-3)(03-13-23)}} = 
            S_{\mu\lambda\rho}
                (K^{\Delta t-1}\eta)^{\mu\nu}
                (K^{\Delta t-1}\eta)^{\lambda\sigma}
                (K^{\Delta t-1}\eta)^{\rho\pi}
            J_{\nu\sigma\pi}
        $$
        After integrating this expression per Remark \ref{rmk:integrate}, we
        recover Example \ref{exm:first}.

    \subsection{Main result}

    %\subsection{Recipe for SGD's expected test loss}
        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        %~~~~~  Recipe for Test Loss  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        Theorem \ref{thm:resum} expresses SGD's test loss as a sum over
        diagrams.  A diagram with $d$ edges scales as $O(\eta^d)$, so the
        following is a series in $\eta$.  We will truncate the series to small
        $d$, thus focusing on few-edged diagrams and easing the combinatorics
        of embeddings.
        \begin{thm}[Special Case] \label{thm:resum}
            For any $T$: for $\eta$ small enough, SGD has expected test loss
            \begin{equation*} \label{eq:resum}
                \sum_{\substack{\text{irreducible} \\ \text{diagrams}~D }}
                \sum_{\substack{\text{embeddings} \\ f~\text{of}~D}}
                \frac{(-1)^{|\edges(D)|}}{\wabs{\Aut_f(D)}}
                \,
                {\rvalue_f}(D)
            \end{equation*}
        \end{thm}

        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        %~~~~~  Convergence  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
        \begin{thm}[Long-Term Behavior near a Local Minimum] \label{thm:converge}
            If $\theta_\star$ locally minimizes $l$ and for some positive form
            $Q$, $Q < \nabla\nabla l_x(\theta_\star)$ for all $x$, then when we
            initialize SGD sufficiently close to $\theta_\star$, the
            $d$th-order truncation of Theorem \ref{thm:resum} converges as $T$
            diverges.
        \end{thm}

        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        %~~~~~  Simplifications  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
        \begin{rmk} \label{rmk:integrate}
            We may approximate sums by integrals and $(I-\eta H)^t$ by $\exp(-
            \eta H t)$, reducing to a routine integration of exponentials
            at the cost of an error factor $1 + o(\eta)$.
        \end{rmk}
      
    \subsection{Insights from the Formalism}
    
        \subsubsection{SGD descends on a $C$-smoothed landscape and prefers
        minima flat w.r.t.\ $C$.}
    
            \begin{cor}[Computed from $\sdia{(01-2-3)(02-12-23)}$] \label{cor:entropic}
                Initialized at a test minimum, and run for long times $T \gg
                1/\eta H$, SGD drifts with an expected time-averaged velocity
                of
                $$
                    v^\lambda
                    =
                    \frac{\eta^3}{T}
                    \sum_{\mu\nu}
                        C_{\mu\nu}
                        \frac{1}{\eta (H_{\mu\mu} + H_{\nu\nu})}
                        J_{\mu\nu\lambda}
                        \frac{1}{H_{\lambda\lambda}}
                    + o(\eta^2)
                    ~~~~~
                    ~~~~~
                    ~~~~~
                    \text{in an eigenbasis for}~\eta H
                $$
            \end{cor}
            
            Intuitively, $D = \sdia{c(01-2-3)(02-12-23)}$
            contains a subdiagram $\sdia{c(01-2)(02-12)} = (K\eta)^2 CH$.
            By a routine check, $CH+o(\eta^2)$ is the loss increase upon
            convolving $l$ with a $C$-shaped Gaussian.  Since
            $D$ connects the subdiagram to {\color{red} to the test
            measurement} via $1$ edge, it couples $CH$ to $l$'s linear part, so
            it represents a displacement of $\theta$ away from high $CH$.  In
            short, \emph{SGD descends on a covariance-smoothed landscape}.
            That is, SGD prefers from among a valley's minima those that are
            flat w.r.t.\ $C$.  Figure \ref{fig:cubicandspring} (left)
            illustrates this intuition.
    
            \cite{ya19b} reports a small-$T$ version of this result that
            scales with $\eta^3$.  Meanwhile, Corollary \ref{cor:entropic}'s
            large-$T$ analysis scales with $\eta^2$.  Our analysis integrates
            the noise over many updates, hence amplifying the contribution of
            $C$, and experiments verify this scaling law.
            %
            We do not make \cite{we19b}'s assumptions of thermal equilibrium,
            fast-slow mode separation, or constant covariance.  This generality
            reveals novel dynamics: that the velocity field above is
            generically non-conservative (Section \ref{subsect:entropic}).
      
        \subsubsection{Both flat and sharp minima overfit less} \label{subsect:curvature-and-overfitting}
            \begin{cor}[Computed from $\sdia{(01-2)(02-12)}$ and $\sdia{(01)(01)}$]\label{cor:overfit}
                Initialize GD at a test minimum.  The test-loss-increase and the
                generalization-gap (test minus train loss) due to training are,
                with errors $o(\eta^2)$ and $o(\eta^1)$:
                $$
                    \frac{C_{\mu\nu}}{2N} ~
                        \wrap{(I - \exp(-\eta T H))^{\otimes 2}}^{\mu\nu}_{\rho\lambda}
                        \wrap{H^{-1}}^{\rho\lambda}
                    ~~~~~ \text{and} ~~~~~
                    \frac{C_{\mu\nu}}{N} ~
                        \wrap{I - \exp(-\eta T H)}^{\nu}_{\lambda}
                        \wrap{H^{-1}}^{\lambda\mu}
                $$
                %In contrast to the later-mentioned Takeuchi estimate, this does not
                %diverge as $H$ shrinks.
            \end{cor}
            This gen.\ gap tends with large $T$ 
            to $C_{\mu\nu}(H^{-1})^{\mu\nu}/N$.  For maximum
            likelihood (ML) estimation in well-specified models near the ``true''
            minimum, $C=H$ is the Fisher metric, so we recover AIC:
            $(\textnormal{number of parameters})/N$.  Unlike AIC, our more general
            expression is descendably smooth, may be used with MAP or ELBO tasks
            instead of just ML, and does not assume a well-specified model.
    
            \begin{figure}[h!]
                \centering
                \plotmooh{diagrams/entropic-force-diagram}{}{0.32\columnwidth} 
                \plotmooh{diagrams/sharp}{}{0.31\columnwidth}
                \caption{
                    {\bf Re-summation reveals novel phenomena.}
                    {\bf Left}:
                        The entropic force mechanism: gradient noise induces a flow
                        toward minima  \emph{with respect to to the
                        covariance}.  Though our analysis assumes neither thermal
                        equilibrium nor fast-slow mode separation, we label ``fast
                        and slow directions'' to ease comparison
                        with \cite{we19b}.  Here, red densities denote
                        the spread predicted by a re-summed $C^{\mu\nu}$, and
                        the spatial variation of curvature corresponds to
                        $J_{\mu\nu\lambda}$. 
                    {\bf Right}:
                        Noise structure determines how curvature affects
                        overfitting.  Geometrically, for (empirical risk
                        minimization on) a vector-perturbed landscape, small
                        Hessians are favored (top row), while for
                        covector-perturbed landscapes, large Hessians are favored
                        (bottom row).  Corollary \ref{cor:overfit} shows how the
                        implicit regularization of fixed-$\eta T$ descent interpolates 
                        between the two rows.
                }
                \label{fig:cubicandspring}
            \end{figure}
    
        \subsubsection{Epochs and batch size} \label{subsect:epochs-batch}
    
            \begin{cor}[Epoch Number] \label{cor:epochs}
                To order $\eta^2$, $M=1$ SGD with learning rate $\eta$ has 
                $
                     \wrap{\frac{M-1}{M}}\wrap{\frac{B+1}{B}}\wrap{\frac{N}{2}}
                     \wrap{\nabla_\mu C^{\nu}_{\nu}} G^\mu / 2
                $
                less test loss than $M=M$ SGD with learning rate $\eta/M$.
            \end{cor}
        
            %Considering $\sdia{c(01-2)(01-12)}$:
            \begin{cor}[Batch Size] \label{cor:batch}
                The expected test loss of pure SGD is, to order $\eta^2$,
                less than that of pure GD by
                $
                      \frac{M(N-1)}{2} ~
                      \wrap{\nabla_\mu C^{\nu}_{\nu}} G^\mu / 2
                $.
                Moreover, if $\hat{C}$ is a smooth unbiased estimator of $C$, then
                GD on a modified loss 
                $
                    \tilde l_n = l_n +
                        \frac{N-1}{4N} ~
                        \hat{C}_\nu^\nu(\theta)
                $
                has an expected test loss that agrees with SGD's to second order.
                We call this method GDC.
            \end{cor}
    
        \subsubsection{Non-Gaussian noise affects SGD but not SDE}
    
            Stochastic Differential Equations (SDE: see \cite{li18}) are a popular
            theoretical approximation to SGD, but SDE and SGD differ in several
            ways.  For instance, the inter-epoch noise correlations in multi-epoch
            SGD measurably affect SGD's final test loss (Corollary
            \ref{cor:epochs}), but SDE assumes uncorrelated gradient updates.  Even
            if we restrict to single-epoch SDE, differences arise due to time
            discretization and non-gaussian noise. 
            %
            \begin{cor}[SGD Differs from ODE, SDE] \label{cor:vsode}
                The test loss of single-epoch, singleton-batch SGD deviates
                from that of ODE and SDE by
                $
                    \frac{T}{2} ~ C_{\mu\nu} H^{\mu\nu} + o(\eta^2)
                $.
                The deviation from SDE due to non-Gaussian noise is
                $
                    - (T/6) \sdia{c(012-3)(03-13-23)}
                    + o(\eta^3)
                    =
                    - (T/6) S_{\mu\nu\lambda} J^{\mu\nu\lambda} 
                    + o(\eta^3)
                $.\footnote{
                    This expression differs from the more exact expression of
                    Example \ref{exm:first} because here we use Remark
                    \ref{rmk:unresum}'s substitution.  One may check that the two
                    expressions agree to leading order.
                }
            \end{cor}
            %
            For finite $N$, this Corollary separates SDE from SGD.  Conversely, as
            $N\to\infty$ with $\eta N$ fixed and $C$ scaling with $\sqrt{N}$, SGD
            converges to SDE, but generalization and optimization respectively
            become trivial and computationally intractable.

%==============================================================================
%    EXPERIMENTS AND APPLICATIONS
%==============================================================================

\section{Applying the Theory}

\subsection{Experiments}

    We run experiments whose rejection of the null hypothesis is so drastic as
    to be visually clear.  E.g., in Figure \ref{fig:thermoandtak},
    \citep{ch18} predicts a velocity of $0$ while we predict a velocity of
    $\eta^2/6$.  
    %
    We use \texttt{I} bars and \texttt{+} signs to mark a 95\% confidence
    interval based on the standard error of the mean.  Appendix
    \ref{sect:landscape} lists architectures and procedure.

    \subsubsection{High-$C$ regions repel few-epoch and small-batch SGD}
        %----------------------------------------------------------------------
        %       Vanilla SGD; Epochs and Overfitting         
        %----------------------------------------------------------------------
        We test Theorem \ref{thm:resum}  on smooth convnets for CIFAR-10 and
        Fashion-MNIST.  Our order $\eta^3$ predictions, simplified via Remark
        \ref{rmk:unresum}, agree with experiment up to $\eta T \approx 10^0$
        (Figure \ref{fig:vanilla}, left).  Also, Corollary \ref{cor:epochs}
        correctly predicts the effect of multi-epoch training (Appendix
        \ref{sect:figures}) for $\eta T \approx 10^{-1/2}$.
        %----------------------------------------------------------------------
        %       Emulating Small Batches with Large Ones     
        %----------------------------------------------------------------------
        {\color{red} FIGURE} tests Corollary \ref{cor:batch}, supporting our
        claim that high-$C$ regions repel SGD more than GD.  This is
        significant because $C$ controls the rate at which the gen\. gap (test
        minus train loss) grows (Corollary \ref{cor:overfit}; {\color{red}
        FIGURE}).
        Overall, these tests verify
        that our proofs hide no mistakes of proportionality or sign.  
       
        \begin{figure}[h!] 
            \centering
            \pmoo{3.0cm}{new-test-0}        \pmoo{3.0cm}{new-big-bm-new}          \pmoo{3.0cm}{new-thermo-linear-screw}
            \pmoo{3.0cm}{rebut-test-1-T100} \pmoo{3.0cm}{rebut-gen-cifar-lenet-4} \pmoo{3.0cm}{new-tak}
            \caption{
                {\bf Left: Perturbation models SGD for small $\eta T$.}
                Fashion-MNIST convnet's test loss vs learning rate;
                un-re-summed predictions.
                %
                \protect\ofsix{0}: For all init.s tested ($1$ shown,
                $11$ unshown), our degree-$3$ prediction agrees with experiment
                through $\eta T \approx 10^0$, corresponding to a decrease
                in $0\mbox{-}1$ error of $\approx 10^{-3}$.
                %
                \protect\ofsix{1}: For large $\eta T$, our predictions
                break down.  Here, the order-$3$ prediction holds until the
                $0\mbox{-}1$ error improves by $5\cdot 10^{-3}$.
                %%%
                %%%
                \newline
                {\bf Center: $C$ controls gen.\ gap and distinguishes GD
                from SGD.}
                %
                \protect\ofsix{2}: With equal-scaled axes, this plot shows that
                GDC matches SGD (small vertical varianec) better than GD
                matches SGD (large horizontal variance) in test loss for a
                range of $\eta$ ($\approx 10^{-3}-10^{-1}$) and
                init.s\ (zero and several Xavier-Glorot trials) for
                logistic regression and convnets.  Here, $T=10$. 
                %
                \protect\ofsix{3}: CIFAR-10 generalization gaps.  For all
                init.s tested ($1$ shown, $11$ unshown), the
                degree-$2$ prediction agrees with experiment through $\eta T
                \approx 5\cdot 10^{-1}$.
                %%%
                %%%
                \newline
                {\bf Right: Re-summed predictions excel even for large $\eta T$.}
                %
                \protect\ofsix{4}: On \Archimedes, SGD travels the valley of
                global minima in the positive $z$ direction.  Since $H$ and $C$
                are bounded and the effect appears for all small $\eta$, the
                effect is not a pathology of well-chosen learning rate or
                divergent noise.  The net displacement of $\approx 10^{1.5}$
                well exceeds the $z$-period of $2\pi$. 
                %
                \protect\ofsix{5}: For \MeanEstimation\ with fixed $C$ and a
                range of $H$s, initialized at the truth, the test losses after
                fixed-$T$ optimization are smallest for very small and very
                large curvatures.  As predicted: both sharp and flat minima
                overfit less.
            }
            \label{fig:vanilla}
        %    \label{fig:batchandgen}
        %    \label{fig:thermoandtak}
        \end{figure}


    %--------------------------------------------------------------------------
    %           Thermodynamic Engine                        
    %--------------------------------------------------------------------------

    \subsubsection{Minima that are flat with respect to $C$ attract SGD} \label{subsect:entropic}
        To test Corollary \ref{cor:entropic}, we construct a counter-intuitive
        loss landscape wherein SGD steadily moves in a direction of $0$ test
        gradient.  Our mechanism differs from that of \cite{ch18}'s approximate
        analysis, which in this case predicts a velocity of $0$.%
        \footnote{
            Indeed, our velocity is $\eta$-perpendicular to the image of $\eta
            C$.
        }
        Specifically, the \Archimedes\ landscape has
        %
        weights     $\theta = (u,v,z) \in \RR^3$,
        %
        data points $x \sim \Nn(0, 1)$,
        %
        and loss:
        %
        $
            l_x(w)
            \triangleq
            \frac{1}{2} H(\theta) + x \cdot S(\theta)
        $,
        %
        where $H(\theta) = u^2 + v^2 + (\cos(z) u + \sin(z) v)^2$
        %
        and   $S(\theta) = \cos(z-\pi/4) u + \sin(z-\pi/4) v$.
        %
        Note that for fixed $z$, $H$ is quadratic and $S$ is linear.  Also,
        since $x \sim \Nn(0,1)$, $x S(\theta)$ has expectation $0$.
        %
        \Archimedes\ thus has valley of global minima on the line $x=y=0$. 
        For SGD initialized at $\theta=0$, Corollary \ref{cor:entropic}
        predicts a $z$-velocity of $+\eta^2/6$ per timestep.  The prediction
        agrees with experiment even as the net displacement exceeds the 
        the landscape's natural length scale of $2\pi$ (Figure
        \ref{fig:thermoandtak}, left).

    %--------------------------------------------------------------------------
    %           Sharp vs Flat Minima                        
    %--------------------------------------------------------------------------

    \subsubsection{Sharp and flat minima both overfit less than medium minima} \label{subsect:overfit}

        Prior work finds both that \emph{sharp} minima overfit less (for, $l^2$
        regularization sharpens minima) or that \emph{flat} minima overfit less
        (for, flat minima are robust to small displacements).  In fact,
        generalization's relationship to curvature depends on the landscape's
        noise structure (Corollary \ref{cor:overfit}, Figure
        \ref{fig:cubicandspring}, right).
        
        To combat overfitting, we may add Corollary \ref{cor:overfit}'s
        expression for gen.\ gap to $l$.  Unlike AIC, which it subsumes, this
        regularizer is continous and thus liable to descent.  We call this
        regularizer \emph{STIC} ({\color{red} Appendix}).  By descending on
        STIC, we may tune smooth hyperparameters such as $l_2$ regularization
        coefficients.  Experiments on \MeanEstimation recommend STIC for model
        selection when $H \ll C/N$ as in the noisy, small-$N$ regime
        {\color{red} Appendix} .  Since matrix exponentiation takes time cubic
        in dimension, exact STIC is most useful for small models on noisy,
        limited data.

%==============================================================================
%    CONCLUSION      
%==============================================================================

\subsection{Conclusion} \label{sect:concl}

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Summarize Contributions  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    We presented a diagram-based method for studying stochastic optimization on
    short timescales or near minima.  Our theory yields several corollaries. 
    %
    Analyzing $\sdia{c(01-2)(02-12)}$, we show that \textbf{flat and sharp
    minima both overfit} less than medium minima.  Intuitively, flat minima are
    robust to vector noise, sharp minima are robust to covector noise, and
    medium minima robust to neither.  We thus propose a
    a smooth analogue of AIC enabling gradient-based hyperparameter tuning.
    %
    Inspecting $\sdia{c(01-2-3)(02-12-23)}$, we extend \cite{we19b} to
    nonconstant, nonisotropic covariance to reveal that \textbf{SGD descends on
    a landscape smoothed by the current covariance $C$}.  As $C$ evolves, the
    smoothed landscape evolves, resulting in non-conservative dynamics.
    %
    Examining $\sdia{c(01-2)(01-12)}$, we show that \textbf{GD may emulate
    SGD}, as conjectured by \cite{ro18}.  This is significant because, while
    small batch sizes can lead to better generalization \citep{bo91}, modern
    infrastructure increasingly rewards large batch sizes \citep{go18}.  

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Anticipate Criticism of Limitations  ~~~~~~~~~~~~~~~~~~~~~~~~~~

    %\subsection{Consequences}

        Corollaries \ref{cor:entropic} and \ref{cor:overfit} together offer
        insight into SGD's success in training deep networks: SGD senses
        curvature, and curvature controls generalization.

        Since our predictions depend only on loss data near initialization,
        they break down after the weight moves far from initialization.  Our
        theory thus best applies to small-movement contexts, whether for long
        times (large $\eta T$) near an isolated minimum or for short times
        (small $\eta T$) in general.  E.g., our theory might especially
        illuminate meta-learners such as MAML \citep{fi17}, which seek models
        initialized near minima and tunable to new data via few updates.

        Much as meteorologists understand how warm and cold fronts interact
        despite long-term forecasting's intractability, we quantify the
        counter-intuitive dynamics governing each short-term interval of SGD's
        trajectory.  Equipped with our theory, practitioners may now
        refine intuitions (e.g.\ that SGD descends on the train loss) to
        account for noise.
       
%==============================================================================
%    CODA               
%==============================================================================

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Broader Impacts  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\section*{Broader Impacts}

    Though machine learning has the long-term potential for vast improvements
    in world-wide quality of life, it is today a source of enormous carbon
    emissions \citep{st19}.  Our analysis of SGD may lead to a reduced carbon
    footprint in three ways. 
     
    First, Section \ref{subsect:epochs-batch} shows how to modify the loss
    landscape so that large-batch GD enjoys the stochastic regularizing
    properties of small-batch SGD, or (symmetrically) so that small-batch SGD
    enjoys the stability of large-batch GD.  By unchaining the effective batch
    size from the actual batch size, we raise the possibility of training
    neural networks on a wider range of hardware than currently practical.  For
    example, asynchronous concurrent small-batch SGD (e.g., \cite{ni11}) might
    require less inter-GPU communication and therefore less power.
     
    Second, Section \ref{sect:concl} discusses an application to meta-learning,
    which has the potential to decrease the per-task sample complexity and
    hence carbon footprint of modern ML.
     
    Third, the generalization of AIC developed in  Sections
    \ref{subsect:curvature-and-overfitting} and \ref{subsect:overfit} permits
    certain forms of model selection by gradient descent rather than brute
    force search.  This might drastically reduce the energy consumed during
    model selection.

    That said, insofar as our theory furthers practice, it may instead
    contribute to the rapidly growing popularity of GPU-intensive learning,
    thus negating the aforementioned benefits and accelerating climate change.

    More broadly, this paper analyzes optimization in the face of uncertainty.
    As ML systems deployed today must increasingly address \emph{user privacy},
    \emph{pedestrian safety}, and \emph{dataset diversity}, it becomes
    important to recognize that training sets and test sets
    differ.  Toward this end, theoretical work relating to non-Gaussian noise
    may assist practitioners in building provably non-discriminatory, safe, or
    private models (e.g., \cite{dw06}).  By quantifying how correlated,
    non-Gaussian gradient noise affects descent-based learning, this paper
    contributes to such broader theory.

    %Authors are required to include a statement of the broader impact of their
    %work, including its ethical aspects and future societal consequences.
    %Authors should discuss both positive and negative outcomes, if any. For
    %instance, authors should discuss a) who may benefit from this research, b)
    %who may be put at disadvantage from this research, c) what are the
    %consequences of failure of the system, and d) whether the task/method
    %leverages biases in the data. If authors believe this is not applicable to
    %them, authors can simply state this.

    %By connecting to physics, we strengthen the bridge between
    %two vibrant research communities.

    %Highlight both benefits and risks from your research. The NeurIPS
    %requirement asks that “Authors should take care to discuss both
    %positive and negative outcomes.” Systematically doing so will help us
    %push through the various biases, personal and institutional, towards
    %overly positive or overly negative assessments.

    %Highlight uncertainties. Foreseeing the impacts of research,
    %especially basic research, is notoriously difficult. We recommend
    %acknowledging your uncertainties, while also not letting it stop you
    %from reflecting about impact.

    %Focus on tractable, neglected, and significant impacts. Scientific
    %research tends to have a bewildering array of potential impacts, more
    %so as the research is more foundational or the considered impacts are
    %more long term.  It will be infeasible to consider them all.

    %Think about impacts even for theoretical work. Theoretical work does
    %have downstream impact — that’s after all a motivation for much
    %theoretical work — and so we encourage researchers to make an attempt,
    %perhaps reflecting on their subfield more broadly.
            
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Acknowledgements  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\section*{Acknowledgements}

    %\begin{ack}
        We feel deep gratitude to
            \textsc{Sho Yaida},
            \textsc{Dan A. Roberts}, and
            \textsc{Josh Tenenbaum}
        for posing some of the problems this work resolves and for their
        patient guidance.  We appreciate the generosity of
            \textsc{Andy Banburski},
            \textsc{Ben R. Bray},
            \textsc{Jeff Lagarias}, and
            \textsc{Wenli Zhao}
        in critiquing our drafts.
        Without the encouragement of
            \textsc{Jason Corso},
            \textsc{Chloe Kleinfeldt},
            \textsc{Alex Lew}, 
            \textsc{Ari Morcos}, and
            \textsc{David Schwab},
        this paper would not be.
        Finally, we thank our anonymous reviewers for inspiring an improved
        presentation.
        %
        This work was funded in part by MIT's Jacobs Presidential Fellowship
        and in part by Facebook AI Research.
    %\end{ack}
        
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  References  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%\section*{References}
    \bibliographystyle{plainnat}
    \bibliography{perturb}

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Diagrams as Graphs  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    \subsection{Diagrams and embeddings}

        %Though a rough, intuitive understanding of concepts such as
        %\emph{diagram} suffices for absorbing this paper's main results, the
        %following definitions may help the reader who wishes to follow our
        %mathematics closely.

        \begin{defn}[Diagrams] \label{dfn:diagrams}
            A \emph{diagram} is a finite rooted tree equipped with a partition
            of its nodes.  We draw the tree using thin edges.  By convention,
            we draw each node to the right of its children; the root is thus
            always rightmost.  We draw the partition by connecting the nodes
            within each part via fuzzy ties.  For example,
            $\sdia{(012-3)(03-13-23)}$ has $2$ parts.
            %
            We insist on using as few fuzzy ties as possible so that, if $d$
            counts edges and $c$ counts ties, then $d+1-c$ counts the parts
            of the partition. 
            %
            There may be multiple ways to draw a single diagram, e.g.
            $\sdia{(01-23)(03-13-23)} = \sdia{(02-13)(03-13-23)}$. 
        \end{defn}

        \begin{defn}[Spacetime] 
            The \emph{spacetime} associated with an SGD run is the set of pairs
            $(n,t)$ where the $n$th datapoint participates in the $t$th
            gradient update.  Spacetimes thus encode batch size, training
            set size, and epoch number.
        \end{defn}

        \begin{defn}[Embedding Diagrams into Spacetime]
            An \emph{embedding} of a diagram $D$ into a spacetime is an
            assignment of $D$'s non-root nodes to pairs $(n,t)$ such that each
            node occurs at a time $t^\prime$ strictly after each of its
            children and such that two nodes occupy the same row $n$ if and
            only if they inhabit the same part of $D$'s partition.
        \end{defn}

        To visualize embeddings, we draw the $(n,t)$ pairs of a space-time as
        shaded cells in an $N\times T$ grid.  A diagram embedding is then an
        assignment of nodes to shaded cells.  The $t<t^\prime$ constraint 
        forbids intra-cell edges (Figure \ref{fig:spacetimes} left), and we may
        interpret each edge as an effect of the past on the future (right).

        \begin{defn}[A Diagram's Un-resummed Value]
            The \emph{un-resummed value} of a diagram $D$ is the product of the
            values of each part $p$ in its partition.  The value
            of a part $p$ with $\wabs{p}$ many nodes is the expectation
            $\expc_x\wasq{(\nabla l_x(\theta))^{\wabs{p}}}$.  The edges of
            $D$'s tree indicate how to multiply the values of these
            parts: each edge indicates a contraction.  For instance,
            since the training points are independent:
            $$
                \sdia{(01-2-3)(02-12-23)}
                    \triangleq
                \expc_{
                    {\color{moor}n},
                    {\color{moog}n^\prime},
                    {\color{moob}n^{\prime\prime}}
                }\wasq{
                    (\nabla_\mu l_{\color{moor}n})
                    (\nabla_\nu l_{\color{moor}n})
                    (\nabla^\mu \nabla^\nu \nabla_\lambda l_{\color{moog}n^\prime})
                    (\nabla^\lambda l_{\color{moob}n^{\prime\prime}})
                }%(\theta_0)
            $$
            Implicit in the three raised indices are three factors of $\eta$.
            We denote $D$'s un-resummed value by $\uvalue(D)$, or by $D$ when
            clear.
        \end{defn}

        \begin{defn}[An Embedding's Re-summed Value]
            The \emph{re-summed value} $\rvalue_f(D)$ of an embedding $f$ of a
            diagram $D$ is the same as the un-resummed value of $D$, save for
            one change having to do with edges.  Consider an edge between two
            nodes embedded to $(n,t)$ and $(n^\prime, t+\Delta t)$.  Whereas
            $\uvalue(D)$ has a factor of $\eta^{\mu\nu}$ for this edge,
            $\rvalue_f(D)$ instead has a factor of
            $
                ((I-\eta H)^{\Delta t - 1})^\mu_\lambda \eta^{\lambda\nu}
            $.  Here, $1 \leq \Delta t$ is the temporal distance between
            the two nodes' embeddings.  
        \end{defn}

        We will often seek \emph{differences}, e.g.\ between ODE's and SGD's
        test loss or between a test loss and a train loss.  We thus define a
        compact notation for differences of diagrams:
        \begin{defn}[Fuzzy Outlines Denote Noise's Net Effect]
            A diagram drawn with one \emph{fuzzy outline} denotes the
            difference between the versions with and without fuzzy ties.  E.g.:
            $$
                \mdia{c(0-12)(01-12)}
                    \triangleq
                \mdia{(0-12)(01-12)}
                    -
                \mdia{(0-1-2)(01-12)}
            $$
            We define a diagram drawn with more than one fuzzy outline as the
            fully tied version minus all the versions with fewer fuzzy outlines
            (these are the M\"obius sums of \cite{ro64}):
            \begin{align*}
                \sdia{c(012-3)(01-13-23)}
                    &\triangleq
                \sdia{(012-3)(01-13-23)}
                    -
                \sdia{c(01-2-3)(01-13-23)}
                    -
                \sdia{c(02-1-3)(01-13-23)}
                    -
                \sdia{c(0-12-3)(01-13-23)}
                    -
                \sdia{(0-1-2-3)(01-13-23)} \\
                    &\triangleq
                \sdia{(012-3)(01-13-23)}
                    -
                \sdia{(01-2-3)(01-13-23)}
                    -
                \sdia{(02-1-3)(01-13-23)}
                    -
                \sdia{(0-12-3)(01-13-23)}
                    +
                2 \sdia{(0-1-2-3)(01-13-23)}
            \end{align*}
        \end{defn}

        \begin{defn}[Irreducible Diagrams]
            A diagram, drawn with fuzzy outlines instead of ties, is
            \emph{irreducible} when none of its degree-$2$ non-root nodes
            participates in fuzzy outlines. 
            So
            $\sdia{c(0-1-2)(02-12)},
            \sdia{c(01-2)(01-12)}$
            are irreducible, 
            but not
            $\sdia{c(0-1-2)(01-12)},
            \sdia{c(02-1-3)(01-12-23)}$.
        \end{defn}

        \begin{figure}[h] 
            \centering  
            \plotmooh{diagrams/spacetime-e}{}{0.26\columnwidth}
            \plotmooh{diagrams/spacetime-f}{}{0.26\columnwidth}
            \caption{
                {\bf Diagrams in Spacetime Depict SGD's Subprocesses.}
                Two spacetimes with $N=8, T=16$.
                {\bf Left}: Batchsize $B=1$ with inter-epoch shuffling. 
                    Embeddings, legal and illegal, of
                        $\sdia{(01-2)(01-12)}$,
                        $\sdia{(01-2)(01-12)}$, and
                        $\sdia{(0-1-2)(01-12)}$.
                {\bf Right}: Batchsize $B=2$ without inter-epoch shuffling. 
                    Interpretation of an order $\eta^4$ diagram embedding. 
            }
            \label{fig:spacetimes}
        \end{figure}
            

        \begin{table}[H]
            \centering 
            \resizebox{0.99\columnwidth}{!}{
            \begin{tabular}{c|c|c}
                {\LARGE $\Theta\left((\eta T)^3 T^{-0}\right)$} &
                {\LARGE $\Theta\left((\eta T)^3 T^{-1}\right)$} &
                {\LARGE $\Theta\left((\eta T)^3 T^{-2}\right)$} \\ \hline
                \begin{tabular}{c}
                    \begin{tabular}{LL}
                        \bdia{(0-1-2-3)(01-12-23)} & \bdia{(0-1-2-3)(01-13-23)}
                    \end{tabular} \\
                    \begin{tabular}{LL}
                        \bdia{(0-1-2-3)(02-13-23)} & \bdia{(0-1-2-3)(03-12-23)}
                    \end{tabular} \\ \hline
                    \begin{tabular}{LL}
                        \bdia{(0-1-2-3)(03-13-23)} & \bdia{(0-1-2-3)(02-12-23)}
                    \end{tabular}
                \end{tabular}
                &
                \begin{tabular}{c}
                    \begin{tabular}{LL}
                        \bdia{(01-2-3)(02-13-23)} & \bdia{(01-2-3)(03-12-23)}
                    \end{tabular} \\ \hline
                    \begin{tabular}{LL}
                        \bdia{(0-12-3)(01-13-23)} & \bdia{(0-12-3)(02-13-23)}
                    \end{tabular} \\ \hline
                    \begin{tabular}{LLL}
                        \bdia{(01-2-3)(03-13-23)} & \bdia{(0-12-3)(03-13-23)} & \bdia{(01-2-3)(02-12-23)} 
                    \end{tabular}
                \end{tabular}
                &
                \begin{tabular}{c}
                    \begin{tabular}{L}
                        \bdia{(012-3)(03-13-23)}
                    \end{tabular}
                \end{tabular}
            \end{tabular}
        }
            \caption{
                {\bf Degree-$3$ diagrams for $B=M=1$ SGD's test loss}.
                The $6$ diagrams have $(4+2)+(2+2+3)+(1)$ total orderings
                relevant to Proposition \ref{prop:vanilla}.
                {\bf Left:} $(d,c)=(3,0)$.  Diagrams for ODE behavior.
                {\bf Center:} $(d,c)=(3,1)$.  $1$st order deviation of SGD
                away from ODE.
                {\bf Right:} $(d,c)=(3,2)$.  $2$nd order deviation of SGD
                from ODE with appearance of non-Gaussian statistics.
            }
            \label{tab:scatthree}
        \end{table}
 
        %The connection between generalization and covariance was first
        %established by \cite{ro18} in the case $T=2$ and to order $\eta^2$. 
        %In fact, that work conjectures the possibility of emulating GD with
        %SGD.  This sub-section extends that work by generalizing to arbitrary
        %$T$ and arbitrary orders $\eta^d$, and by concretely defining GDC.

        %In these experiments, we used a covariance estimator $\hat C \propto
        %\nabla l_x (\nabla l_x - \nabla l_y)$ evaluated on two batches $x, y$
        %that evenly partition the train set.  For typical architectures, we may
        %compute $\nabla \hat C$ with the same memory and time as the usual
        %gradient $\nabla l_t$, up to a multiplicative constant. 


    %--------------------------------------------------------------------------
    %           Comparison to Continuous Time               
    %--------------------------------------------------------------------------

    \subsection{Comparison to continuous time} \label{subsect:gaussfit}
        Consider fitting a centered normal $\Nn(0, \sigma^2)$ to data $x$ drawn 
        i.i.d. from a centered standard normal.  We parameterize the landscape
        by $h=\log(\sigma^2)$ so that the Fisher information matches the
        standard dot product \citep{am98}.  The gradient at sample $x$ and
        weight $h$ is then $g_x(h) = (1-x^2\exp(-h))/2$.  Since $x\sim
        \Nn(0, 1)$, $g_x(h)$ will be affinely related to a chi-squared and in
        particular non-Gaussian.
        %At $h=0$, the expected gradient vanishes, so
        %each diagram with a singleton leaf evaluates to $0$.  The test loss of
        %SGD thus involves only the remaining diagrams; to $3$rd order, it is
        %$$
        %    \sdia{(0)()}
        %    +\frac{T}{2} \sdia{c(01-2)(02-12)}
        %    -{T\choose 2} \sdia{c(03-1-2)(01-12-23)}
        %    -\frac{T}{6} \sdia{c(012-3)(03-13-23)}
        %$$
        {\color{red} FIGURE} shows that even for this simple learning problem, 
        SGD and SDE differ as predicted.

        It is routine to check that, by stitching together copies of this
        example, we may cause SGD to travel along paths that are closed loops
        or unbounded curvesWe may even add a small linear component so
        that SGD steadily climbs uphill.  

        So, even in a valley of global minima,
        SGD will move away from minima whose Hessian aligns with the current
        covariance.  However, by the time it moves, the new covariance might
        differ from the old one, and SGD will be repelled by different Hessians
        than before.  Setting the covariance to lag the Hessian by a phase, we
        construct a landscape in which this entropic force dominates.  This
        ``\emph{linear screw}'' landscape has


        %{\color{red} CITE TIC}
        %Because the TIC estimates a smooth hypothesis class's generalization
        %gap, it is tempting to use it as an additive regularization term.
        %However, since the TIC is singular where the Hessian is singular, it
        %gives insensible results for over-parameterized models.  E.g.,
        %\cite{di18} report numerical difficulties requiring an arbitrary
        %cutoff. 

        %Fortunately, by Corollary \ref{cor:overfit}, the implicit
        %regularization of gradient descent both demands and enables a
        %singularity-removing correction to the TIC (Figure
        %\ref{fig:thermoandtak}, right).  
        %%
        %The resulting \emph{Stabilized TIC} (STIC) uses the metric $\eta^{-1}$
        %implicit in gradient descent to threshold flat from sharp
        %minima\footnote{
        %    The notion of $H$'s width depends on a choice of
        %    metric.  Prior work chooses this metric arbitrarily.  We show that
        %    choosing $\eta^{-1}$ is a natural choice because it leads to a
        %    prediction of the gen.\ gap.
        %}.
        %%
        %It thus offers a principled method for
        %optimizer-aware model selection easily compatible with automatic
        %differentiation systems.  By descending on STIC, we may tune smooth
        %hyperparameters such as $l_2$ coefficients.  Experiments on an
        %artificial Mean Estimation problem (task in Appendix
        %\ref{sect:landscape}, plot in Appendix \ref{sect:figures}) recommend
        %STIC for model selection when $H$ is negligible compared to $C/N$ as in
        %the noisy, small-$N$ regime.  Because diagonalization typically takes
        %time cubic in dimension, exact STIC regularization is most useful for
        %small models on noisy and limited data.

    \subsection{Questions}

        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        %~~~~~  Ask Questions  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        The diagram method opens the door to exploration of Lagrangian
        formalisms and curved backgrounds\footnote{
            \cite{la60, la51} introduce these concepts.
        }:
        \begin{quest}
            Does some least-action principle govern SGD; if not, what is an
            essential obstacle to this characterization?
        \end{quest}
        Lagrange's least-action formalism intimately intertwines with the
        diagrams of physics.  Together, they afford a modular framework for
        introducing new interactions as new terms or diagram nodes.  In fact,
        we find that some \emph{higher-order} methods --- such as the
        Hessian-based update
        $
            \theta \leftsquigarrow
            \theta -
            (\eta^{-1} + \lambda \nabla \nabla l_t(\theta))^{-1}
            \nabla l_t(\theta)
        $
        parameterized by small $\eta, \lambda$ --- admit diagrammatic analysis
        when we represent the $\lambda$ term as a second type of diagram node.
        Though diagrams suffice for computation, it is Lagrangians that most
        deeply illuminate scaling and conservation laws.
        \begin{conj}[Riemann Curvature Regularizes]
            For small $\eta$, SGD's gen. gap decreases as sectional curvature
            grows.
        \end{conj}
        Though our work so far assumes a flat metric $\eta^{\mu\nu}$, it
        generalizes to curved weight spaces\footnote{
            One may represent the affine connection as a node, thus giving
            rise to non-tensorial and hence gauge-dependent diagrams.
        }.
        Curvature finds concrete application in the \emph{learning on
        manifolds} paradigm of \cite{ab07, zh16}, notably specialized to
        \cite{am98}'s \emph{natural gradient descent} and \cite{ni17}'s
        \emph{hyperbolic embeddings}.  We are optimistic our formalism may
        resolve conjectures such as above.


    %%--------------------------------------------------------------------------
    %%           History of SGD
    %%--------------------------------------------------------------------------

    %It was \cite{ki52} who, in uniting gradient descent \citep{ca47} with
    %stochastic approximation \citep{ro51}, invented SGD.  Since the development
    %of back-propagation for efficient differentiation \citep{we74}, SGD has
    %been used to train connectionist models including neural networks
    %\citep{bo91}, in recent years to remarkable success \citep{le15}.

    %\subsection{Single-Epoch, Singleton-Batch SGD}
        For SGD with $1$ epoch and batch size $1$, Theorem \ref{thm:resum} then
        specializes to: 
        \begin{prop} \label{prop:vanilla}
            Single-epoch singleton-batch SGD has expected test loss
            \begin{equation*}\label{eq:sgdbasiccoef}
                \sum_{0\leq d<\infty}
                \frac{(-1)^d}{d!} \sum_{D} 
                |\ords(D)| \, {N \choose P-1} \, \frac{d!}{\prod d_p!}
                \uvalue(D)
            \end{equation*}
            where $D$ has $P$ parts with sizes $d_p$.
            Here, $D$ ranges over $d$-edged diagrams none of whose parts
            contains any of its nodes' ancestors, and
            $|\ords(D)|$ counts the total orderings of $D$'s nodes s.t.\
            children precede parents and parts are contiguous.
        \end{prop}
        A diagram with $d$ thin edges and $c$ fuzzy ties (hence $d+1-c$
        parts) thus contributes $\Theta\wrap{(\eta T)^d T^{-c}}$ to SGD's test
        loss. 
        %
        Intuitively, $\eta T$ measures the physical time of descent and
        $T^{-1}$ measures the coarseness of time discretization.  We thus
        regard Proposition \ref{prop:vanilla} as a double series in $(\eta T)^d
        T^{-c}$, where each term isolates the $d$th order effect of time and
        the $c$th order effect of noise.  Indeed, $c$ counts fuzzy ties and
        hence the $c=0$ terms do not model correlations and hence do not model
        noise.  That is, the $c=0$ terms give an ODE approximation to SGD.  The
        remaining terms give the corrections due to noise.  See Table
        \ref{tab:scatthree}. 

            {\color{red} APPENDIX}'s estimators for $C$ et al.
            take time comparable to backpropogation.
 
    %-----------------  experiments  ------------------------------------------
    Our experiments on image classifiers show that even a single evaluation of
    our force laws may predict SGD's motion through macroscopic timescales,
    e.g.\ long enough to decrease error by $0.5$ percentage points.

        \begin{thm}
            For any $T$: for $\eta$ small enough, SGD has expected test loss
            \begin{equation*} \label{eq:resum}
                \sum_{\substack{D \\ \text{irreducible}}}
                \sum_{\substack{\text{embeddings} \\ f}}
                \frac{1}{\wabs{\Aut_f(D)}}
                \frac{{\rvalue_f}(D)}{(-B)^{|\edges(D)|}}
            \end{equation*}
            Here, $D$ ranges through irreducible outlined diagrams, $f$ ranges
            through embeddings of $D$ into the SGD's spacetime, and
            $\wabs{\Aut_f(D)}$ counts the automorphisms of $D$ that
            preserve $f$'s assignment of nodes to $(n,t)$ pairs.
        \end{thm}

        \begin{rmk}
            Sometimes, we prefer $\uvalue(D)$ to $\rvalue_f(D)$ for its
            simplicity.  Theorem \ref{thm:resum} persists if we replace each
            $\rvalue_f(D)$ by $\uvalue(D)$ and sum all tied diagrams instead
            of irreducible outlined diagrams.  But the large-$T$ convergence
            no longer holds {\color{red} PLOT}.
        \end{rmk}


        \begin{rmk}
            The above gives SGD's expected loss on the test set.  How about
            the train set?  Or weight displacements?  Or variances?
            Theorem \ref{thm:resum} and Remark \ref{rmk:unresum} have simple
            analogues for each of these $2^3$ possibilities, which we discuss
            in the appendix. 
        \end{rmk}

        We may approximate $\rvalue_f(D)$ by simpler \emph{un-resummed values}
        at the cost of losing Theorem \ref{thm:converge}'s large-$T$ convergence.
        We state the theorems for expected loss on the test set, but they
        generalize to variances, weight displacements, and training instead of
        testing curves {\color{red} APPENDIX}.
 
\end{document}
