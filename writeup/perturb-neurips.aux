\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{bo91}
\citation{fe49}
\citation{pe71}
\FN@pp@footnotehinttrue 
\providecommand {\FN@pp@footnotehinttrue }{}
\providecommand {\FN@pp@footnote@aux }[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Example of diagram-based computation of SGD's test loss}{1}{subsection.1.1}}
\newlabel{subsect:example}{{1.1}{1}{Example of diagram-based computation of SGD's test loss}{subsection.1.1}{}}
\citation{bo13}
\citation{ki52}
\citation{ca47}
\citation{ro51}
\citation{we74}
\citation{bo91}
\citation{le15}
\FN@pp@footnote@aux{1}{2}
\newlabel{exm:first}{{1}{2}{}{exm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Background, notation, and assumptions}{2}{subsection.1.2}}
\newlabel{sect:background}{{1.2}{2}{Background, notation, and assumptions}{subsection.1.2}{}}
\FN@pp@footnote@aux{2}{2}
\citation{ne17a}
\citation{ba17}
\citation{zh17}
\citation{ne17b}
\citation{ch18}
\citation{ya19a}
\citation{ro18}
\citation{li18}
\citation{ho17}
\citation{ke17}
\citation{wa18}
\citation{st56}
\citation{di17}
\citation{wu18}
\citation{dy19}
\citation{ch18}
\citation{li17}
\citation{ro12,ku19}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Related work}{3}{subsection.1.3}}
\newlabel{sect:related}{{1.3}{3}{Related work}{subsection.1.3}{}}
\FN@pp@footnote@aux{3}{3}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theory, specialized to $E=B=1$ SGD's test loss}{3}{section.2}}
\newlabel{sect:calculus}{{2}{3}{Theory, specialized to $E=B=1$ SGD's test loss}{section.2}{}}
\FN@pp@footnote@aux{4}{3}
\citation{we19b}
\citation{ya19b}
\citation{we19b}
\citation{we19b}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Main result}{4}{subsection.2.1}}
\newlabel{thm:resum}{{1}{4}{Special case of $E=B=1$}{thm.1}{}}
\newlabel{eq:resum}{{1}{4}{Special case of $E=B=1$}{thm.1}{}}
\newlabel{thm:converge}{{2}{4}{}{thm.2}{}}
\newlabel{rmk:integrate}{{2}{4}{}{rmk.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Insights from the formalism}{4}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}SGD descends on a $C$-smoothed landscape and prefers minima flat w.r.t.\ $C$.}{4}{subsubsection.2.2.1}}
\newlabel{cor:entropic}{{1}{4}{Computed from $\sdia {c(01-2-3)(02-12-23)}$}{cor.1}{}}
\FN@pp@footnote@aux{5}{4}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Both flat and sharp minima overfit less}{4}{subsubsection.2.2.2}}
\newlabel{subsect:curvature-and-overfitting}{{2.2.2}{4}{Both flat and sharp minima overfit less}{subsubsection.2.2.2}{}}
\newlabel{cor:overfit}{{2}{4}{from $\sdia {c(01-2)(02-12)}$, $\sdia {c(01)(01)}$}{cor.2}{}}
\citation{ch87}
\citation{li18}
\citation{ch18}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Geometric intuition for curvature-noise interactions.} \textbf  {Left}: Gradient noise pushes SGD toward minima that are flat \emph  {with respect to the covariance} (Corollary \ref  {cor:entropic}). The red densities show the typical $\theta $s, perturbed from the minimum due to noise $C$, in two cross sections of the loss valley. $J = \nabla H$ measures how curvature changes across the valley. Our theory does not assume separation between ``fast'' and ``slow'' modes, but we label them in the picture to ease comparison with \cite  {we19b}. \textbf  {\bf  Right}: Both curvature and the structure of noise affect overfitting. In each of the four subplots, the $\leftrightarrow $ axis represents weight space and the $\delimiter "326C33F $ axis represents loss. \offive {1}: \emph  {covector}-perturbed landscapes favor large $H$s. \offive {2}: \emph  {vector}-perturbed landscapes favor small $H$s. SGD's implicit regularization interpolates between these rows (Corollary \ref  {cor:overfit}). }}{5}{figure.1}}
\newlabel{fig:cubicandspring}{{1}{5}{\textbf {Geometric intuition for curvature-noise interactions.} \textbf {Left}: Gradient noise pushes SGD toward minima that are flat \emph {with respect to the covariance} (Corollary \ref {cor:entropic}). The red densities show the typical $\theta $s, perturbed from the minimum due to noise $C$, in two cross sections of the loss valley. $J = \nabla H$ measures how curvature changes across the valley. Our theory does not assume separation between ``fast'' and ``slow'' modes, but we label them in the picture to ease comparison with \cite {we19b}. \textbf {\bf Right}: Both curvature and the structure of noise affect overfitting. In each of the four subplots, the $\leftrightarrow $ axis represents weight space and the $\updownarrow $ axis represents loss. \protect \offive {1}: \emph {covector}-perturbed landscapes favor large $H$s. \protect \offive {2}: \emph {vector}-perturbed landscapes favor small $H$s. SGD's implicit regularization interpolates between these rows (Corollary \ref {cor:overfit})}{figure.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}High-$C$ regions repel small-$(E,B)$ SGD more than large-$(E,B)$ SGD}{5}{subsubsection.2.2.3}}
\newlabel{subsect:epochs-batch}{{2.2.3}{5}{High-$C$ regions repel small-$(E,B)$ SGD more than large-$(E,B)$ SGD}{subsubsection.2.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  \textbf  {Chladni plate}. Grains of sand on a vibrating plate tend toward stationary regions. }}{5}{figure.2}}
\newlabel{fig:chladni}{{2}{5}{\textbf {Chladni plate}. Grains of sand on a vibrating plate tend toward stationary regions}{figure.2}{}}
\FN@pp@footnote@aux{6}{5}
\newlabel{cor:batch}{{3}{5}{$\sdia {c(01-2)(01-12)}$}{cor.3}{}}
\newlabel{cor:epochs}{{4}{5}{$\sdia {c(01-2)(01-12)}$}{cor.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Non-Gaussian noise affects SGD but not SDE}{5}{subsubsection.2.2.4}}
\newlabel{cor:vsode}{{5}{5}{$\sdia {c(01-2)(02-12)}$, $\sdia {c(012-3)(03-13-23)}$}{cor.5}{}}
\FN@pp@footnote@aux{7}{5}
\citation{di18}
\citation{di18}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments}{6}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Training time, epochs, and batch size; $C$ repels SGD more than GD}{6}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  {\bf  Left: Perturbation models SGD for small $\eta T$.} Fashion-MNIST convnet's test loss vs learning rate. In this small $T$ setting, we choose to use our theory's simpler un-resummed values (\ref  {appendix:evaluate-embeddings}) instead of the more precise $\text  {\textnormal  {rvalue}}$s. \ofsix {0}: For all init.s tested ($1$ shown, $11$ unshown), the order $3$ prediction agrees with experiment through $\eta T \approx 10^0$, corresponding to a decrease in $0\unhbox \voidb@x \hbox {-}1$ error of $\approx 10^{-3}$. \ofsix {1}: For large $\eta T$, our predictions break down. Here, the order-$3$ prediction holds until the $0\unhbox \voidb@x \hbox {-}1$ error improves by $5\cdot 10^{-3}$. Beyond this, $2$nd order agreement with experiment is coincidental. \newline  {\bf  Center: $C$ controls gen.\ gap and distinguishes GD from SGD.} With equal-scaled axes, \ofsix {2} shows that GDC matches SGD (small vertical varianec) better than GD matches SGD (large horizontal variance) in test loss for a range of $\eta $ ($\approx 10^{-3}-10^{-1}$) and init.s\ (zero and several Xavier-Glorot trials) for logistic regression and convnets. Here, $T=10$. \ofsix {3}: CIFAR-10 generalization gaps. For all init.s tested ($1$ shown, $11$ unshown), the degree-$2$ prediction agrees with experiment through $\eta T \approx 5\cdot 10^{-1}$. \newline  {\bf  Right: Predictions near minima excel for large $\eta T$.}\ofsix {4}: SGD travels \textsc  {Archimedes}' valley of global minima in the positive $z$ direction. Note: $H$ and $C$ are bounded across the valley, we see drift for all small $\eta $, and we see displacement exceeding the landscape's period of $2\pi $. So: the drift is not a pathology of well-chosen $\eta $, of divergent noise, or of ephemeral initial conditions. \ofsix {5}: For \textsc  {Mean Estimation}\tmspace  +\thinmuskip {.1667em} with fixed $C$ and a range of $H$s, initialized at the truth, the test losses after fixed-$T$ GD are smallest for very sharp and very flat $H$. Near $H=0$, our predictions improve on Takeuchi information \citep  {di18} and thus on AIC. }}{6}{figure.3}}
\newlabel{fig:vanilla}{{3}{6}{{\bf Left: Perturbation models SGD for small $\eta T$.} Fashion-MNIST convnet's test loss vs learning rate. In this small $T$ setting, we choose to use our theory's simpler un-resummed values (\ref {appendix:evaluate-embeddings}) instead of the more precise $\rvalue $s. \protect \ofsix {0}: For all init.s tested ($1$ shown, $11$ unshown), the order $3$ prediction agrees with experiment through $\eta T \approx 10^0$, corresponding to a decrease in $0\mbox {-}1$ error of $\approx 10^{-3}$. \protect \ofsix {1}: For large $\eta T$, our predictions break down. Here, the order-$3$ prediction holds until the $0\mbox {-}1$ error improves by $5\cdot 10^{-3}$. Beyond this, $2$nd order agreement with experiment is coincidental. \newline {\bf Center: $C$ controls gen.\ gap and distinguishes GD from SGD.} With equal-scaled axes, \protect \ofsix {2} shows that GDC matches SGD (small vertical varianec) better than GD matches SGD (large horizontal variance) in test loss for a range of $\eta $ ($\approx 10^{-3}-10^{-1}$) and init.s\ (zero and several Xavier-Glorot trials) for logistic regression and convnets. Here, $T=10$. \protect \ofsix {3}: CIFAR-10 generalization gaps. For all init.s tested ($1$ shown, $11$ unshown), the degree-$2$ prediction agrees with experiment through $\eta T \approx 5\cdot 10^{-1}$. \newline {\bf Right: Predictions near minima excel for large $\eta T$.}\protect \ofsix {4}: SGD travels \Archimedes ' valley of global minima in the positive $z$ direction. Note: $H$ and $C$ are bounded across the valley, we see drift for all small $\eta $, and we see displacement exceeding the landscape's period of $2\pi $. So: the drift is not a pathology of well-chosen $\eta $, of divergent noise, or of ephemeral initial conditions. \protect \ofsix {5}: For \MeanEstimation \, with fixed $C$ and a range of $H$s, initialized at the truth, the test losses after fixed-$T$ GD are smallest for very sharp and very flat $H$. Near $H=0$, our predictions improve on Takeuchi information \citep {di18} and thus on AIC}{figure.3}{}}
\citation{ch18}
\citation{we19b}
\citation{ro18}
\citation{bo91}
\citation{go18}
\citation{fi17}
\citation{st19}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Minima that are flat \emph  {with respect to} $C$ attract SGD}{7}{subsection.3.2}}
\newlabel{subsect:entropic}{{3.2}{7}{Minima that are flat \emph {with respect to} $C$ attract SGD}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  \textbf  {Two views of \textsc  {Archimedes}.} Green sheet: a level surface of $l$ with a valley of minima ($z$ axis) at its center; $l$ is large outside this surface. Magenta tubes: typical $\theta $s due to non-isotropic noise. \textbf  {Left}: $z$ axis points into the page. \textbf  {Right}: $z$ axis points upward. The typical locations of $\theta $ are pulled toward lower loss (steepest descent toward the level surface) and so toward larger $z$. }}{7}{figure.4}}
\newlabel{fig:landscapes}{{4}{7}{\textbf {Two views of \Archimedes .} Green sheet: a level surface of $l$ with a valley of minima ($z$ axis) at its center; $l$ is large outside this surface. Magenta tubes: typical $\theta $s due to non-isotropic noise. \textbf {Left}: $z$ axis points into the page. \textbf {Right}: $z$ axis points upward. The typical locations of $\theta $ are pulled toward lower loss (steepest descent toward the level surface) and so toward larger $z$}{figure.4}{}}
\FN@pp@footnote@aux{8}{7}
\FN@pp@footnote@aux{9}{7}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Sharp and flat minima both overfit less than medium minima}{7}{subsection.3.3}}
\newlabel{subsect:overfit}{{3.3}{7}{Sharp and flat minima both overfit less than medium minima}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion: implications for practice}{7}{section.4}}
\newlabel{sect:concl}{{4}{7}{Conclusion: implications for practice}{section.4}{}}
\citation{ni11}
\citation{dw06}
\citation{ar19}
\bibstyle{plainnat}
\bibdata{perturb}
\bibcite{ab07}{{1}{2007}{{Absil et~al.}}{{Absil, Mahony, and Sepulchre}}}
\bibcite{am98}{{2}{1998}{{Amari}}{{}}}
\bibcite{ar19}{{3}{2019}{{Ardila\IeC {\textendash }Mantilla}}{{}}}
\bibcite{ba17}{{4}{2017}{{Bartlett et~al.}}{{Bartlett, Foster, and Telgarsky}}}
\bibcite{bo13}{{5}{2013}{{Bonnabel}}{{}}}
\bibcite{bo91}{{6}{1991}{{Bottou}}{{}}}
\bibcite{ca47}{{7}{1847}{{Cauchy}}{{}}}
\bibcite{ch18}{{8}{2018}{{Chaudhari and Soatto}}{{}}}
\bibcite{ch87}{{9}{1787}{{Chladni}}{{}}}
\bibcite{di17}{{10}{2017}{{Dinh et~al.}}{{Dinh, Pascanu, Bengio, and Bengio}}}
\bibcite{di18}{{11}{2018}{{Dixon and Ward}}{{}}}
\bibcite{dw06}{{12}{2006}{{Dwork et~al.}}{{Dwork, McSherry, Nissim, and Smith}}}
\bibcite{dy19}{{13}{2019}{{Dyer and Gur-Ari}}{{}}}
\bibcite{dy49a}{{14}{1949}{{Dyson}}{{}}}
\bibcite{fe49}{{15}{1949}{{Feynman}}{{}}}
\bibcite{fi17}{{16}{2017}{{Finn et~al.}}{{Finn, Abbeel, and Levine}}}
\bibcite{ga23}{{17}{1823}{{Gauss}}{{}}}
\bibcite{go18}{{18}{2018}{{Goyal et~al.}}{{Goyal, Doll\'{a}r, Girshick, Noordhuis, Wesolowski, Kyrola, Tulloch, Jia, and He}}}
\bibcite{ho17}{{19}{2017}{{Hoffer et~al.}}{{Hoffer, Hubara, and Soudry}}}
\bibcite{ke17}{{20}{2017}{{Keskar et~al.}}{{Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang}}}
\bibcite{ki52}{{21}{1952}{{Kiefer and Wolfowitz}}{{}}}
\bibcite{kr09}{{22}{2009}{{Krizhevsky}}{{}}}
\bibcite{ku19}{{23}{2019}{{Kunstner et~al.}}{{Kunstner, Hennig, and Balles}}}
\bibcite{la51}{{24}{1951}{{Landau and Lifshitz}}{{}}}
\bibcite{la60}{{25}{1960}{{Landau and Lifshitz}}{{}}}
\bibcite{le15}{{26}{2015}{{LeCun et~al.}}{{LeCun, Bengio, and Hinton}}}
\bibcite{li17}{{27}{2017}{{Li et~al.}}{{Li, Tai, and E}}}
\bibcite{li18}{{28}{2018}{{Liao et~al.}}{{Liao, Miranda, Banburski, Hidary, and Poggio}}}
\bibcite{ne17a}{{29}{2017{a}}{{Neyshabur et~al.}}{{Neyshabur, Bhojanapalli, McAllester, and Srebro}}}
\bibcite{ne17b}{{30}{2017{b}}{{Neyshabur et~al.}}{{Neyshabur, Tomioka, Salakhutdinov, and Srebro}}}
\bibcite{ni17}{{31}{2017}{{Nickel and Kiela}}{{}}}
\bibcite{ni11}{{32}{2011}{{Niu et~al.}}{{Niu, Recht, R\'e, and Wright}}}
\bibcite{pa19}{{33}{2019}{{Paszke et~al.}}{{Paszke, Gross, Massa, Lerer, Bradbury, Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala}}}
\bibcite{pe71}{{34}{1971}{{Penrose}}{{}}}
\bibcite{ro51}{{35}{1951}{{Robbins and Monro}}{{}}}
\bibcite{ro18}{{36}{2018}{{Roberts}}{{}}}
\bibcite{ro12}{{37}{2012}{{Roux et~al.}}{{Roux, Bengio, and Fitzgibbon}}}
\bibcite{st56}{{38}{1956}{{Stein}}{{}}}
\bibcite{st19}{{39}{2019}{{Strubell et~al.}}{{Strubell, Ganesh, and McCallum}}}
\bibcite{vi00}{{40}{circa $10^{1/2}$ b.c.e.}{{Vitruvius}}{{}}}
\bibcite{wa18}{{41}{2018}{{Wang et~al.}}{{Wang, Keskar, Xiong, and Socher}}}
\bibcite{we19b}{{42}{2019}{{Wei and Schwab}}{{}}}
\bibcite{we74}{{43}{1974}{{Werbos}}{{}}}
\bibcite{wu18}{{44}{2018}{{Wu et~al.}}{{Wu, Ma, and E}}}
\bibcite{xi17}{{45}{2017}{{Xiao et~al.}}{{Xiao, Rasul, and Vollgraf}}}
\bibcite{ya19a}{{46}{2019{a}}{{Yaida}}{{}}}
\bibcite{ya19b}{{47}{2019{b}}{{Yaida}}{{}}}
\bibcite{zh17}{{48}{2017}{{Zhang et~al.}}{{Zhang, Bengio, Hardt, Recht, and Vinyals}}}
\bibcite{zh16}{{49}{2016}{{Zhang et~al.}}{{Zhang, Reddi, and Sra}}}
\FN@pp@footnotehinttrue 
\citation{ch87}
\@writefile{toc}{\contentsline {section}{\numberline {A}How to calculate test losses: a practical guide}{13}{section.1}}
\newlabel{appendix:tutorial}{{A}{13}{How to calculate test losses: a practical guide}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}An example calculation}{13}{subsection.1.1}}
\newlabel{appendix:example}{{A.1}{13}{An example calculation}{subsection.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}How to identify the relevant space-time}{13}{subsection.1.2}}
\newlabel{appendix:draw-spacetime}{{A.2}{13}{How to identify the relevant space-time}{subsection.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}How to identify the relevant diagram embeddings}{13}{subsection.1.3}}
\newlabel{appendix:draw-embeddings}{{A.3}{13}{How to identify the relevant diagram embeddings}{subsection.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}How to evaluate each embedding}{13}{subsection.1.4}}
\newlabel{appendix:evaluate-embeddings}{{A.4}{13}{How to evaluate each embedding}{subsection.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}How to sum the embeddings' values}{13}{subsection.1.5}}
\newlabel{appendix:sum-embeddings}{{A.5}{13}{How to sum the embeddings' values}{subsection.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.6}Interpreting diagrams to build intuition}{13}{subsection.1.6}}
\newlabel{appendix:interpret-diagrams}{{A.6}{13}{Interpreting diagrams to build intuition}{subsection.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.7}How to solve variant problems}{13}{subsection.1.7}}
\newlabel{appendix:solve-variants}{{A.7}{13}{How to solve variant problems}{subsection.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.8}Do diagrams streamline computation?}{13}{subsection.1.8}}
\newlabel{appendix:diagrams-streamline}{{A.8}{13}{Do diagrams streamline computation?}{subsection.1.8}{}}
\citation{dy49a}
\@writefile{toc}{\contentsline {section}{\numberline {B}Mathematics of the theory}{14}{section.2}}
\newlabel{appendix:math}{{B}{14}{Mathematics of the theory}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Assumptions and definitions}{14}{subsection.2.1}}
\newlabel{appendix:assumptions}{{B.1}{14}{Assumptions and definitions}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}A key lemma \`a la Dyson}{14}{subsection.2.2}}
\newlabel{appendix:key-lemma}{{B.2}{14}{A key lemma \`a la Dyson}{subsection.2.2}{}}
\newlabel{lem:dyson}{{B.2}{14}{}{subsection.2.2}{}}
\newlabel{eq:dyson}{{1}{14}{}{equation.2.1}{}}
\newlabel{eq:smalleta}{{2}{14}{A key lemma \`a la Dyson}{equation.2.2}{}}
\FN@pp@footnote@aux{10}{14}
\newlabel{eq:expansion}{{3}{14}{A key lemma \`a la Dyson}{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}From Dyson to diagrams}{15}{subsection.2.3}}
\newlabel{appendix:toward-diagrams}{{B.3}{15}{From Dyson to diagrams}{subsection.2.3}{}}
\newlabel{thm:pathint}{{3}{15}{Test Loss as a Path Integral}{thm.3}{}}
\newlabel{eq:sgdcoef}{{3}{15}{Test Loss as a Path Integral}{thm.3}{}}
\newlabel{eq:toprove}{{4}{15}{From Dyson to diagrams}{equation.2.4}{}}
\newlabel{rmk:leibniz}{{3}{15}{Differentiating Products}{rmk.3}{}}
\newlabel{rmk:leibniz}{{B.3}{16}{From Dyson to diagrams}{rmk.3}{}}
\newlabel{prop:vanilla}{{1}{17}{}{prop.1}{}}
\newlabel{eq:sgdbasiccoef}{{1}{17}{}{prop.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Interlude: a review of M\"obius inversion}{17}{subsection.2.4}}
\newlabel{appendix:mobius}{{B.4}{17}{Interlude: a review of M\"obius inversion}{subsection.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5}Theorems \ref  {thm:resum} and \ref  {thm:converge}}{17}{subsection.2.5}}
\newlabel{appendix:resum}{{B.5}{17}{Theorems \ref {thm:resum} and \ref {thm:converge}}{subsection.2.5}{}}
\newlabel{subsubsect:mobius}{{B.5}{17}{Theorems \ref {thm:resum} and \ref {thm:converge}}{subsection.2.5}{}}
\newlabel{eq:hard}{{5}{17}{Theorems \ref {thm:resum} and \ref {thm:converge}}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.6}How to modify proofs to handle variants}{18}{subsection.2.6}}
\newlabel{appendix:prove-variants}{{B.6}{18}{How to modify proofs to handle variants}{subsection.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.7}Proofs of corollaries}{18}{subsection.2.7}}
\newlabel{appendix:corollaries}{{B.7}{18}{Proofs of corollaries}{subsection.2.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.7.1}Corollary \ref  {cor:entropic}}{18}{subsubsection.2.7.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.7.2}Corollary \ref  {cor:overfit}'s first part}{18}{subsubsection.2.7.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.7.3}Corollary \ref  {cor:overfit}'s second part}{18}{subsubsection.2.7.3}}
\citation{la60,la51}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.7.4}Corollaries \ref  {cor:epochs} and \ref  {cor:batch}}{19}{subsubsection.2.7.4}}
\newlabel{prop:ordtwo}{{2}{19}{}{prop.2}{}}
\newlabel{tbl:ordtwo}{{B.7.4}{19}{Corollaries \ref {cor:epochs} and \ref {cor:batch}}{prop.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.7.5}Corollary \ref  {cor:vsode}}{19}{subsubsection.2.7.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.8}Future topics}{19}{subsection.2.8}}
\newlabel{appendix:future}{{B.8}{19}{Future topics}{subsection.2.8}{}}
\FN@pp@footnote@aux{11}{19}
\citation{ab07,zh16}
\citation{am98}
\citation{ni17}
\FN@pp@footnote@aux{12}{20}
\citation{am98}
\citation{vi00}
\citation{kr09}
\citation{xi17}
\@writefile{toc}{\contentsline {section}{\numberline {C}Experimental methods}{21}{section.3}}
\newlabel{appendix:experiments}{{C}{21}{Experimental methods}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}What artificial landscapes did we use?}{21}{subsection.3.1}}
\newlabel{appendix:artificial}{{C.1}{21}{What artificial landscapes did we use?}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}What image-classification landscapes did we use?}{22}{subsection.3.2}}
\newlabel{appendix:natural}{{C.2}{22}{What image-classification landscapes did we use?}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}Measurement process}{22}{subsection.3.3}}
\newlabel{appendix:measure}{{C.3}{22}{Measurement process}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.4}Implementing optimizers}{22}{subsection.3.4}}
\newlabel{appendix:optimizers}{{C.4}{22}{Implementing optimizers}{subsection.3.4}{}}
\citation{pa19}
\citation{ga23}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.5}Software frameworks and hardware}{23}{subsection.3.5}}
\newlabel{appendix:frameworks}{{C.5}{23}{Software frameworks and hardware}{subsection.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.6}Unbiased estimators of landscape statistics}{23}{subsection.3.6}}
\newlabel{appendix:bessel}{{C.6}{23}{Unbiased estimators of landscape statistics}{subsection.3.6}{}}
\newlabel{eq:bessel}{{6}{23}{Unbiased estimators of landscape statistics}{equation.3.6}{}}
\newlabel{eq:newbessel}{{7}{23}{Unbiased estimators of landscape statistics}{equation.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.7}Additional figures}{24}{subsection.3.7}}
\newlabel{appendix:figures}{{C.7}{24}{Additional figures}{subsection.3.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  \textbf  {Further experimental results}. \textbf  {Left}: SGD with $2, 3, 5, 8$ epochs incurs greater test loss than one-epoch SGD (difference shown in I bars) by the predicted amounts (predictions shaded) for a range of learning rates. Here, all SGD runs have $N=10$; we scale the learning rate for $E$-epoch SGD by $1/E$ to isolate the effect of inter-epoch correlations away from the effect of larger $\eta T$. \textbf  {Center}: SGD's difference from SDE after $\eta T \approx 10^{-1}$ with maximal coarseness on \textsc  {Gauss}. Two effects not modeled by SDE --- time-discretization and non-Gaussian noise oppose on this landscape but do not completely cancel. Our theory approximates the above curve with a correct sign and order of magnitude; we expect that the fourth order corrections would improve it further. \textbf  {Right}: Blue intervals regularization using Corollary \ref  {cor:overfit}. When the blue intervals fall below the black bar, this proposed method outperforms plain GD. For \textsc  {Mean Estimation}\ with fixed $C$ and a range of $H$s, initialized a fixed distance \emph  {away} from the true minimum, descent on an $l_2$ penalty coefficient $\lambda $ improves on plain GD for most Hessians. The new method does not always outperform GD, because $\lambda $ is not perfectly tuned according to STIC but instead descended on for finite $\eta T$. }}{24}{figure.5}}
\newlabel{fig:takreg}{{5}{24}{\textbf {Further experimental results}. \textbf {Left}: SGD with $2, 3, 5, 8$ epochs incurs greater test loss than one-epoch SGD (difference shown in I bars) by the predicted amounts (predictions shaded) for a range of learning rates. Here, all SGD runs have $N=10$; we scale the learning rate for $E$-epoch SGD by $1/E$ to isolate the effect of inter-epoch correlations away from the effect of larger $\eta T$. \textbf {Center}: SGD's difference from SDE after $\eta T \approx 10^{-1}$ with maximal coarseness on \Gauss . Two effects not modeled by SDE --- time-discretization and non-Gaussian noise oppose on this landscape but do not completely cancel. Our theory approximates the above curve with a correct sign and order of magnitude; we expect that the fourth order corrections would improve it further. \textbf {Right}: Blue intervals regularization using Corollary \ref {cor:overfit}. When the blue intervals fall below the black bar, this proposed method outperforms plain GD. For \MeanEstimation \ with fixed $C$ and a range of $H$s, initialized a fixed distance \emph {away} from the true minimum, descent on an $l_2$ penalty coefficient $\lambda $ improves on plain GD for most Hessians. The new method does not always outperform GD, because $\lambda $ is not perfectly tuned according to STIC but instead descended on for finite $\eta T$}{figure.5}{}}
\FN@pp@footnotehinttrue 
