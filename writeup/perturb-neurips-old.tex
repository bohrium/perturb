%   author: samtenka
%   change: 2020-05-31
%   create: 2020-04-16
%   descrp: LaTeX source for perturb project
%   to use: compile along with perturb.bib and diagram and plot directories

%==============================================================================
%=====  LATEX PREAMBLE  =======================================================
%==============================================================================

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Document Styling  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  global style  -----------------------------------------

\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage{microtype}
%\usepackage{icml2019}
\usepackage[accepted]{icml2019}

%---------------------  mathematics  ------------------------------------------

\usepackage{amsmath, amssymb, amsthm, mathtools}

%---------------------  tables  ----------------------------------------------- 

\usepackage{booktabs}
\usepackage{array}
\newcolumntype{L}{>{$}l<{$}}

%---------------------  graphics and figures  ---------------------------------

\usepackage{graphicx}
\usepackage{float, subfigure}
\usepackage{hanging, txfonts, ifthen}

%\addtolength{\textfloatsep}    {-5mm}
%\addtolength{\dbltextfloatsep} {-5mm}
%\addtolength{\abovecaptionskip}{-5mm}
%\addtolength{\belowcaptionskip}{-5mm}

%---------------------  colors  -----------------------------------------------

\usepackage{xcolor, framed}
\definecolor{moolime}{rgb}{0.90,1.00,0.90}
\definecolor{moosky}{rgb}{0.90,0.90,1.00}
\definecolor{moopink}{rgb}{1.00,0.90,0.90}
\definecolor{moor}{rgb}{0.8,0.2,0.2}
\definecolor{moog}{rgb}{0.2,0.8,0.2}
\definecolor{moob}{rgb}{0.2,0.2,0.8}
\definecolor{mooteal}{rgb}{0.1,0.6,0.4}

%---------------------  intertext: footnotes and hyperlinks  ------------------ 

\usepackage[perpage]{footmisc}
\renewcommand*{\thefootnote}{\color{red}\fnsymbol{footnote}} 

\usepackage{hyperref}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Theorem Environments  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  mathematical results  ---------------------------------

\theoremstyle{plain}
    \newtheorem*{klem*}{Key Lemma}
    \newtheorem{thm}{Theorem}
    \newtheorem*{thm*}{Theorem}
    \newtheorem{cor}{Corollary}
    \newtheorem{prop}{Proposition}

%---------------------  mathematical questions  -------------------------------

    \newtheorem{conj}{Conjecture}
    \newtheorem{quest}{Question}
    \newtheorem*{quest*}{Question}
    \newtheorem*{quests*}{Questions}

%---------------------  definitions, answers, remarks  ------------------------

\theoremstyle{definition}
    \newtheorem{defn}{Definition}
    \newtheorem*{answ*}{Answer}
    \newtheorem{rmk}{Remark}
    \newtheorem*{midea*}{Main Idea}
    \newtheorem*{rmk*}{Remark}
    \newtheorem{exm}{Example}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Custom Math Commands  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  expanding containers  ---------------------------------

\newcommand{\wrap}[1]{\left(#1\right)}
\newcommand{\wasq}[1]{\left[#1\right]}
\newcommand{\wang}[1]{\left\langle#1\right\rangle}
\newcommand{\wive}[1]{\left\llbracket#1\right\rrbracket}
\newcommand{\worm}[1]{\left\|#1\right\|}
\newcommand{\wabs}[1]{\left|#1\right|}
\newcommand{\wurl}[1]{\left\{#1\right\}}

\newcommand{\partitionbox}[1]{
    \text{
        \fboxsep=0.5pt
        \tiny
        \fbox{#1}
    }
}

%---------------------  special named objects  --------------------------------

\newcommand{\Free}{\mathcal{F}}
\newcommand{\Forg}{\mathcal{G}}
\newcommand{\Mod}{\mathcal{M}}
\newcommand{\Hom}{\text{\textnormal{Hom}}}
\newcommand{\Aut}{\text{\textnormal{Aut}}}
\newcommand{\image}{\text{\textnormal{im}}}
\newcommand{\dvalue}{\text{\textnormal{value}}}
\newcommand{\rvalue}{\text{\textnormal{rvalue}}}
\newcommand{\edges}{\text{\textnormal{edges}}}
\newcommand{\ords}{\text{\textnormal{ords}}}
\newcommand{\parts}{\text{\textnormal{parts}}}
\newcommand{\SGD}{\text{\textnormal{SGD}}}
\DeclareMathOperator*{\Avg}{\text{\sffamily A}}
\newcommand{\expc}{\mathbb{E}}
\newcommand{\expct}[1]{\mathbb{E}\left[#1\right]}

%---------------------  fancy letters  ----------------------------------------

\newcommand{\Aa}{\mathcal{A}}
\newcommand{\Bb}{\mathcal{B}}
\newcommand{\Cc}{\mathcal{C}}   \newcommand{\CC}{\mathbb{C}}
\newcommand{\Dd}{\mathcal{D}}
\newcommand{\Ee}{\mathcal{E}}
\newcommand{\Ff}{\mathcal{F}}
\newcommand{\Gg}{\mathcal{G}}
\newcommand{\Hh}{\mathcal{H}}
\newcommand{\Ll}{\mathcal{L}}
\newcommand{\Mm}{\mathcal{M}}
\newcommand{\Nn}{\mathcal{N}}   \newcommand{\NN}{\mathbb{N}}
\newcommand{\Oo}{\mathcal{O}}
\newcommand{\Pp}{\mathcal{P}}
\newcommand{\Qq}{\mathcal{Q}}   \newcommand{\QQ}{\mathbb{Q}}
\newcommand{\Rr}{\mathcal{R}}   \newcommand{\RR}{\mathbb{R}}
\newcommand{\Ss}{\mathcal{S}}
\newcommand{\Tt}{\mathcal{T}}
\newcommand{\Uu}{\mathcal{U}}
\newcommand{\Vv}{\mathcal{V}}
\newcommand{\Ww}{\mathcal{W}}
\newcommand{\Xx}{\mathcal{X}}
\newcommand{\Yy}{\mathcal{Y}}
\newcommand{\Zz}{\mathcal{Z}}   \newcommand{\ZZ}{\mathbb{Z}}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Pictures  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  pictures with specified width or height  --------------

\newcommand{\plotmoow}[3]{\includegraphics[width=#2          ]{../#1}}
\newcommand{\plotmooh}[3]{\includegraphics[         height=#3]{../#1}}

%---------------------  inline diagrams of various sizes  ---------------------

\newcommand{\sizeddia}[2]{
    \begin{gathered}
        \includegraphics[scale=#2]{../diagrams/#1.png}
    \end{gathered}
}
\newcommand{\bdia}[1]{\protect \sizeddia{#1}{0.22}}
\newcommand{\dia} [1]{\protect \sizeddia{#1}{0.18}}
\newcommand{\mdia}[1]{\protect \sizeddia{#1}{0.14}}
\newcommand{\sdia}[1]{\protect \sizeddia{#1}{0.10}}

\newcommand{\mend}{\hfill $\Diamond$}

\begin{document}

%==============================================================================
%=====  FRONT MATTER  =========================================================
%==============================================================================

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Title and Author  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  small title at top of each page  ----------------------

\icmltitlerunning{A Space-Time Approach to Analyzing Stochastic Gradient Descent}

\twocolumn[
    %-----------------  big title at top of document  -------------------------
    \icmltitle{A Space-Time Approach to Analyzing Stochastic Gradient Descent}
    %-----------------  author data at top of first page  ---------------------
    \begin{icmlauthorlist}
        \icmlauthor{Samuel C.~Tenka}{mit}
    \end{icmlauthorlist}
    \icmlaffiliation{mit}{
        Computer Science and AI Lab,
        Massachusetts Institute of Technology,
        Cambridge, MA, USA
    }
    %-----------------  author data at bottom of first page  ------------------
    \icmlcorrespondingauthor{Samuel C.~Tenka}{
        \texttt{colimit.edu};~insert \texttt{@}-sign appropriately
    }
    %-----------------  metadata: content tags  -------------------------------
    \icmlkeywords{Machine Learning, SGD, ICML}
    \vskip 0.3in
]
\printAffiliationsAndNotice{}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Abstract  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\begin{abstract}
    %
    %-----------------  hammer and general nail  ------------------------------
    %
    We analyze of Stochastic Gradient Descent (SGD) at small learning rates.
    Unlike prior analyses based on stochastic differential equations, our
    theory models discrete time and hence non-Gaussian noise.
    %
    %-----------------  applications  -----------------------------------------
    %
    We prove that gradient noise systematically pushes SGD toward flatter
    minima.  We characterize when and why flat minima overfit less than sharp
    minima.  We generalize the Akaike Info.\ Criterion (AIC) to a smooth
    estimator of overfitting, hence enabling gradient-based model selection.
    We show how non-stochastic GD with a modified loss function may emulate
    SGD.
    %
    %-----------------  mention of experiments  -------------------------------
    %
    We verify our predictions on convnets for CIFAR-10 and Fashion-MNIST.
\end{abstract}

%==============================================================================
%=====  INTRODUCTION  =========================================================
%==============================================================================

\section{Introduction}

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Orienting Invitation  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    %-----------------  object of study  --------------------------------------

    Practitioners benefit from the intuition that SGD approximates noiseless
    GD \cite{bo91}.  In this paper, we refine that intuition by showing
    how gradient noise \emph{biases} learning toward certain areas of weight
    space.
    
    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Concrete Applications  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    %-----------------  vs ODE and SDE  ---------------------------------------

    Departing from prior work, we model discrete time and hence non-Gaussian
    noise.  Indeed, we derive corrections to continuous-time, Gaussian-noise
    approximations such as ordinary and stochastic differential equations (ODE,
    SDE).
    For example, we construct a loss landscape on which SGD eternally cycles
    counterclockwise, a phenomenon impossible with ODEs. 
    %-----------------  experiments  ------------------------------------------
    %
    Our experiments on image classifiers show that even a single evaluation of
    our force laws may predict SGD's motion through macroscopic timescales,
    e.g.\ long enough to decrease error by $0.5$ percentage points.

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Soft Benefits: Physical Intuition and Further Applications  ~~~

    %-----------------  retrospective  ----------------------------------------

    Our work offers a novel interpretation of SGD as a superposition of
    concurrent interactions between weights an data, each represented by a
    diagram analogous to those of \citet{fe49, pe71}.
    %
    %-----------------  prospective  ------------------------------------------
    %
    In the conclusion, we discuss this bridge to physics --- and its relation
    to Hessian methods and natural GD --- as topics for future research.

    \subsection{Example of diagram-based reasoning}

        \newcommand{\nb} { \nabla }
        \newcommand{\lx} { l_x(\theta) }
        \newcommand{\teq} { \triangleq }
        \newcommand{\ex}[1] { \expc_x \wasq{#1} }

        Our theory analyzes SGD in terms of combinatorial objects we call
        \emph{diagrams}.  Deferring details, we illustrate how our theory
        yields non-trivial results via short arguments. 
        %
        First, we list how components of diagrams encode statistics of the loss
        $l_x(\theta)$ at weight $\theta$ and datapoint $x$:
        \begin{table}[h]\centering
            \resizebox{\columnwidth}{!}{%
            \begin{tabular}{ll}
                $G \teq \ex{\nb\lx}       \teq \dia{MOO(0)(0)}     $ &                                                           \\
                $H \teq \ex{\nb\nb\lx}    \teq \dia{MOO(0)(0-0)}   $ & $C \teq \ex{(\nb\lx - G)^2} \teq \dia{MOOc(01)(0-1)}    $ \\
                $J \teq \ex{\nb\nb\nb\lx} \teq \dia{MOO(0)(0-0-0)} $ & $S \teq \ex{(\nb\lx - G)^3} \teq \dia{MOOc(012)(0-1-2)} $ 
            \end{tabular}
            }
            \caption{
                {\bf Notation}.
                Throughout, $G, H, J$ denote the $1$st, $2$nd, and $3$rd
                derivatives of the loss function.  We write $C, S$ for the
                $2$nd and $3$rd cumulants of the gradient distribution.  We
                differentiate w.r.t.\ the weight $\theta$ and we take
                expectations w.r.t.\ the datapoints $x$.  Note: the tensors $J,
                S$ have three indices.
                %
                Each $\nabla^d l_x$ corresponds to a node with $d$ thin edges
                emanating, and fuzzy outlines connect nodes that occur within
                the same expectation.  
            }
            \label{tbl:notation}
        \end{table}

        We may connect Table \ref{tbl:notation}'s diagrams together to obtain
        \emph{complete diagrams} without loose ends.  For example, we may
        connect two copies of
        $
            G = \sdia{MOO(0)(0)}
        $
        with one copy of
        $
            H = \sdia{MOO(0)(0-0)}
        $
        to obtain
        $
            \sdia{(0-1-2)(01-12)}
        $.\footnote{
            We {\color{moor} color} nodes for convenient reference (e.g. to a
            diagram's ``green nodes'').  As mere labels, colors lack
            mathematical meaning. 
        }
        %%%% 
        %%%% 
        If we run SGD for $T$ gradient steps with learning rate $\eta$
        starting at $\theta_0$, then by Taylor expansion we may express the
        expected test loss at the final weight $\theta_T$ in terms of the
        statistics in Table \label{lbl:notation} evaluated at the
        initialization $\theta_0$.  Diagrams organize the computation of this
        Taylor series.  
        \begin{midea*}[Informal]
            There is a method to assign to any complete diagram a number that
            depends on $\eta, T$.  SGD's expected test loss is a sum, over all
            complete diagrams, of these numbers.  We incur only an $o(\eta^d)$
            error if we consider only diagrams with at most $d$ edges.
        \end{midea*}

        \begin{exm}[How does non-Gaussian noise affect test loss?] \label{exm:first}
            Assume\footnote{
                for simplicity.  Our theory is not limited to this setting. 
            } $\theta_0$ minimizes the test loss and that
            we run SGD for $1$ epoch with batch size $1$.
            The skew $S$ is $0$
            for Gaussians, and we seek the effect of non-zero $S$.  To 
            compute the leading-order effect of $S$ on test loss,
            we identify the fewest-edged complete diagrams containing $S =
            \sdia{MOOc(012)(0-1-2)}$.  In this case, there is one such diagram:
            $
                \sdia{c(012-3)(03-13-23)}
            $.
            Then, working in a basis that diagonalizes $\eta H$, we obtain the
            leading-order effect of $S$ on test loss (with error $o(\eta^3)$):
            $$
                -\frac{\eta^3}{6}
                \sum_{\mu\nu\lambda}
                    S_{\mu\nu\lambda}
                    \frac{
                        1 - \exp(-T\eta (H_{\mu\mu} + H_{\nu\nu} + H_{\lambda\lambda}))
                    }{
                        \eta (H_{\mu\mu} + H_{\nu\nu} + H_{\lambda\lambda})
                    }
                    J_{\mu\nu\lambda}
            $$
        \end{exm}
        \begin{rmk}
            The $S$, the three $H$'s, and the $J$ above respectively correspond
            to
            $
                \sdia{c(012-3)(03-13-23)}
            $'s
            group of red nodes, three thin edges, and green node.  Each diagram
            encodes many Taylor terms, and the fact that we may evaluate each
            diagram as a whole is an advantage of our calculational framework.
            Intuitively, each diagram gives the net effect of a certain
            combination of gradients ($G$), noise ($C, S, \cdots$) and
            curvature ($H, J, \cdots$).  After developing our theory more
            precisely, we will return to these intuitive interpretations.
        \end{rmk}

%==============================================================================
%=====  BACKGROUND AND NOTATION  ==============================================
%==============================================================================

\section{Background and Notation} \label{sect:background}

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  The Loss Landscape  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    \subsection{Loss landscape}

        %-------------  the landscape  ----------------------------------------

        We henceforth fix a space $\Mm$ of weights on which a loss function
        $l:\Mm\to\RR$ is defined.  SGD operates on unbiased estimates of $l$
        drawn from some fixed probability distribution $\Dd$.  We thus denote
        by $(l_n: 0\leq n<N)$ an i.i.d.\ training sequence of such estimates.
        We will refer both to $n$ and to $l_n$ as \emph{training points}.  We
        likewise write $l_x$ for a sample from $\Dd$ independent from the
        training sequence.   We assume the regularity conditions listed in
        Appendix {\color{red} FILL IN}, e.g.\ that $l, l_x$ are analytic and
        that all moments exist.

        %-------------  specialization to a common case  ----------------------

        E.g.: our theory applies to $\tanh$ networks with cross entropy
        loss on bounded data --- and with arbitrary weight sharing, skip
        connections, soft attention, dropout, and weight decay.
        
    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Tensor Conventions  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    \subsection{Tensor conventions}
        Adopting Einstein's convention, we implicitly sum repeated Greek
        indices: if $A_\mu, B^\mu$ are the coefficients of a covector $A$ and a
        vector $B$\footnote{
            Vectors/covectors are also called column/row vectors.
        }, indexed by basis elements $\mu$, then
        $
            A_\mu B^\mu
            \triangleq
            \sum_\mu A_\mu \cdot B^\mu
        $.
        To expedite dimensional analysis, we regard the learning rate as an
        inverse metric $\eta^{\mu\nu}$ that converts a gradient covector into a
        displacement vector \citep{bo13}, and we use $\eta$ to raise indices:
        e.g., in
        $
            H^{\mu}_{\lambda}
            \triangleq
            \eta^{\mu\nu} H_{\nu\lambda}
        $,
        $\eta$ raises one of $H_{\mu\nu}$'s indices.  Another example is
        $
            C^{\mu}_{\mu}
            \triangleq
            \sum_{\mu \nu} \eta^{\mu\nu} \cdot C_{\nu\mu}
        $.
        Standard syntactic constraints make manifest which expressions
        transform naturally.%with respect to optimization dynamics.
        %Appendix {\color{red} FILL IN} explains these conventions further.

        We say two expressions \emph{agree to order $\eta^d$} when their
        difference, divided by some homogeneous degree-$d$ polynomial of
        $\eta$, tends to $0$ as $\eta$ shrinks.  Their difference is then $\in
        o(\eta^d)$.
        
    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Names of SGD Parameters  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    \subsection{SGD terminology}
        SGD decreases the objective $l$ via $T$ steps of discrete-time
        $\eta$-steepest\footnote{
            To define ``steepness'' requires a metric on $l$'s domain.  We
            regard $\eta^{\mu\nu}$ as an (inverse) metric.
        } descent on the estimates $l_n$.
        %
        We describe SGD in terms of $N,T,B,E,M$:
            $N$ counts training points,
            $T$ counts updates,
            $B$ counts points per batch,
            $E=TN/B$ counts epochs, 
            and $M=E/B=T/N$.
            %\footnote{
            %    Since $\eta,N,M$ determine SGD's final loss on a noiseless,
            %    linear landscape, it is natural to compare SGD variants of
            %    equal M.
            %}
        Concretely, SGD performs $T=NM$ updates of the form:
        $$
            \theta^\mu
            \leftsquigarrow
            \theta^\mu -
            \eta^{\mu\nu} \nabla_\nu
                \wrap{\frac{1}{B} \sum_{n\in \Bb_t} l_n(\theta)}
        $$
        We write $l_t$ for the loss $\frac{1}{B}\sum_{\Bb_t} l_n$ on the $t$th
        batch. 
        %
        {\color{red} no VANILLA!}  
        %The cases $B=1$ and $B=N$ we call \emph{pure SGD} and \emph{pure GD}.
        %The $M=1$ case of pure SGD we call \emph{vanilla SGD}.

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Diagrams as Graphs  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    \subsection{Diagrams and embeddings}

        Though a rough, intuitive understanding of concepts such as
        \emph{diagram} suffices for absorbing this paper's main results, the
        following definitions may help the reader who wishes to follow our
        mathematics closely.

        \begin{defn}[Diagrams] \label{dfn:diagrams}
            A \emph{diagram} is a finite rooted tree equipped with a partition
            of its nodes.  We draw the tree using thin edges.  By convention,
            we draw each node to the right of its children; the root is thus
            always rightmost.  We draw the partition by connecting the nodes
            within each part via fuzzy ties.  For example,
            $\sdia{(012-3)(03-13-23)}$ has $2$ parts.
            %
            We insist on using as few fuzzy ties as possible so that, if $d$
            counts edges and $c$ counts ties, then $d+1-c$ counts the parts
            of the partition. 
            %
            There may be multiple ways to draw a single diagram, e.g.
            $\sdia{(01-23)(03-13-23)} = \sdia{(02-13)(03-13-23)}$. 
        \end{defn}

        \begin{defn}[Spacetime] 
            The \emph{spacetime} associated with an SGD run is the set of pairs
            $(n,t)$ where the $n$th datapoint participates in the $t$th
            gradient update.  Spacetimes thus encode batch size, training
            set size, and epoch number.
        \end{defn}

        \begin{defn}[Embedding Diagrams into Spacetime]
            An \emph{embedding} of a diagram $D$ into a spacetime is an
            assignment of $D$'s non-root nodes to pairs $(n,t)$ such that each
            node occurs at a time $t^\prime$ strictly after each of its
            children and such that two nodes occupy the same row $n$ if and
            only if they inhabit the same part of $D$'s partition.
        \end{defn}

        \begin{figure}[h] 
            \centering  
            \plotmooh{diagrams/spacetime-e}{}{0.26\columnwidth}
            \plotmooh{diagrams/spacetime-f}{}{0.26\columnwidth}
            \caption{
                {\bf Diagrams in Spacetime Depict SGD's Subprocesses.}
                Two spacetimes with $N=8, T=16$.
                {\bf Left}: Batchsize $B=1$ with inter-epoch shuffling. 
                    Embeddings, legal and illegal, of
                        $\sdia{(01-2)(01-12)}$,
                        $\sdia{(01-2)(01-12)}$, and
                        $\sdia{(0-1-2)(01-12)}$.
                {\bf Right}: Batchsize $B=2$ without inter-epoch shuffling. 
                    Interpretation of an order $\eta^4$ diagram embedding. 
            }
            \label{fig:spacetimes}
        \end{figure}

        To visualize embeddings, we draw the $(n,t)$ pairs of a space-time as
        shaded cells in an $N\times T$ grid.  A diagram embedding is then an
        assignment of nodes to shaded cells.  The $t<t^\prime$ constraint 
        forbids intra-cell edges (Figure \ref{fig:spacetimes} left), and we may
        interpret each edge as an effect of the past on the future (right).

        \begin{defn}[A Diagram's Un-resummed Value]
            The \emph{un-resummed value} of a diagram $D$ is the product of the
            values of each part $p$ in its partition.  The value
            of a part $p$ with $\wabs{p}$ many nodes is the expectation
            $\expc_x\wasq{(\nabla l_x(\theta))^{\wabs{p}}}$.  The edges of
            $D$'s tree indicate how to multiply the values of these
            parts: each edge indicates a contraction.  For instance,
            since the training points are independent:
            $$
                \sdia{(01-2-3)(02-12-23)}
                    \triangleq
                \expc_{
                    {\color{moor}n},
                    {\color{moog}n^\prime},
                    {\color{moob}n^{\prime\prime}}
                }\wasq{
                    (\nabla_\mu l_{\color{moor}n})
                    (\nabla_\nu l_{\color{moor}n})
                    (\nabla^\mu \nabla^\nu \nabla_\lambda l_{\color{moog}n^\prime})
                    (\nabla^\lambda l_{\color{moob}n^{\prime\prime}})
                }%(\theta_0)
            $$
            Implicit in the three raised indices are three factors of $\eta$.
            We denote $D$'s un-resummed value by $\dvalue(D)$, or by $D$ when
            clear.
        \end{defn}

        \begin{defn}[An Embedding's Re-summed Value]
            The \emph{re-summed value} $\rvalue_f(D)$ of an embedding $f$ of a
            diagram $D$ is the same as the un-resummed value of $D$, save for
            one change having to do with edges.  Consider an edge between two
            nodes embedded to $(n,t)$ and $(n^\prime, t+\Delta t)$.  Whereas
            $\dvalue(D)$ has a factor of $\eta^{\mu\nu}$ for this edge,
            $\rvalue_f(D)$ instead has a factor of
            $
                ((I-\eta H)^{\Delta t - 1})^\mu_\lambda \eta^{\lambda\nu}
            $.  Here, $1 \leq \Delta t$ is the temporal distance between
            the two nodes' embeddings.  
        \end{defn}

        We will often seek \emph{differences}, e.g.\ between ODE's and SGD's
        test loss or between a test loss and a train loss.  We thus define a
        compact notation for differences of diagrams:
        \begin{defn}[Fuzzy Outlines Denote Noise's Net Effect]
            A diagram drawn with one \emph{fuzzy outline} denotes the
            difference between the versions with and without fuzzy ties.  E.g.:
            $$
                \mdia{c(0-12)(01-12)}
                    \triangleq
                \mdia{(0-12)(01-12)}
                    -
                \mdia{(0-1-2)(01-12)}
            $$
            We define a diagram drawn with more than one fuzzy outline as the
            fully tied version minus all the versions with fewer fuzzy outlines
            (these are the M\"obius sums of \cite{ro64}):
            \begin{align*}
                \sdia{c(012-3)(01-13-23)}
                    &\triangleq
                \sdia{(012-3)(01-13-23)}
                    -
                \sdia{c(01-2-3)(01-13-23)}
                    -
                \sdia{c(02-1-3)(01-13-23)}
                    -
                \sdia{c(0-12-3)(01-13-23)}
                    -
                \sdia{(0-1-2-3)(01-13-23)} \\
                    &\triangleq
                \sdia{(012-3)(01-13-23)}
                    -
                \sdia{(01-2-3)(01-13-23)}
                    -
                \sdia{(02-1-3)(01-13-23)}
                    -
                \sdia{(0-12-3)(01-13-23)}
                    +
                2 \sdia{(0-1-2-3)(01-13-23)}
            \end{align*}
        \end{defn}

        \begin{defn}[Irreducible Diagrams]
            A diagram, drawn with fuzzy outlines instead of ties, is
            \emph{irreducible} when none of its degree-$2$ non-root nodes
            participates in fuzzy outlines. 
            So
            $\sdia{c(0-1-2)(02-12)},
            \sdia{c(01-2)(01-12)}$
            are irreducible, 
            but not
            $\sdia{c(0-1-2)(01-12)},
            \sdia{c(02-1-3)(01-12-23)}$.
        \end{defn}

%==============================================================================
%=====  DIAGRAM CALCULUS FOR SGD  =============================================
%==============================================================================

\section{Diagram Calculus for SGD} \label{sect:calculus}

    \subsection{Recipe for SGD's expected test loss}
        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        %~~~~~  Recipe for Test Loss  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
               
        Our Main Theorem expresses SGD's test loss as a sum over diagram
        embeddings.  Recalling that a diagram with $d$ edges is $O(\eta^d)$,
        we may read this Theorem as a Taylor series in the learning rate.  In
        practice, we truncate the series to small $d$, thus focusing on the
        few-edged diagrams.
        \begin{thm}[Test Loss as a Path Integral] \label{thm:resum}
            For any $T$: for $\eta$ sufficiently small, SGD's expected test
            loss is
            \begin{equation*} \label{eq:resum}
                \sum_{\substack{D \\ \text{irreducible}}}
                \sum_{\substack{\text{embeddings} \\ f}}
                \frac{1}{\wabs{\Aut_f(D)}}
                \frac{{\rvalue_f}(D)}{(-B)^{|\edges(D)|}}
            \end{equation*}
            Here, $D$ ranges through irreducible diagrams drawn with fuzzy
            outlines instead of ties, $f$ ranges through embeddings of $D$ into
            the SGD's spacetime, and $\wabs{\Aut_f(D)}$ counts the graph
            automorphisms of $D$ that preserve $f$'s assignment of nodes to
            $(n,t)$ pairs.  As a reminder, $B$ is the batch size. 
        \end{thm}
        Though the combinatorics of embeddings and graph automorphisms may seem
        forbidding, our focus on few-edged diagrams will make this counting
        nearly trivial.

        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        %~~~~~  Convergence  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
        \begin{thm}[Long-Term Behavior at a Local Minimum] \label{thm:converge}
            When SGD is initialized at a local minimum of test loss, and when
            $\nabla\nabla l_x$ is bounded below by some positive form that
            doesn't depend on $x$, then the $d$th-order truncation of Theorem 
            \ref{thm:resum} converges as $T$ diverges.
        \end{thm}

        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        %~~~~~  Simplifications  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
        \begin{rmk}[Approximation by integrals]
            In practice, we approximate sums over embeddings by integrals over
            times and $(I-\eta H)^t$ by $\exp(- \eta H t)$.  This incurs a
            multiplicative error of $1 + o(\eta)$ that preserves
            leading order results.  So diagrams induce easily evaluated
            integrals of exponentials.
        \end{rmk}

        \begin{rmk}[Using Un-resummed Values] \label{rmk:unresum}
            $\dvalue(D)$ is simpler to work with than $\rvalue_f(D)$.  Theorem
            \ref{thm:resum} remains true if we replace each $\rvalue_f(D)$ by
            $\dvalue(D)$, so long as we drop the constraint that $D$ be
            irreducible and we use diagrams drawn with fuzzy ties instead of
            fuzzy outlines.  However, Theorem \ref{thm:converge}'s convergence
            guarantee no longer applies, and empirically we find deteriorated
            predictions.
        \end{rmk}

        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        %~~~~~  Variants  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        
        \begin{rmk}[Variants]
            The above gives SGD's expected test loss.  What if we seek train
            instead of test losses?  Or net weight displacements instead of losses?
            Or variances instead of expectations?  
            Theorem \ref{thm:resum} and Remark \ref{rmk:unresum} have simple
            analogues for each of these $2^3$ possibilities, which we discuss
            in the appendix. 
        \end{rmk}

    \subsection{Single-Epoch, Singleton-Batch SGD}
        For SGD with $1$ epoch and batch size $1$, Theorem \ref{thm:resum} then
        specializes to: 
        \begin{prop} \label{prop:vanilla}
            Single-epoch singleton-batch SGD has expected test loss
            \begin{equation*}\label{eq:sgdbasiccoef}
                \sum_{0\leq d<\infty}
                \frac{(-1)^d}{d!} \sum_{D} 
                |\ords(D)| \, {N \choose P-1} \, \frac{d!}{\prod d_p!}
                \dvalue(D)
            \end{equation*}
            where $D$ has $P$ parts with sizes $d_p$.
            Here, $D$ ranges over $d$-edged diagrams none of whose parts
            contains any of its nodes' ancestors, and
            $|\ords(D)|$ counts the total orderings of $D$'s nodes s.t.\
            children precede parents and parts are contiguous.
        \end{prop}
        A diagram with $d$ thin edges and $c$ fuzzy ties (hence $d+1-c$
        parts) thus contributes $\Theta\wrap{(\eta T)^d T^{-c}}$ to SGD's test
        loss. 

        Intuitively, $\eta T$ measures the physical time of descent and
        $T^{-1}$ measures the coarseness of time discretization.  We thus
        regard Proposition \ref{prop:vanilla} as a double series in $(\eta T)^d
        T^{-c}$, where each term isolates the $d$th order effect of time and
        the $c$th order effect of noise.  Indeed, $c$ counts fuzzy ties and
        hence the $c=0$ terms do not model correlations and hence do not model
        noise.  That is, the $c=0$ terms give an ODE approximation to SGD.  The
        remaining terms give the corrections due to noise.  See Table
        \ref{tab:scatthree}. 

        \begin{table}[H]
            \centering 
            \resizebox{\columnwidth}{!}{%
            \begin{tabular}{c|c|c}
                {\LARGE $\Theta\left((\eta T)^3 T^{-0}\right)$} &
                {\LARGE $\Theta\left((\eta T)^3 T^{-1}\right)$} &
                {\LARGE $\Theta\left((\eta T)^3 T^{-2}\right)$} \\ \hline
                \begin{tabular}{c}
                    \begin{tabular}{LL}
                        \bdia{(0-1-2-3)(01-12-23)} & \bdia{(0-1-2-3)(01-13-23)}
                    \end{tabular} \\
                    \begin{tabular}{LL}
                        \bdia{(0-1-2-3)(02-13-23)} & \bdia{(0-1-2-3)(03-12-23)}
                    \end{tabular} \\ \hline
                    \begin{tabular}{LL}
                        \bdia{(0-1-2-3)(03-13-23)} & \bdia{(0-1-2-3)(02-12-23)}
                    \end{tabular}
                \end{tabular}
                &
                \begin{tabular}{c}
                    \begin{tabular}{LL}
                        \bdia{(01-2-3)(02-13-23)} & \bdia{(01-2-3)(03-12-23)}
                    \end{tabular} \\ \hline
                    \begin{tabular}{LL}
                        \bdia{(0-12-3)(01-13-23)} & \bdia{(0-12-3)(02-13-23)}
                    \end{tabular} \\ \hline
                    \begin{tabular}{LLL}
                        \bdia{(01-2-3)(03-13-23)} & \bdia{(0-12-3)(03-13-23)} & \bdia{(01-2-3)(02-12-23)} 
                    \end{tabular}
                \end{tabular}
                &
                \begin{tabular}{c}
                    \begin{tabular}{L}
                        \bdia{(012-3)(03-13-23)}
                    \end{tabular}
                \end{tabular}
            \end{tabular}
            }
            \caption{
                {\bf Degree-$3$ diagrams for $B=M=1$ SGD's test loss}.
                The $6$ diagrams have $(4+2)+(2+2+3)+(1)$ total orderings
                relevant to Proposition \ref{prop:vanilla}.
                {\bf Left:} $(d,c)=(3,0)$.  Diagrams for ODE behavior.
                {\bf Center:} $(d,c)=(3,1)$.  $1$st order deviation of SGD
                away from ODE.
                {\bf Right:} $(d,c)=(3,2)$.  $2$nd order deviation of SGD
                from ODE with appearance of non-Gaussian statistics.
            }
            \label{tab:scatthree}
        \end{table}
       
\section{Insights from the Formalism}

    \subsection{SGD descends on a $C$-smoothed landscape}

        Integrating $\rvalue_f(\sdia{(01-2-3)(02-12-23)})$ over embeddings $f$, we see:
        \begin{cor}[Minima flat w.r.t. $C$ attract SGD]\label{cor:entropic}
            Initialized at a test minimum, vanilla SGD's weight moves to
            order $\eta^2$ with a long-time-averaged\footnote{
                That is, $T$ so large that $C \exp(-\eta K T)$ is negligible.
                Appendix \ref{sect:calculations} gives a similar expression for general $T$.
            }
            expected velocity of
            $$
                v^\pi = C_{\mu \nu}
                \wrap{F^{-1}}^{\mu\nu}_{\rho\lambda}
                J^{\rho\lambda}_{\sigma}
                \wrap{\frac{I - \exp(-T \eta H)}{T \eta H} \eta}^{\sigma \pi}
            $$
            per timestep.
            Here, $F = \eta H \otimes I + I \otimes \eta H$, a
            $4$-valent tensor. 
        \end{cor}
        
        The intuition behind the Corollary is that the diagram
        $
            \sdia{c(01-2-3)(02-12-23)}
        $
        contains a subdiagram
        $
            \sdia{c(01-2)(02-12)}=CH
        $; by a routine check, this subdiagram
        is the leading-order loss increase when we convolve the landscape
        with a $C$-shaped Gaussian.  Since
        $
            \sdia{c(01-2-3)(02-12-23)}
        $ connects the subdiagram
        to the test measurement via 1 edge, it couples
        $
            \sdia{c(01-2)(02-12)}
        $ to the linear part of the test loss and hence represents a
        displacement of weights away from high $CH$.
        In short,
        $
            \sdia{c(01-2-3)(02-12-23)}
        $
        reveals that \emph{SGD descends on a covariance-smoothed landscape}.
        See Figure \ref{fig:cubicandspring} (right).

        An un-resummed version of this result was first reported by
        \citet{ya19b}; however, for fixed $T$, the un-resummed result scales
        with $\eta^3$ while Corollary \ref{cor:entropic} scales with $\eta^2$.
        The discrepancy occurs, intuitively, because the re-summed analysis
        accounts for the accumulation of noise from many updates, hence
        amplifying the contribution of $C$.  Our experiments verify our scaling
        law.

        Unlike \citet{we19b}, we make no assumptions of thermal equilibrium,
        fast-slow mode separation, or constant covariance.  This generality
        reveals a novel dynamical phenomenon, namely that
        the velocity field above need not be conservative (see Section
        \ref{subsect:entropic})
  
    \subsection{Curvature controls overfitting} \label{subsect:curvature-and-overfitting}

        Integrating $\rvalue_f(\sdia{(01-2)(02-12)})$ and
        $\rvalue_f(\sdia{(01)(01)})$ yields:
        \begin{cor}[Flat, Sharp Minima Overfit Less]\label{cor:overfit}
            Initialized at a test minimum, pure GD's test loss is to
            order $\eta$
            $$
                \frac{1}{2N} ~
                    C_{\mu\nu}
                    \wrap{(I - \exp(-\eta T H))^{\otimes 2}}^{\mu\nu}_{\rho\lambda}
                    \wrap{H^{-1}}^{\rho\lambda}
            $$
            above the minimum.  This vanishes when $H$ does. 
            Likewise, pure GD's generalization gap is to order $\eta$:  
            $$
                \frac{1}{N} ~
                    C_{\mu\nu}
                    \wrap{I - \exp(-\eta T H)}^{\nu}_{\lambda}
                    \wrap{H^{-1}}^{\lambda\mu}
            $$
            In contrast to the later-mentioned Takeuchi estimate, this does not
            diverge as $H$ shrinks.
        \end{cor}
        Corollary \ref{cor:overfit}'s generalization gap converges after large
        $T$ to $C_{\mu\nu}(H^{-1})^{\mu\nu}/N$, also known as Takeuchi's
        Information Criterion (TIC).  In turn,  $C=H$ is the Fisher metric in the classical setting of
        maximum likelihood (ML) estimation (in well-specified models) near the
        ``true'' test minimum, so we recover AIC $(\textnormal{number of
        parameters})/N$.  Unlike AIC, our more general expression is
        descendably smooth, may be used with MAP or ELBO tasks instead of just
        ML, and makes no model well-specification assumptions.

        \begin{figure}[h!]
            \centering
            \plotmooh{diagrams/entropic-force-diagram}{}{0.32\columnwidth} 
            \plotmooh{diagrams/sharp}{}{0.31\columnwidth}
            \caption{
                {\bf Re-summation reveals novel phenomena.}
                {\bf Left}:
                    The entropic force mechanism: gradient noise induces a flow
                    toward minima  \emph{with respect to to the
                    covariance}.  Though our analysis assumes neither thermal
                    equilibrium nor fast-slow mode separation, we label ``fast
                    and slow directions'' to ease comparison
                    with \citet{we19b}.  Here, red densities denote
                    the spread predicted by a re-summed $C^{\mu\nu}$, and
                    the spatial variation of curvature corresponds to
                    $J_{\mu\nu\lambda}$. 
                {\bf Right}:
                    Noise structure determines how curvature affects
                    overfitting.  Geometrically, for (empirical risk
                    minimization on) a vector-perturbed landscape, small
                    Hessians are favored (top row), while for
                    covector-perturbed landscapes, large Hessians are favored
                    (bottom row).  Corollary \ref{cor:overfit} shows how the
                    implicit regularization of fixed-$\eta T$ descent interpolates 
                    between the two rows.
            }
            \label{fig:cubicandspring}
        \end{figure}

    \subsection{Effects of epochs and of batch size} \label{subsect:epochs-batch}

        \begin{cor}[Epoch Number] \label{cor:epochs}
            To order $\eta^2$, one-epoch SGD has 
            $
                 \wrap{\frac{M-1}{M}}\wrap{\frac{B+1}{B}}\wrap{\frac{N}{2}}
                 \wrap{\nabla_\mu C^{\nu}_{\nu}} G^\mu / 2
            $
            less test loss than $M$-epoch SGD with learning rate $\eta/M$.
        \end{cor}
    
        Analyzing $\sdia{c(01-2)(01-12)}$, we find that we may cause GD to
        mimic SGD using any smooth unbiased estimator $\hat{C}$ of $C$:
        \begin{cor}[Batch Size] \label{cor:batch}
            The expected test loss of pure SGD is, to order $\eta^2$,
            less than that of pure GD by
            $
                  \frac{M(N-1)}{2} ~
                  \wrap{\nabla_\mu C^{\nu}_{\nu}} G^\mu / 2
            $.
            Moreover, GD on a modified loss 
            $
                \tilde l_n = l_n +
                    \frac{N-1}{4N} ~
                    \hat{C}_\nu^\nu(\theta)
            $
            has an expected test loss that agrees with SGD's to second order.
            We call this method GDC.
        \end{cor}
 
    \subsection{Non-Gaussian noise affects SGD but not SDE}

        Stochastic Differential Equations (SDE: see \citet{li18}) are a popular
        theoretical approximation to SGD, but SDE and SGD differ in several
        ways.  For instance, the inter-epoch noise correlations in multi-epoch
        SGD measurably affect SGD's final test loss (Corollary
        \ref{cor:epochs}), but SDE assumes uncorrelated gradient updates.  Even
        if we restrict to single-epoch SDE, differences arise due to time
        discretization and, more interestingly, due to non-gaussian noise. 
        %
        \begin{cor}[SGD Differs from ODE, SDE] \label{cor:vsode}
            The test loss of single-epoch, singleton-batch SGD deviates
            from that of ODE and SDE by
            $
                \frac{T}{2} ~ C_{\mu\nu} H^{\mu\nu} + o(\eta^2)
            $.
            The leading order deviation from SDE due to non-Gaussian noise is
            $
                - (T/6) \sdia{c(012-3)(03-13-23)}
                + o(\eta^3)
                =
                - (T/6) S_{\mu\nu\lambda} J^{\mu\nu\lambda} 
                + o(\eta^3)
            $.\footnote{
                This expression differs from the more exact expression of
                Example \ref{exm:first} because here we use Remark
                \ref{rmk:unresum}'s substitution.  One may check that the two
                expressions agree to leading order.
            }
        \end{cor}
        %
        For finite $N$, this Corollary separates SDE from SGD.  Conversely, as
        $N\to\infty$ with $\eta N$ fixed and $C$ scaling with $\sqrt{N}$, SGD
        converges to SDE, but generalization and optimization respectively
        become trivial and computationally intractable.

%==============================================================================
%    EXPERIMENTS AND APPLICATIONS
%==============================================================================

\section{Experiments}

    We focus on experiments whose rejection of the null hypothesis (and hence
    support of our theory) is so drastic as to be visually obvious.  For
    example, in Figure \ref{fig:thermoandtak}, \citep{ch18} predicts a velocity
    of $0$ while we predict a velocity of $\eta^2/6$.  
    %
    Throughout, \texttt{I} bars and \texttt{+} marks denote a 95\% confidence
    interval based on the standard error of the mean, in the vertical or
    vertical-and-horizontal directions, respectively.  See Appendix
    \ref{sect:landscape} for experimental procedure including architectures and
    sample size.

    %--------------------------------------------------------------------------
    %           Vanilla SGD; Epochs and Overfitting         
    %--------------------------------------------------------------------------

    \subsection{Basic predictions}
        We test Theorem \ref{thm:resum} and Remark \ref{rmk:unresum} on smooth
        convnets for CIFAR-10 and Fashion-MNIST.  Our order $\eta^3$
        predictions agree with experiment up to $\eta T \approx 10^0$ (Figure
        \ref{fig:vanilla}, left).  Also, Corollary \ref{cor:epochs}
        correctly predicts the effect of multi-epoch training (Appendix
        \ref{sect:figures}) for $\eta T \approx 10^{-1/2}$.  These tests verify
        that our proofs hide no mistakes of proportionality or sign.  

        \begin{figure}[h!] 
            \centering
            \plotmooh{plots/new-test-0}{}{3.0cm} 
            \plotmooh{plots/rebut-test-1-T100}{}{3.0cm} 
            \caption{
                {\bf Perturbation models SGD for small $\eta T$.}
                Test loss vs learning rate on a Fashion-MNIST convnet, with
                un-re-summed predictions.
                {\bf Left}: For the instance shown and all $11$ other
                initializations unshown, our degree-$3$ prediction
                agrees with experiment through $\eta T \approx 10^0$, which
                corresponds to a decrease in $0\mbox{-}1$ error of $\approx
                10^{-3}$.
                %
                {\bf Right}: For larger $\eta T$, our predictions can break
                down.  Here, the order-$3$ prediction holds until the $0\mbox{-}1$
                error improves by $5\cdot 10^{-3}$.  Beyond this, close
                agreement with experiment is coincidental.
            }
            \label{fig:vanilla}
        \end{figure}



    %--------------------------------------------------------------------------
    %           Emulating Small Batches with Large Ones     
    %--------------------------------------------------------------------------

    \subsection{Emulating small batches with large ones}
        By Corollary \ref{cor:batch}, SGD avoids high-$C$ regions more than GD
        We artificially correct GD accordingly, yielding an optimizer, GDC,
        that indeed behaves like SGD on a range of landscapes (Figure
        \ref{fig:batchandgen} (left)).  It may be important to emulate SGD's
        avoidance of high-$C$ regions because we $C$ controls the rate at which
        each new update increases the generalization gap\footnote{Reminder: for
        us, generalization gap is test minus train loss.} (Figure
        \ref{fig:batchandgen} (right)).
        
        \begin{figure}[h!] 
            \centering
            \plotmooh{plots/new-big-bm-new}{}{3.0cm}
            \plotmooh{plots/rebut-gen-cifar-lenet-4}{}{3.0cm}
            \caption{
                {\bf $C$ controls generalization and distinguishes GD from
                SGD.}
                {\bf Left}: With equal-scaled axes, this plot shows that GDC
                matches SGD (small vertical variation) better than GD matches
                SGD (large horizontal variation) in test loss, for a variety of
                learning rates ($\approx 10^{-3}-10^{-1}$) and initializations
                (zero and several Xavier-Glorot trials) on logistic and
                architectures for image classification.  Here, $T=10$. 
                {\bf Right}: CIFAR-10 generalization gaps.
                For the instance shown and all $11$ other
                initializations unshown, the degree-$2$ prediction agrees with
                experiment through $\eta T \approx 5\cdot 10^{-1}$.
            }
            \label{fig:batchandgen}
        \end{figure}

        The connection between generalization and covariance was first
        established by \citet{ro18} in the case $T=2$ and to order $\eta^2$. 
        In fact, that work conjectures the possibility of emulating GD with
        SGD.  This sub-section extends that work by generalizing to arbitrary
        $T$ and arbitrary orders $\eta^d$, and by concretely defining GDC.

        In these experiments, we used a covariance estimator $\hat C \propto
        \nabla l_x (\nabla l_x - \nabla l_y)$ evaluated on two batches $x, y$
        that evenly partition the train set.  For typical architectures, we may
        compute $\nabla \hat C$ with the same memory and time as the usual
        gradient $\nabla l_t$, up to a multiplicative constant. 

    %--------------------------------------------------------------------------
    %           Comparison to Continuous Time               
    %--------------------------------------------------------------------------

    \subsection{Comparison to continuous time} \label{subsect:gaussfit}
        Consider fitting a centered normal $\Nn(0, \sigma^2)$ to data $x$ drawn 
        i.i.d. from a centered standard normal.  We parameterize the landscape
        by $h=\log(\sigma^2)$ so that the Fisher information matches the
        standard dot product \citep{am98}.  The gradient at sample $x$ and
        weight $h$ is then $g_x(h) = (1-x^2\exp(-h))/2$.  Since $x\sim
        \Nn(0, 1)$, $g_x(h)$ will be affinely related to a chi-squared and in
        particular non-Gaussian.
        %At $h=0$, the expected gradient vanishes, so
        %each diagram with a singleton leaf evaluates to $0$.  The test loss of
        %SGD thus involves only the remaining diagrams; to $3$rd order, it is
        %$$
        %    \sdia{(0)()}
        %    +\frac{T}{2} \sdia{c(01-2)(02-12)}
        %    -{T\choose 2} \sdia{c(03-1-2)(01-12-23)}
        %    -\frac{T}{6} \sdia{c(012-3)(03-13-23)}
        %$$
        {\color{red} FIGURE} shows that even for this simple learning problem, 
        SGD and SDE differ as predicted.

    %--------------------------------------------------------------------------
    %           Thermodynamic Engine                        
    %--------------------------------------------------------------------------

    \subsection{Nonconservative entropic force} \label{subsect:entropic}
        To test Corollary \ref{cor:entropic}'s predicted force, we construct a
        counter-intuitive loss landscape wherein, for arbitrarily small
        learning rates, SGD steadily increases the weight's z component despite
        0 test gradient in that direction.  Our mechanism differs from that
        discovered by \citet{ch18}.  Specifically, because in this landscape
        the force is $\eta$-perpendicular to the image of $\eta C$, that work
        predicts an entropic force of $0$.  This disagreement in predictions is
        possible because our analysis does not make any assumptions of
        equilibrium, conservatism, or continuous time.
       
        So, even in a valley of global minima,
        SGD will move away from minima whose Hessian aligns with the current
        covariance.  However, by the time it moves, the new covariance might
        differ from the old one, and SGD will be repelled by different Hessians
        than before.  Setting the covariance to lag the Hessian by a phase, we
        construct a landscape in which this entropic force dominates.  This
        ``\emph{linear screw}'' landscape has
        %
        $3$-dimensional $w\in \RR^3$ (initialized to $0$) and
        %
        $1$-dimensional $x \sim \Nn(0, 1)$:
        $$
            l_x(w)
            \triangleq
            \frac{1}{2} H(z)(w, w) + x \cdot S(z)(w)  
        $$
        Here, $H(z)(w, w) = w_x^2 + w_y^2 + (\cos(z) w_x + \sin(z) w_y)^2$
        %
        and   $S(z)(w)    = \cos(z-\pi/4) w_x + \sin(z-\pi/4) w_y$.
        %
        There is a valley of global minima defined by $x=y=0$. 
        If SGD is initialized there, then to leading order in $\eta$ and for
        large $T$, the re-summed theory predicts a $z$-speed of $\eta^2/6$ 
        per timestep.  Our re-summed predictions agree for
        with experiment for $\eta T$ so large that the weight moves about $5$
        times the landscape's natural length scale of $2\pi$ (Figure
        \ref{fig:thermoandtak}, left).

        It is routine to check that, by stitching together copies of this
        example, we may cause SGD to travel along paths that are closed loops
        or unbounded curves.  We may even add a small linear component so
        that SGD steadily climbs uphill.  

    %--------------------------------------------------------------------------
    %           Sharp vs Flat Minima                        
    %--------------------------------------------------------------------------

    \subsection{Sharp and flat minima both overfit less} \label{subsect:overfit}

        Prior work has varyingly found that \emph{sharp} minima overfit less
        (after all, $l^2$ regularization increases curvature) or that
        \emph{flat} minima overfit less (after all, flat minima are more
        robust to small displacements in weight space).  Corollary
        \ref{cor:overfit} reconciles these competing intuitions by showing
        how the relationship of generalization and curvature depends on the
        learning task's noise structure and how the metric $\eta^{-1}$ mediates
        this distinction
        (Figure \ref{fig:cubicandspring}, right).
        
        \begin{figure}[h!] 
            \centering
            \plotmoow{plots/new-thermo-linear-screw}{0.48\columnwidth}{4.0cm}
            \plotmoow{plots/new-tak}{0.48\columnwidth}{4.0cm}
            \caption{
                {\bf Re-summed predictions excel even for large $\eta T$ for
                SGD near minima.}
                {\bf Left}: On Linear Screw, the persistent entropic force
                pushes the weight through a valley of global minima not at a
                $T^{1/2}$ diffusive rate but at a directional $T^1$ rate.
                Since Hessians and covariances are bounded throughout
                the valley and the effect appears for all sufficiently small
                $\eta$, the effect is not a pathological artifact of
                well-chosen learning rate or divergent covariance noise.  The
                net displacement of $\approx 10^{1.5}$ well exceeds the
                $z$-period of $2\pi$. 
                {\bf Right}: For Mean Estimation with fixed covariance and a
                range of Hessians, initialized at the true minimum, the test
                losses after fixed-$\eta T$ optimization are smallest for very
                small and very large curvatures.  This evidences our prediction
                that both sharp and flat minima overfit less and that TIC's
                singularity is suppressed.
            }
            \label{fig:thermoandtak}
        \end{figure}

        Because the TIC estimates a smooth hypothesis class's generalization
        gap, it is tempting to use it as an additive regularization term.
        However, since the TIC is singular where the Hessian is singular, it
        gives insensible results for over-parameterized models.  Indeed,
        \citet{di18} report numerical difficulties requiring an arbitrary
        cutoff. 

        Fortunately, by Corollary \ref{cor:overfit}, the implicit
        regularization of gradient descent both demands and enables a
        singularity-removing correction to the TIC (Figure
        \ref{fig:thermoandtak}, right).  
        %
        The resulting \emph{Stabilized TIC} (STIC) uses the metric $\eta^{-1}$
        implicit in gradient descent to threshold flat from sharp
        minima\footnote{
            The notion of $H$'s width depends on a choice of
            metric.  Prior work chooses this metric arbitrarily.  We show that
            choosing $\eta^{-1}$ is a natural choice because it leads to a
            prediction of the gen.\ gap.
        }.
        %
        It thus offers a principled method for
        optimizer-aware model selection easily compatible with automatic
        differentiation systems.  By descending on STIC, we may tune smooth
        hyperparameters such as $l_2$ coefficients.  Experiments on an
        artificial Mean Estimation problem (task in Appendix
        \ref{sect:landscape}, plot in Appendix \ref{sect:figures}) recommend
        STIC for model selection when $H$ is negligible compared to $C/N$ as in
        the noisy, small-$N$ regime.  Because diagonalization typically takes
        time cubic in dimension, exact STIC regularization is most useful for
        small models on noisy and limited data.

%==============================================================================
%    RELATED WORK    
%==============================================================================

\section{Related Work} \label{sect:related}

    %--------------------------------------------------------------------------
    %           History of SGD
    %--------------------------------------------------------------------------

    It was \citet{ki52} who, in uniting gradient descent \citep{ca47} with
    stochastic approximation \citep{ro51}, invented SGD.  Since the development
    of back-propagation for efficient differentiation \citep{we74}, SGD has
    been used to train connectionist models including neural networks
    \citep{bo91}, in recent years to remarkable success \citep{le15}.

    %--------------------------------------------------------------------------
    %           Analyzing Overfitting; Relevance of Optimization; SDE Errs  
    %--------------------------------------------------------------------------

    Several lines of work quantify the overfitting of SGD-trained networks
    \citep{ne17a}.  For instance, \citet{ba17} controls the Rademacher
    complexity of deep hypothesis classes, leading to generalization bounds
    that are optimizer-agnostic.  However, since SGD-trained networks
    generalize despite their seeming ability to shatter large sets
    \citep{zh17}, one infers that generalization arises from the aptness to
    data of not only architecture but also optimization \citep{ne17b}.  Others
    have focused on the implicit regularization of SGD itself, for instance by
    modeling descent via stochastic differential equations (SDEs) (e.g.
    \citet{ch18}).  However, per \citet{ya19a}, such continuous-time analyses
    cannot treat covariance correctly, and so they err when interpreting
    results about SDEs as results about SGD for finite trainsets.

    %--------------------------------------------------------------------------
    %           We Extend Dan's Approach                     
    %--------------------------------------------------------------------------

    Following \citet{ro18}, we avoid continuous-time approximations and
    Taylor-expand around $\eta=0$.  We hence
    extend that work beyond leading order and beyond $2$ time steps,
    allowing us to compare, for instance, the expected test losses of
    multi-epoch and one-epoch SGD.  We also quantify the overfitting effects of
    batch size, whence we propose a regularizer that causes large-batch
    GD to emulate small-batch SGD.  In doing so, we establish a precise version
    of the relationship --- between covariance, batch size, and generalization
    --- conjectured by \citet{ja18}.  
    
    %--------------------------------------------------------------------------
    %           Phenomenology of Rademacher Correlates such as Hessians
    %--------------------------------------------------------------------------

    While we make rigorous, architecture-agnostic predictions of learning
    curves, these predictions become vacuous for large $\eta$.  Other
    discrete-time dynamical analyses allow large $\eta$ by treating deep
    generalization phenomenologically, whether by fitting to an
    empirically-determined correlate of Rademacher bounds \citep{li18}, by
    exhibiting generalization of local minima \emph{flat} with respect to the
    standard metric (see \citet{ho17}, \citet{ke17}, \citet{wa18}), or by
    exhibiting generalization of local minima \emph{sharp} with respect to the
    standard metric (see \citet{st56}, \citet{di17}, \citet{wu18}).  Our work
    reconciles those seemingly clashing claims.
    
    %--------------------------------------------------------------------------
    %           Our Work vs Other Perturbative Approaches            
    %--------------------------------------------------------------------------

    Others have perturbatively analyzed descent:  \citet{dy19} perturb
    in inverse network width, employing Feynman-'t Hooft diagrams to correct
    the Gaussian Process approximation for a specific class of deep networks.
    Meanwhile, \cite{ch18} and \citet{li17} perturb in learning rate to second
    order by approximating noise between updates as Gaussian and uncorrelated.
    In neglecting correlations and heavy tails, that work neither extends to
    higher orders not describes SGD's generalization behavior.
    By contrast, we use Feynman-Penrose diagrams to compute test and train
    losses to arbitrary order in learning rate.  Our method accounts for
    non-Gaussian and correlated noise and applies to \emph{any} sufficiently
    smooth architecture.  For example, since our work does not rely on
    information-geometric relationships between $C$ and $H$
    \citep{am98}\footnote{
        Disagreement of $C$ and $H$ is typical in modern learning \citep{ro12,
        ku19}.
    }, it applies to inexact-likelihood landscapes such as VAEs'. 

%==============================================================================
%    CONCLUSION      
%==============================================================================

\section{Conclusion} \label{sect:concl}

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Summarize Contributions  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    We present a diagram-based method for studying stochastic optimization
    on short timescales.
    Theorem \ref{thm:resum} justifies long-time predictions of SGD's
    dynamics near minima.  Our theory answers the following questions.

    \textbf{Which Minima Overfit Less?}
    By analyzing $\sdia{c(01-2)(02-12)}$, we find that flat and sharp minima
    both overfit less than minima of curvature comparable to $(\eta T)^{-1}$.
    Flat minima are robust to vector-valued noise, sharp minima are robust to
    covector-valued noise, and medium minima attain the worst of both worlds.
    We thus reconcile prior intuitions that sharp \citep{ke17, wa18} or flat 
    \citep{di17, wu18} minima overfit worse.  These considerations lead us to a
    smooth generalization of AIC enabling hyperparameter tuning by gradient
    descent.

    \textbf{Which Minima Does SGD Prefer?}
    Analyzing $\sdia{c(01-2-3)(02-12-23)}$, we refine \citet{we19b} to
    nonconstant, nonisotropic covariance to reveal that SGD descends on a loss
    landscape smoothed by the \emph{current} covariance $C$.  In particular,
    SGD moves toward regions flat with respect to $C$.  As $C$ evolves, the
    smoothing mask and thus the effective landscape evolves.  These dynamics
    are generically nonconservative.  In contrast to \citet{ch18}'s SDE
    approximation, SGD does not generically converge to a limit cycle. 

    \textbf{Can GD Emulate SGD?}
    By analyzing $\sdia{c(01-2)(01-12)}$, we prove the conjecture of
    \citet{ro18}, that large-batch GD can be made to emulate small-batch SGD.
    We show how to do this by adding a multiple of an unbiased covariance
    estimator to the descent objective.  This emulation is significant because,
    while small batch sizes can lead to better generalization \citep{bo91},
    modern infrastructure increasingly rewards large batch sizes \citep{go18}.  

    %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    %~~~~~~~~~  Anticipate Criticism of Limitations  ~~~~~~~~~~~~~~~~~~~~~~~~~~

    \subsection{Consequences}

        Our analysis of which minima (among a valley of minima) SGD prefers
        --- and our characterization of when SGD overfits less in certain
        minima --- together offer insight into SGD's success in training
        over-parameterized models. 

        Our results may also help to analyze fine-tuning procedures
        such as the meta-learning of MAML \cite{fi17}.  Indeed, those methods
        seek models initialized near minima and tunable to new data
        through a small number of updates, a setting matched to our
        theory's assumptions.

        Since our predictions depend only on loss data near initialization,
        they break down after the weight moves far from initialization.  Our
        theory thus best applies to small-movement contexts, whether for long
        times (large $\eta T$) near an isolated minimum or for short times
        (small $\eta T$) in general.
        
        Yet, even short-time predictions show how curvature and noise ---
        and not just averaged gradients --- repel or attract SGD's current
        weight.  For example, we proved that SGD in a valley moves toward
        regions flat with respect to the current covariance $C$.
        %Initial data rarely suffices to predict long-time behavior because
        %landscapes can be arbitrarily complex.
        Much as meteorologists understand how warm and
        cold fronts interact despite the intractability of long-term weather
        forecasting, we quantify the counter-intuitive dynamics governing SGD's
        short-time behavior.\footnote{
            Because our analysis holds for any initialization, one may imagine
            SGD's coarse-grained trajectory as an integral curve of the vector
            field given by our theory.
        }
        Our results enhance the intuitions of practitioners --- e.g. that "SGD
        descends on the train loss" --- by summarizing the effect of noise in
        closed-form dynamical laws valid in each short-term interval of SGD's
        trajectory.

        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        %~~~~~  Discuss Limitations: Forces and Computational Difficulties  ~~~

        %As expected for truncated Taylor series, our perturbative predictions
        %of final test loss based on initial data typically disagree with
        %experiment for large $\eta T$.  Moreover, our predictions often involve
        %many-indexed tensors (such as $J$ or $(I\otimes I)/(\eta H \otimes I +
        %I \otimes \eta H)$) inconvenient to compute in real-world settings. 
        %
        %That said, we have demonstrated that our predictions agree with
        %experiment for $\eta T$ large enough that continuous time
        %approximations fail.  That is, we predict force laws different from
        %those of continuous-time approximations.  For example, we supplement
        %the intuition that ``SGD performs steepest descent on the train
        %landscape'' by showing that, even at a train loss minimum, SGD drifts
        %in a specific direction so as to dis-align $H$ from the current $C$.
        %To the practitioner, our contributions may thus play the same role as
        %that intuition of steepest descent.

    \subsection{Questions}

        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        %~~~~~  Ask Questions  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        The diagram method opens the door to exploration of Lagrangian
        formalisms and curved backgrounds\footnote{
            Landau and Lifshitz introduce these concepts
            \yrcite{la60, la51}.
        }:
        \begin{quest}
            Does some least-action principle govern SGD; if not, what is an
            essential obstacle to this characterization?
        \end{quest}
        Lagrange's least-action formalism intimately intertwines with the
        diagrams of physics.  Together, they afford a modular framework for
        introducing new interactions as new terms or diagram nodes.  In fact,
        we find that some \emph{higher-order} methods --- such as the
        Hessian-based update
        $
            \theta \leftsquigarrow
            \theta -
            (\eta^{-1} + \lambda \nabla \nabla l_t(\theta))^{-1}
            \nabla l_t(\theta)
        $
        parameterized by small $\eta, \lambda$ --- admit diagrammatic analysis
        when we represent the $\lambda$ term as a second type of diagram node.
        Though diagrams suffice for computation, it is Lagrangians that most
        deeply illuminate scaling and conservation laws.
        \begin{conj}[Riemann Curvature Regularizes]
            For small $\eta$, SGD's gen. gap decreases as sectional curvature
            grows.
        \end{conj}
        Though our work so far assumes a flat metric $\eta^{\mu\nu}$, it
        generalizes to curved weight spaces\footnote{
            One may represent the affine connection as a node, thus giving
            rise to non-tensorial and hence gauge-dependent diagrams.
        }.
        Curvature finds concrete application in the \emph{learning on
        manifolds} paradigm of \citet{ab07, zh16}, notably specialized to
        \citet{am98}'s \emph{natural gradient descent} and \citet{ni17}'s
        \emph{hyperbolic embeddings}.  We are optimistic our formalism may
        resolve conjectures such as above.

%==============================================================================
%    CODA               
%==============================================================================

    \newpage

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Broader Impacts  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\subsection{Broader Impacts}

    Though machine learning has the long-term potential for vast improvements
    in world-wide quality of life, it is today a source of enormous carbon
    emissions \cite{st19}.  Our analysis of SGD may lead to a reduced carbon
    footprint in three ways. 
     
    First, Section \ref{subsect:epochs-batch} shows how to modify the loss
    landscape so that large-batch GD enjoys the stochastic regularizing
    properties of small-batch SGD, dually, so that small-batch SGD enjoys the
    stability of large-batch GD.  By unchaining the effective batch size from
    the actual batch size, we raise the possibility of training neural networks
    on a wider range of hardware than currently practical.  For example,
    asynchronous concurrent small-batch SGD (e.g. \citet{ni11}) might require
    less inter-GPU communication and therefore less power.
     
    Second, Section \ref{sect:concl} discusses an application to meta-learning,
    which has the potential to decrease the per-task sample complexity and
    hence carbon footprint of modern ML.
     
    Third, the generalization of AIC developed in  Sections
    \ref{subsect:curvature-and-overfitting} and \ref{subsect:overfit} permits
    certain forms of model selection by gradient descent rather than brute
    force search.  This might drastically reduce the energy consumed during
    model selection.

    That said, insofar as our theory furthers practice, it may instead
    contribute to the rapidly growing popularity of GPU-intensive learning,
    thus negating the aforementioned benefits and accelerating climate change.

    More broadly, this paper analyzes \emph{optimization in the face of
    uncertainty}.  As ML systems deployed today must increasingly address
    privacy, adversaries, pedestrian safety, and bias reflected in training
    data, it becomes increasingly important to model the fact that training
    sets and test sets may differ.  By quantifying effect of sample noise in
    learning, our work contributes to this goal. 

    %By connecting to physics, we strengthen the bridge between
    %two vibrant research communities.

    %Highlight both benefits and risks from your research. The NeurIPS
    %requirement asks that Authors should take care to discuss both
    %positive and negative outcomes. Systematically doing so will help us
    %push through the various biases, personal and institutional, towards
    %overly positive or overly negative assessments.

    %Highlight uncertainties. Foreseeing the impacts of research,
    %especially basic research, is notoriously difficult. We recommend
    %acknowledging your uncertainties, while also not letting it stop you
    %from reflecting about impact.

    %Focus on tractable, neglected, and significant impacts. Scientific
    %research tends to have a bewildering array of potential impacts, more
    %so as the research is more foundational or the considered impacts are
    %more long term.  It will be infeasible to consider them all.

    %Think about impacts even for theoretical work. Theoretical work does
    %have downstream impact  thats after all a motivation for much
    %theoretical work  and so we encourage researchers to make an attempt,
    %perhaps reflecting on their subfield more broadly.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Acknowledgements  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\vfill
\subsection{Acknowledgements}
    We feel deep gratitude to
        Sho Yaida,
        Dan A. Roberts, and
        Josh Tenenbaum
    for posing several of the questions this work resolves and for their
    patient guidance.  We appreciate the generosity of
        Andrzej Banburski,
        Ben R. Bray,
        Jeff Lagarias,
        Sasha Rakhlin,
        Greg Wornell, and
        Wenli Zhao
    in critiquing our drafts.
    Without the encouragement of
        Jason Corso,
        Chloe Kleinfeldt,
        Alex Lew, 
        Ari Morcos, and
        David Schwab,
    this paper would not be.
    Finally, we thank our anonymous reviewers for inspiring an improved
    presentation of our work.

%==============================================================================
%    REFERENCES      
%==============================================================================

%\section*{References}
    \bibliography{perturb}
    \bibliographystyle{icml2019}

%==============================================================================
%    APPENDICES      
%==============================================================================

\clearpage
\newpage
\renewcommand{\thesection}{\Alph{section}}
\setcounter{section}{0}

\section*{Organization of Appendices}
    These three appendices respectively serve three functions:
    \begin{itemize}
        \item to explain how to calculate using diagrams;
        \item to prove our theorems, corollaries, and claims; and
        \item to specify our experimental methods and results.
    \end{itemize}

\section{How to Calculate Expected Test Losses Using Diagrams}
    Our work introduces a novel technique for calculating the expected learning
    curves of SGD in terms of statistics of the loss landscape near
    initialization.  Here, we explain this technique.  There are {\bf four
    steps} to computing the expected test loss after a specific number of
    gradient updates: 
    \begin{itemize}
        \item Based on the chosen optimization hyperparameters (namely, batch
              size, training set size, and number of epochs):
              {\bf draw the spacetime grid} that encodes these hyperparameters.
        \item Based on our desired level of precision,
              {\bf draw all the relevant embeddings} of diagrams into the
              spacetime.
        \item {\bf Evaluate each diagram embedding}.
        \item {\bf Sum the embeddings' values} to obtain the quantity of
              interest as a function of the learning rate.
    \end{itemize}

    After presenting a small, complete example calculation that follows these
    four steps, we explain how to perform each of these steps in its own
    sub-section.  We then discuss how diagrams often offer intuition as well as
    calculational help.  Though we focus on the computation of expected test
    losses, we explain how a small change in the above four steps allows for
    the computation also of variances (instead of expectations) and of train
    losses (instead of test losses).  We conclude by comparing direct
    calculation based on our Key Lemma to the diagram method; we point out when
    and why diagrams streamline computation.

    \subsection{An example calculation}                             %\label{subsect:simple-example}
        Let's compute the expected test loss of batchsize-$1$ SGD on $N$
        training points after $E$ epochs.  We'll do this calculation to order
        $\eta^2$, meaning that our answer will be a function of the learning
        rate $\eta$ and that its error will shrink faster than quadratically as
        $\eta$ becomes small. 

        First, we identify the relevant spacetime.  A spacetime is a set of
        cells indicating which training points are used in which gradient
        update.  Since our problem has $N E$ many updates, each on a
        batch of size $1$, there will be $N E\times 1$ many cells in the
        relevant spacetime.  We arrange these cells in a grid whose vertical
        axis indexes training points and whose horizontal axis indexes training 
        times:
        {\color{red} FILL IN}
        %The benefit of drawing a spacetime is that .  

        Next, we identify the relevant diagram embeddings.
        The benefit of drawing this diagram.  

        Then, we evaluate each diagram embedding.
        The benefit of drawing this diagram.  

        Finally, we sum the embeddings' values to arrive at an answer.
        The benefit of drawing this diagram.  

    \subsection{How to identify the relevant space-time}            %\label{subsect:draw-spacetime}
    \subsection{How to identify the relevant diagram embeddings}    %\label{subsect:draw-embeddings}
    \subsection{How to evaluate each embedding}                     %\label{subsect:evaluate-embeddings}
    \subsection{How to sum the embeddings' values}                  %\label{subsect:sum-embeddings}
    \subsection{Interpreting diagrams to build intuition}           %\label{subsect:interpret-diagrams}
    \subsection{How to solve variant problems}                      %\label{subsect:solve-variants}
    \subsection{Do diagrams streamline computation?}                %\label{subsect:diagrams-streamline}

\section{Assumptions and Proofs}
    \subsection{Setup and assumptions of our theory}                %\label{subsect:assumptions}
    \subsection{Proof of the Key Lemma}                             %\label{subsect:key-lemme}
    \subsection{Proof of Theorem 1}                                 %\label{subsect:un-resummed}
    \subsection{On Mobius inversion for resummed diagrams}          %\label{subsect:mobius}
    \subsection{Proof of Theorem 2}                                 %\label{subsect:resummed}
    \subsection{Proofs of corollaries}                              %\label{subsect:corollaries}
    \subsection{Proofs of miscellaneous claims}                     %\label{subsect:claims}

\section{Exerimental Methods and Results}
    \subsection{What loss landscapes did we use?}                   %\label{subsect:tutorial}
    \subsection{Implementing optimizers}                            %\label{subsect:tutorial}
    \subsection{Evaluating diagrams on a given landscape}           %\label{subsect:tutorial}
    \subsection{Software frameworks and hardware}                   %\label{subsect:tutorial}
    \subsection{Additional figures}                                 %\label{subsect:tutorial}

\subsection{Informal overview of the perturbative approach}

        Consider running SGD on $N$ training points for $T$ steps with learning
        rate $\eta$, starting at a weight $\theta_0$.  Our method expresses the
        expectation (over randomly sampled training sets) of quantities such as
        the final weight (or test or train loss) as a sum of diagrams, where
        each diagram evaluates to a statistic of the loss landscape at
        initialization.  Diagrams with $e$ edges contribute only $O(\eta^e)$ to
        the quantities of interest, so for small $\eta$ we sum only the
        few-edged diagrams and incur an $o(\eta^e)$ error term.

        The rule for evaluating diagrams is that each degree-$d$ node evaluates
        to the $d$th derivative of the test loss $l$ at $\theta_0$.  The
        edges indicate the order in which those derivatives are multiplied.
        Most simply, $\sdia{(0)()} = l(\theta_0)$, a $0$th derivative.  The
        diagram $\sdia{(0-1)(01)}$.
        evaluates to the dot product $\eta GG$, where $G=\nabla l$ is the
        gradient of the expected loss $l$, evaluated at $\theta_0$.  Likewise,
        $\sdia{(0-1-2)(01-12)} = \eta^2 GHG$, where $H=\nabla\nabla l$ is the
        hessian.  And $\sdia{(0-1-2-3)(02-12-23)} = \eta^3 GGJG$, where
        $J=\nabla\nabla\nabla l$ is $l$'s third
        derivative.%\footnotemark[\ref{note1}]

        A diagram tells us about the loss landscape but not about SGD
        parameters such as $T$ or inter-epoch shuffling.  We summarize those
        parameters as sets of pairs $(n, t)$, one for each participation of the
        $n$th datapoint in the $t$th update.  Full-batch GD will have $NT$ many
        pairs, for instance, while singeleton-batch SGD will have $T$ many
        pairs.

        Each of a diagram's nodes abstractly represents an event at such a
        pair, and we may \emph{embed} a diagram by assigning to each node a
        concrete pair $(n, t)$.  We will intuitively interpret an embedded edge
        from $(n,t)$ to $(n^\prime,t^\prime)$ (from left to right) as depicting
        information flow from training point $n$ at time $t$ to training point
        $n^\prime$ at time $t^\prime$.  Thus, we permit only embeddings whose
        edges have $t<t^\prime$.  The rightmost node represents measurement at
        test time, so we do not assign a pair $(n,t)$ to it; it does not
        participate in $t<t^\prime$ constraints.

        \begin{thm*}[Theorem \ref{thm:sgdcoef}, Informal]
            Fix SGD parameters, namely $N$, $T$, a batch size $B$, and a
            deterministic routine to sample each batch from a train set.   
            Sum the diagrams with at most $d$ edges, where a diagram with $e$
            edges and $c$ many embeddings is weighted by  
            $c/(-B)^{e}$.  This sum agrees with SGD's expected final test loss
            to order $o(\eta^d)$. 
        \end{thm*}

        \begin{exm} \label{exm:fst}
            What is SGD's expected test loss to order $\eta^1$?  Two
            diagrams have $\leq 1$ edges: $\sdia{(0)()} = l(\theta_0)$ and
            $\sdia{(0-1)(01)} = \eta GG$.  For SGD with batchsize $1$,
            $\sdia{(0-1)(01)}$ has $T$ many embeddings, since its rightmost
            node must represent the test measurement and its other node can
            represent any of $T$ many $(n,t)$ pairs.  By the Theorem, the
            answer is $l(\theta_0) - T \cdot \eta GG + o(\eta^1)$. 
        \end{exm}

        Example \ref{exm:fst} is well-known (e.g. \citet{ne04}).  Indeed, it 
        quantifies the intuition that, in each of $T$ steps, SGD moves the
        weight by $\eta G$ and hence decreases the loss by $\eta GG$.  The
        expression is exact for a noiseless linear landscape, but, because it
        fails to model how gradients depend on the current weight (curvature)
        or on the current training point (noise), it is typically an
        approximation.  Diagrams beyond $\sdia{(0-1)(01)}$ correct the
        expression by modeling curvature and noise.

        Like Example \ref{exm:fst}, our predictions depend only on loss data
        near $\theta_0$ and hence break down after the weight moves far from
        initialization.  Our theory thus best applies to small-movement
        contexts, whether for long times near a minimum or for short times in
        general.
        %\footnote{
        %    It is routine to check that SGD dynamics can efficiently simulate  
        %    a Turing Machine and thus that general long-time prediction should
        %    be intractable.
        %}
        For instance, we analyze SGD overfitting near a minimum (Corollary
        \ref{cor:overfit}).  Invoking Theorem \ref{thm:resum}
        %to find $T$ so large that SGD senses curvature and noise
        ,
        %yet small enough for our theory to hold, 
        we analyze how curvature and noise --- and not just
        gradients --- repel or attract the evolving weight (Corollary
        \ref{cor:entropic}).

    \subsection{Embeddings into spacetime}

        \begin{figure}[H] 
            \centering  
            \plotmooh{diagrams/spacetime-e}{}{0.26\columnwidth}
            \plotmooh{diagrams/spacetime-f}{}{0.26\columnwidth}
            \caption{
                {\bf Diagrams in Spacetime Depict SGD's Subprocesses.}
                Two spacetimes with $N=8, T=16$.
                {\bf Left}: Batchsize $B=1$ with inter-epoch shuffling. 
                    Embeddings, legal and illegal, of
                        $\sdia{(01-2)(01-12)}$,
                        $\sdia{(01-2)(01-12)}$, and
                        $\sdia{(0-1-2)(01-12)}$.
                {\bf Right}: Batchsize $B=2$ without inter-epoch shuffling. 
                    Interpretation of an order $\eta^4$ diagram embedding. 
            }
            \label{fig:spacetimes}
        \end{figure}

        To visualize embeddings, we draw $(n,t)$ pairs as shaded cells in
        an $N\times T$ grid.  A diagram embedding is then an assignment of
        nodes to shaded cells.  The $t<t^\prime$ constraint then forbids
        intra-cell edges (Figure \ref{fig:spacetimes} left), and we may
        interpret each edge as an effect of the past on the future (right).  We
        call an SGD run's set of shaded cells its \emph{spacetime}.

    \subsection{A first look at curvature and noise}

        Intuitively, our diagrams' edges depict higher derivatives and hence
        the test loss $l$'s curvature.  However, to study noise and
        generalization, we need to represent how different datapoints $n$
        induce different loss functions $l_n$.  The test loss $l$ is then the
        expectated value of $l_n$.
        %
        We depict correlations and hence noise by a new structure: fuzzy
        ``ties''.  For example, $\sdia{(0-1-2)(01-12)}$ and
        $\sdia{(01-2)(01-12)}$ are two valid and distinct diagrams.  Fuzzy ties
        determine which derivatives occur within the same expectation, so we
        have
        $$
            \sdia{(0-1-2)(01-12)}
                \triangleq
            \eta^2 \expct{\nabla l_n} \expct{\nabla\nabla l_n} \expct{\nabla l_n}
                =
            \eta^2 GHG  
                =
            \eta^2 \frac{\nabla\wrap{GG}}{2} G
        $$
        and, writing $C$ for the covariance of gradients,
        $$
            \sdia{(01-2)(01-12)}
                \triangleq
            \eta^2 \expct{\nabla l_n \nabla\nabla l_n} \expct{\nabla l_n}
                =
            \eta^2 \frac{\nabla\wrap{GG+C}}{2} G
        $$
        The rule is that two nodes connected by a fuzzy tie occur in the same
        expectation brackets.  Since fuzzy ties depict correlations, we demand
        that each embedding of a diagram sends any two nodes that are connected
        by a fuzzy tie to pairs $(n,t), (n, t^\prime)$ that share a training
        point index $n$.

        \begin{exm} \label{exm:sgdvs}
            When $N=T$, then singleton-batch SGD permits permits no
            concretizations of $\sdia{(01-2)(01-12)}$, since the edge
            constraint $t<t^\prime$ conflicts with the tie constraint
            $n=n^\prime$ when, as in this case, the permitted $(n,t)$ pairs
            comprise a bijection between $n$s and $t$s.  
        \end{exm}

        \begin{exm}
            By constrast, when $N=T$, full-batch GD permits $N{N \choose 2}$
            many concretizations of $\sdia{(01-2)(01-12)}$, since all $NT$
            possible $(n,t)$ pairs occur.  Those concretizations 
            $(n,t),(n,t^\prime)$ have as close analogues the concretizations
            $(n,t), (n^\prime,t^\prime)$ in Example \ref{exm:sgdvs}'s setting
            of the tie-less diagram $\sdia{(0-1-2)(01-12)}$.
        \end{exm}

        Comparing the two examples above reveals a difference between
        batchsize-$1$ and batchsize-$N$ descent for $N=T$: by the Main Theorem,
        the latter incurs an additional test loss
        $$
            \frac{c}{(-B)^e} \wrap{\sdia{(01-2)(01-12)} - \sdia{(0-1-2)(01-12)}}
                =
                \stackrel{\mathclap{\mbox{\emph{algebra}}}}{~~~~~\cdots~~~~~}
                =
            \frac{\eta^2 (N-1)}{4} G \nabla C
        $$
        It turns out that $\sdia{(01-2)(01-12)}$ is the only $2$-edged diagram
        whose concretizations in SGD and GD differ.  Thus, this test loss
        difference between SGD and GD is correct to order $\eta^2$.

        The above generalizes \citet{ro18}'s $T=2$ result, proved without
        diagrams, to arbitrary $T$.  In principle, one could avoid diagrams
        completely by direct use of our Key Lemma (stated in the Appendix).
        However, as shown in the Appendix, counting embeddings of diagrams
        streamlines calculation, yielding shorter and more interpretable
        arguments compared to direct calculation.

        \begin{defn}[Un-resummed Value of a Diagram]
            In the context of a loss landscape and an initial weight
            $\theta_0$, a diagram evaluates to the expectation (over all i.i.d.
            of datapoints to parts) of a product of derivatives, one $d$th
            derivative $\nabla^d l(\theta_0)$ for each degree-$d$ node.   Each
            edge denotes a contraction of its two nodes by the inverse metric
            $\eta$.  For example, 
            $$
                \sdia{(0-1)(01)}
                    \triangleq
                \expc_{
                    {\color{moor}n},
                    {\color{moog}n^\prime}
                }\wasq{
                    (\nabla_\mu l_{\color{moor}n})
                    (\nabla^\mu l_{\color{moog}n^\prime})
                }(\theta_0)
            $$
            $$
                \sdia{(01-2-3)(02-12-23)}
                    \triangleq
                \expc_{
                    {\color{moor}n},
                    {\color{moog}n^\prime},
                    {\color{moob}n^{\prime\prime}}
                }\wasq{
                    (\nabla_\mu l_{\color{moor}n})
                    (\nabla_\nu l_{\color{moor}n})
                    (\nabla^\mu \nabla^\nu \nabla_\lambda l_{\color{moog}n^\prime})
                    (\nabla^\lambda l_{\color{moob}n^{\prime\prime}})
                }(\theta_0)
            $$
            We write $\dvalue(D)$ for a diagram $D$'s
            value, or $D$ when clear.
        \end{defn}

        \begin{defn}[Fuzzy Outlines Denote Noise's Net Effect]
            We may join any two parts $p, \tilde p$ of a diagram $D$ to obtain
            a new diagram $D_{p\tilde p}$.  For instance,
            $
                (\sdia{(01-2-3)(02-12-23)})_{\text{\color{moor}red}~\text{\color{moob}blue}}
                    \triangleq
                \sdia{(013-2)(02-12-23)}
            $.
            Since fuzzy ties denote correlation and noise, differences such as
            $D_{p\tilde p}-D$ quantify noise's net effect.  So, for
            convenience, we define a diagram with fuzzy \emph{outlines} as the
            difference between its fuzzy tied and untied versions, e.g.:
            $$
                \sdia{c(0-12)(01-12)}
                    \triangleq
                (\sdia{(0-1-2)(01-12)})_{\text{\color{moog}green}~\text{\color{moob}blue}}
                    -
                \sdia{(0-1-2)(01-12)}
                    =
                \sdia{(0-12)(01-12)}
                    -
                \sdia{(0-1-2)(01-12)}
            $$
        \end{defn}

        \begin{defn}[Embedding-Sensitive Values]
            \label{defn:rvalue}
            Let $\rvalue_f(D)$ be the expected value of $D$'s corresponding
            tensor expression, where instead of using $\eta$ to contract
            two tensors embedded to times $t, t+\Delta t$, we use
            $
                K(\Delta t) = (I-\eta H)^{\Delta t - 1} \eta
            $.
            %
            Actually, it will be most convenient to let $\rvalue$s represent a
            \emph{difference} from the noiseless case.  For example, to compute
            $\rvalue(\sdia{(01-2)(01-12)})$, we will replace $\eta$ by
            $K(\Delta t)$ in $\value(\sdia{c(01-2)(01-12)})$ instead of in
            $\value(\sdia{(01-2)(01-12)})$. 
            %
            This way, each diagram represents a net effect of noise.  For the
            small diagrams we consider, we obtain $\rvalue$s by replacing fuzzy
            ties by fuzzy outlines; larger diagrams present complications
            addressed in Appendix \ref{subsubsect:mobius}.
        \end{defn}

        \begin{thm}[Test Loss as a Path Integral] \label{thm:sgdcoef}
            For all $T$: for $\eta$ sufficiently small, SGD's expected test
            loss is
            \begin{equation*}\label{eq:sgdcoef}
                \sum_{D}
                %\wrap{
                    \sum_{\text{embeddings}~f}
                    \frac{1}{\wabs{\Aut_f(D)}}
                %}
                \frac{\dvalue(D)}{(-B)^{|\edges(D)|}}
            \end{equation*}
            Here, $D$ is a diagram whose root $r$ does not participate in any fuzzy
            edge, $f$ is an embedding of $D$ into spacetime, and
            $\wabs{\Aut_f(D)}$ counts the graph-automorphisms of $D$ that
            preserve $f$'s assignment of nodes to cells.
            %
            If we replace $D$ by 
            $
                \wrap{-\sum_{p \in \parts(D)} (D_{rp} - D)/N}
            $, where $r$ is $D$'s root,
            we obtain the expected generalization gap (test minus train loss).
        \end{thm}
 
        %\begin{exm}
        By Proposition \ref{prop:vanilla}, a diagram with $d$ thin edges and
        $f$ fuzzy ties (hence $d+1-c$ parts), contributes $\Theta\wrap{(\eta
        T)^d T^{-c}}$ to vanilla SGD's test loss.  
        
        Intuitively, $\eta T$ measures the physical time of descent and
        $T^{-1}$ measures the coarseness of time discretization.  We thus
        obtain a double series in $(\eta T)^d T^{-c}$; the $c=0$ terms
        correspond to a noiseless, discretization-agnostic (hence ODE)
        approximation to SGD, the the remaining terms model time-discretization
        and noise.  See Table \ref{tab:scatthree}. 

                %\end{exm}


    %--------------------------------------------------------------------------
    %           Toward Effective Theories                   
    %--------------------------------------------------------------------------

    \subsection{Exploiting curvature} \label{subsect:effective}
        Intuitively, the order-$\eta^d$ truncation of Theorem
        \ref{thm:sgdcoef}'s series depends on simple loss statistics near
        initialization, so it will fail when $\eta T$ is large enough for the
        weight to drift far from initialization.  An especially interesting
        case where weights do not drift far is the case of SGD dynamics near an
        isolated minimum.  A generic minimum is characterized among critical
        points by its curvature, so we analyze the case where $H$ is positive.
        In doing so, we follow prior work that uses lower bounds on the loss
        landscape's curvature to restrict the hypothesis space to a small basin
        near a minimum and thus sharpen analyses of optimization and
        generalization \cite{ba05}.

        We will incorporate the positive-$H$ assumption into our theory via
        ``re-summation'' so that our re-summed order-$\eta^d$ predictions near
        isolated minima will remain finite for fixed $\eta T$ and arbitrary
        $T$.  More concretely, whenever we compute a diagram, we will also
        compute the unboundedly many cousins of that diagram that arise by
        inserting degree-$2$ nodes onto thin edges.  We will sum these
        diagrams' contributions to Theorem \ref{thm:sgdcoef}'s series, arriving
        at a closed form expression.  Theorem \ref{thm:resum} establishes the
        correctness of this approach.  Thus, by thoroughly incorporating
        curvature information, re-summation will help us reason about long-term
        equilibrium near an isolated minimum and short-term drifts within a
        valley of minima.

        To illustrate the idea, consider this class of topologically related
        diagrams:
        $
            \sdia{(0-1)(01)},
            \sdia{(0-1-2)(01-12)},
            \sdia{(0-1-2-3)(01-12-23)},
            \cdots
        $.
        Intuitively, these diagrams all represent the effect of the leftmost
        node on the rightmost node, with some number of degree-$2$ nodes
        mediating.  Since degree-$2$ nodes evaluate to hessians $H$, we regard
        these diagrams as versions of $\sdia{(0-1)(01)}$ modulated by
        curvature.  
        %
        Each of the above diagrams has some number of embeddings into spacetime.
        Here (but not in Theorem \ref{thm:resum}), we will for simplicity consider embeddings
        into vanilla SGD's spacetime.  Moreover, let us consider only embeddings
        that map the start and end nodes to fixed cells $(n_0,t_0)$ and $(n_+,t_+)$
        separated $\Delta t=t_+-t_0$ timesteps.  We will also temporarily relax
        the constraint on embeddings by allowing each of the middle nodes to occupy
        any row --- and in particular the same row as other nodes.\footnote{
            Because we here allow more embeddings than occur in Theorem
            \ref{thm:sgdcoef}, we are overcounting.  It turns out that our use
            of differences as mentioned in Definition \ref{defn:rvalue} leads
            to a telescoping cancellation that exactly counters this
            overcounting.  We offer mathematical details in Appendix E.4 and
            the proof of Theorem \ref{thm:resum}.  For now, we note that
            Theorem \ref{thm:resum} will abstract away the middle nodes
            altogether, meaning that the problem of overcounting is relevant
            only to proof details.
        }
        Then, a routine invocation of the
        Binomial Theorem shows that these embeddings together contribute the
        following to Theorem \ref{thm:sgdcoef}'s series:
        $$
            -G (I-\eta H)^{\Delta t-1} \eta G
        $$
        For comparison, the analogous embeddings (in this case, there is only
        one) of the smallest diagram $\sdia{(0-1)(01)}$ sum to 
        $$
            -G \eta G
        $$
        which matches like the overall sum if we replace $\eta$ by an
        ``effective learning rate''
        $$
            K(\Delta t) \triangleq (I-\eta H)^{\Delta t-1} \eta
        $$

        In the proof of Theorem \ref{thm:resum}, we see that this generalizes:
        in order to sum over a class of related diagrams' embeddings, we may
        sum over embeddings of the smallest diagram in that class, then
        replace each $\eta$ corresponding to a duration-$\Delta t$ edge by
        $K(\Delta t)$.

        \begin{exm}
            The family
            $
                \sdia{c(01-2)(01-12)},
                \sdia{c(02-1-3)(01-12-23)},
                \sdia{c(01-2-3)(01-13-23)},
                \cdots
            $
            includes variants of $\sdia{c(01-2)(01-12)}$ where we insert new
            nodes along $\sdia{c(01-2)(01-12)}$'s two thin edges.
            The diagram $\sdia{c(01-2)(01-12)}$ evaluates to
            $$
                \frac{1}{2}
                (\nabla_\mu C_{\nu\lambda}) \eta^{\nu\lambda} \eta^{\mu\rho} G_\rho 
            $$  
            So the overall family evaluates to  
            $$
                \frac{1}{2}
                (\nabla_\mu C_{\nu\lambda}) K(\Delta t)^{\nu\lambda} K(\Delta t)^{\mu\rho} G_\rho 
            $$
        \end{exm}

\end{document}
