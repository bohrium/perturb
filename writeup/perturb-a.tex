%==============================================================================
%    LATEX PREAMBLE  
%==============================================================================

\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{hyperref, xcolor}
\usepackage{amsmath, amssymb, amsthm, hanging, graphicx, txfonts, ifthen}
        
\usepackage[percent]{overpic}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newtheorem{klem}{Key Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{prop}{Proposition}

\usepackage{icml2019}
%\usepackage[accepted]{icml2019}

\usepackage{array}   % for \newcolumntype macro
\newcolumntype{L}{>{$}l<{$}}

\definecolor{moor}{rgb}{0.8,0.2,0.2}
\definecolor{moog}{rgb}{0.2,0.8,0.2}
\definecolor{moob}{rgb}{0.2,0.2,0.8}

\newcommand{\Free}{\mathcal{F}}
\newcommand{\Forg}{\mathcal{G}}
\newcommand{\Mod}{\mathcal{M}}
\newcommand{\Hom}{\text{\textnormal{Hom}}}
\newcommand{\Ss}{\mathcal{S}}
\newcommand{\Pp}{\mathcal{P}}
\newcommand{\SGD}{\text{\textnormal{SGD}}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\expc}{\mathbb{E}}
\newcommand{\expct}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\wrap}[1]{\left( #1 \right)}
\newcommand{\wang}[1]{\left\langle #1 \right\rangle}
\newcommand{\wive}[1]{\left\llbracket #1 \right\rrbracket}
\newcommand{\worm}[1]{\left\| #1 \right\|}

\newcommand{\plotplace}[1]{
    \begin{overpic}[width=4.0cm]{../plots/blank.png}
        \put( 5, 85){
            \begin{tabular}{p{3.0cm}}
                #1
            \end{tabular}
        }
    \end{overpic}
}

\newcommand{\bdia}[1]{\begin{gathered}\includegraphics[scale=0.22]{../diagrams/#1.png}\end{gathered}}
\newcommand{\dia} [1]{\begin{gathered}\includegraphics[scale=0.18]{../diagrams/#1.png}\end{gathered}}
\newcommand{\mdia}[1]{\begin{gathered}\includegraphics[scale=0.14]{../diagrams/#1.png}\end{gathered}}
\newcommand{\sdia}[1]{\begin{gathered}\includegraphics[scale=0.10]{../diagrams/#1.png}\end{gathered}}

\newcommand{\half}{\frac{1}{2}}
\newcommand{\sixth}{\frac{1}{6}}

\newcommand{\ofsix}[1]{
    {\tiny $\substack{
        \ifthenelse{\equal{#1}{0}}{\blacksquare}{\square}
        \ifthenelse{\equal{#1}{1}}{\blacksquare}{\square} \\
        \ifthenelse{\equal{#1}{2}}{\blacksquare}{\square} 
        \ifthenelse{\equal{#1}{3}}{\blacksquare}{\square} \\
        \ifthenelse{\equal{#1}{4}}{\blacksquare}{\square}
        \ifthenelse{\equal{#1}{5}}{\blacksquare}{\square}
    }$}
}


\newcommand{\lorem}[1]{
    Lorem ipsum dolor sit amet, consectetur adipiscing elit...\\
    \nopagebreak\vspace{#1cm} \ \\
    ...sunt in culpa qui officia deserunt mollit anim id est laborum.
}


\begin{document}

%==============================================================================
%    TITLE AND AUTHOR
%==============================================================================

\icmltitlerunning{Descent as Scattering}

\twocolumn[
    \icmltitle{A Space-Time Approach to Analyzing Stochastic Gradient Descent}
    
    \begin{icmlauthorlist}
        \icmlauthor{Samuel C.~Tenka}{mit}
    \end{icmlauthorlist}
    \icmlaffiliation{mit}{
        Computer Science and Artificial Intelligence Lab,
        Massachusetts Institute of Technology,
        Cambridge, Massachusetts, USA
    }
    \icmlcorrespondingauthor{Samuel C.~Tenka}{coli@mit.edu}
    
    \icmlkeywords{Machine Learning, SGD, ICML}
    
    \vskip 0.3in
]
\printAffiliationsAndNotice{}

%==============================================================================
%    ABSTRACT        
%==============================================================================

\begin{abstract}
    We present a diagrammatic calculus for reasoning about the behavior, at
    small learning rates, of SGD and its variants.  We interpret the diagrams
    as histories of scattering events, thus offering a new physical analogy for
    descent.  Illustrating this technique, we construct a regularizing term
    that causes large-batch GD to emulate small-batch SGD, present a
    model-selection heuristic that depends only on statistics measured before
    optimization, and exhibit a counter-intuitive loss landscape wherein SGD
    eternally cycles counterclockwise around a circle of minima. 
\end{abstract}

%==============================================================================
%    INTRODUCTION    
%==============================================================================

\section{Introduction}
    Stochastic gradient descent (SGD) decreases an unknown objective $l$ by
    performing discrete-time steepest descent on noisy estimates of $l$.  A key
    question is how the noise affects the final objective value.  We connect
    SGD dynamics to physical scattering theory, thus providing a quantitative
    and qualitative toolkit for answering this question.

    Specifically, we derive a diagram-based formalism for reasoning about SGD
    via a path integral over possible interactions between weights and data.
    The formalism permits perturbative analysis, leading to predictions of
    learning curves for small $\eta$.  Unlike the continuous-time limits of
    previous work, this framework models discrete time, and with it, the
    potential {\bf non-Gaussianity} of noise.  We thus obtain new results
    quantifying the {\bf effect of epoch number, batch size, and momentum} on
    SGD test loss.  We also contrast SGD against popular continuous-time
    approximations such as ordinary or stochastic differential equations (ODE,
    SDE).
    
    Path integrals offer not only quantitative predictions but also an exciting
    new viewpoint --- that of iterative optimization as a {\bf scattering
    process}.  Much as individual Feynman diagrams (see \citet{dy49a}) depict
    how local particle interactions compose into global outcomes, our diagrams
    depict how individual SGD updates influence each other before affecting a
    final test loss.  In fact, we import from physics tools such as {\bf
    crossing symmetries} (see \citet{dy49b}) and {\bf re-normalization} (see
    \citet{ge54}) to simplify our calculations and refine our estimates.
    The diagrams' combinatorial properties immediately yield several precise
    qualitative conclusions as well, for instance that to order $\eta^2$, {\bf
    inter-epoch} shuffling does not affect expected test loss.


%==============================================================================
%    RELATED WORK    
%==============================================================================

\subsection{Related Work}
    It was \citet{ki52} who, in uniting gradient descent \citep{ca47} with
    stochastic approximation \citep{ro51}, invented SGD.  Since the development
    of back-propagation for efficient differentiation \citep{we74}, SGD and its
    variants have been used to train connectionist models including neural
    networks \citep{bo91}, in recent years to remarkable success \citep{le15}.

    Several lines of work quantify the overfitting of SGD-trained networks
    \citep{ne17a}.  For instance, \citet{ba17} controls the Rademacher
    complexity of deep hypothesis classes, leading to generalization bounds
    that are post hoc or optimizer-agnostic.  However, since deep networks
    trained via SGD generalize despite their seeming ability to shatter large
    sets \citep{zh17}, one infers that generalization arises from the aptness
    to data of not only architecture but also optimization \citep{ne17b}.
    Others have focused on the implicit regularization of SGD itself, for
    instance by modeling descent via stochastic differential equations (e.g.
    \citet{ch18}).  However, as explained by \citet{ya19}, such continuous-time
    analyses cannot treat covariance correctly, and so they err when
    interpreting results about SDEs as results about SGD.

    Following \citet{ro18}, we avoid making a continuous-time approximation by
    instead Taylor-expanding around the learning rate $\eta=0$.  In fact, we
    develop a diagrammatic language for evaluating each Taylor term that is
    similar to and inspired by the field theory methods popularized by
    \citet{dy49a}.  Using this technique, we quantify the overfitting effects
    of batch size and epoch number, and based on this analysis, propose a
    regularizing term that causes large-batch GD to emulate small-batch SGD,
    thus establishing a precise version of the
    Covariance-BatchSize-Generalization relationship conjectured in
    \citet{ja18}.  
    
    While we make rigorous, architecture-agnostic predictions of learning
    curves, these predictions become vacuous for large $\eta$.  Other
    discrete-time dynamical analyses allow large $\eta$ by treating neural
    generalization phenomenologically, whether by fitting to an
    empirically-determined correlate of Rademacher bounds \citep{li18}, by
    exhibiting generalization of local minima {\bf flat} with respect to the
    standard metric (see \citet{ho17}, \citet{ke17}, citet{wa18}), or by
    exhibiting generalization of local minima {\bf sharp} with respect to the
    standard metric (see \citet{st56}, \citet{di17}, \citet{wu18}).  Our work,
    which makes explicit the dependence of generalization on the underlying
    metric and on the form of noise present, provides a framework to reconcile
    those latter, seemingly clashing claims.
    
    {\color{red} TODO: COMPARE TO CHAUDHARI!}
    
    {\color{red} TODO: COMPARE TO DYER and WEINAN E!}

    \lorem{3}


    IDEA: ASCENT?

%==============================================================================
%    BACKGROUND AND NOTATION
%==============================================================================

\section{Background and Notation}

\subsection{A Smooth Stage: Tensor Conventions}
    We adopt summation notation for Greek but not Roman indices, suppressing
    indices when convenient and clear.  To expedite dimensional analysis, we
    follow \cite{bo13} in considering the learning rate as an inverse metric
    $\eta^{\mu\nu}$ that converts a gradient (row vector) into a displacement
    (column vector).  Viewing $\eta^{-1}$ as the only available flat metric, we
    will use $\eta$ to raise indices; for example, with $C$ denoting the
    covariance of gradients, its ``trace'' will be $C^{\mu}_{\mu} =
    \eta^{\mu\nu} C_{\mu\nu}$.  The standard syntactic constraints on indexed
    expressions then give a strong check on which expressions transform
    naturally with respect to optimization dynamics.
    
    We assume that every all moments of the $0$th and higher derivatives of the
    losses $l_n$, considered as random functions on weight space, exist and are
    infinitely differentiable.
    
    Kol\'{a}\u{r} gives a careful introduction to these differential geometric
    ideas \yrcite{ko93}.

\subsection{Combinatorial Costumes: Structure Sets}
    We make use of \emph{structure sets}, i.e.  sets $S$ equipped with a
    preorder $\leq$ and an equivalence relation $\sim$.  The morphisms of
    structure sets are  non-decreasing maps that preserve $\sim$ and its
    negation.  A structure set is \emph{pointed} if it has a unique maximum
    element and this element forms a singleton $\sim$-class.  The categories
    $\Ss$ of structure sets and $\Pp$ of pointed structure sets enjoy a
    free-forgetful adjunction $\Free, \Forg$.  Modding out a structure set $S$
    by its $\sim$ yields another structure set $\Mod(S)$. 

    A \emph{diagram} is a rooted tree equipped with an equivalence relation on
    nodes.  We draw the tree of $\leq$ by thin edges, with the root at the far
    right, and we draw the equivalence relation $\sim$ by fuzzy ties.  By
    reading the tree as a Hasse graph, we see that each diagram $D$ induces a
    pointed structure set, by abuse of notation also named $D$.  A map from
    this induced $D$ to a total order with finest $\sim$ is an \emph{ordering}
    of $D$. 

    Fong gives a swift introduction to these category theoretic and
    diagrammatic ideas \yrcite{fo19}.
        
\subsection{The Parameterized \emph{Personae}: Forms of SGD}
    SGD decreases an objective $l$ by updating on smooth, unbiased i.i.d.
    estimates $(l_n: 0\leq n<N)$ of $l$.  The pattern of updates is determined
    by a structure set $\S$ whose preorder is a total preorder: for a map $\pi:S\to
    [N]$ that induces $\sim$, we define SGD inductively as
    $\text{SGD}_{S}(\theta) = \theta$ when $S$ is empty and otherwise
    $$
        \SGD_S(\theta) =
            \SGD_{S\setminus M}(\theta^\mu - \eta^{\mu\nu} \nabla_\nu l_{M}(\theta))
    $$
    where $M = \min S \subseteq S$ specifies a batch and $l_M =
    \frac{1}{M} \sum_{m\in M} l_{\pi(m)}$ is a batch average.  Since the
    distribution of $l_n$ is permutation invariant, the non-canonical choice
    of $\pi$ does not affect the distribution of output $\theta$s.

    Of special interest are structure sets that divide into $M\times B$ many
    \emph{epochs} each with $N/B$ many disjoint \emph{batches} of size $B$.  An
    SGD instance is then determined by $N, B, M$, and an \emph{inter-epoch
    shuffling scheme}.  The cases $B=1$ and $B=N$ we call \emph{pure SGD} and
    \emph{pure GD}.

\subsection{The Tempting Tool: Taylor Series}
    Intuitively, each descent step displaces $\theta$ by $-\eta \nabla l$ and
    hence decreases the loss $l(\theta)$ by $\eta (\nabla l)^2$; thus, we
    expect after $T$ steps a net decrease of $T \eta (\nabla l)^2$:
    \begin{equation} \label{eq:motone}
        l(\theta_T) \approx l(\theta_0) - T \cdot \eta \cdot (\nabla l(\theta_0))^2
    \end{equation}
    This intuition fails to capture two crucial facts: {\bf curvature} --- that
    as $\theta$ changes during training, so may $\nabla l(\theta)$ --- and {\bf
    noise} --- that $l_n$ and $l$ may differ.

    To account for noise, we should replace each $(\nabla l_t)(\nabla l)$ by
    an expectation.  If we are interested in train instead of test loss, 
    We get some expectations of the form $(\nabla l_t)(\nabla l_t)$, and hence
    obtain a different result than for test loss.

    To account for curvature, {\color{red} FILL IN}
       
%==============================================================================
%    DIAGRAM CALCULUS FOR SGD
%==============================================================================

\section{Diagram Calculus for SGD}
\subsection{Role of Diagrams}
    Suppose $s$ is smooth on weight space; for example, $s$ may be a test or
    train loss.  We may track $s(\theta)$ as $\theta$ is updated by SGD as
    follows:
    \begin{klem}
        The formal Maclaurin series of $s(\theta_T)$ with respect to $\eta$ is:
        \begin{equation*}\label{eq:dyson}
            \sum_{0\leq d < \infty} (-\eta)^d \sum_{\substack{(d_t: 0\leq t<T) \\ \sum_t d_t = d}}
            \left(
                \prod_{0 \leq t < T}
                    \left.  \frac{(g \nabla)^{d_t}}{d_t!} \right|_{g=\nabla l_t(\theta)}
            \right)
            s (\theta_0)
        \end{equation*}
    \end{klem}
    In averaging over training sets (and hence over the sequence $(l_t: 0\leq
    t<T)$ considered as a random variable), we may factor the expectation of
    the above product according to independence relations between the $l_t$.
    We view various training procedures (e.g. GD, SGD with(out) inter-epoch
    shuffling) as {\bf prescribing different independence relations} that lead
    to different factorizations and hence to potentially different
    generalization behavior at each order of $\eta$.

    An instance of the above product (for $s=l_a$ drawn from a test set and
    $0\leq c\leq b<T$) is
    $\eta^3 (\nabla l_c \nabla)^2 (\nabla l_b \nabla) l_a$, which is
    {\small
    \begin{align*}
          (\nabla^\lambda l_c) (\nabla^\mu l_c) (\nabla_\lambda \nabla_\mu \nabla^\nu l_b) (\nabla_\nu l_a)   
        + (\nabla^\lambda l_c) (\nabla^\mu l_c) (\nabla_\lambda \nabla^\nu l_b) (\nabla_\mu \nabla_\nu l_a) \\
        + (\nabla^\lambda l_c) (\nabla^\mu l_c) (\nabla_\mu \nabla^\nu l_b) (\nabla_\lambda \nabla_\nu l_a)   
        + (\nabla^\lambda l_c) (\nabla^\mu l_c) (\nabla^\nu l_b) (\nabla_\lambda \nabla_\mu \nabla_\nu l_a)
    \end{align*}
    }
    To reduce clutter, we adapt the string notation of \citet{pe71}.  Then, in
    expectation over $(l_c, l_b, l_a)$ drawn i.i.d.:
    \begin{align*}
        \cdots
        &= 
             \sdia{(01-2-3)(02-12-23)}
            +\sdia{(01-2-3)(02-13-23)}
            +\sdia{(01-2-3)(03-12-23)}
            +\sdia{(01-2-3)(03-13-23)} \\
        &=
            \underbrace{2\sdia{(01-2-3)(02-12-23)}}_{
                2~\expct{{\color{moor}(\nabla l)(\nabla l)}}~\expct{{\color{moog}\nabla\nabla\nabla l}}~\expct{{\color{moob} \nabla l}}
            }
            +
            \underbrace{2\sdia{(01-2-3)(02-13-23)}}_{
                2~\expct{{\color{moor}(\nabla l)(\nabla l)}}~\expct{{\color{moog}\nabla \nabla l}}~\expct{{\color{moob}\nabla \nabla l}}
            }
    \end{align*}
    Above, each node corresponds to a loss function (here, red for $l_c$, green
    for $l_b$, blue for $l_a$), differentiated $d$ times for a degree-$d$ node
    (for instance, $l_b$ is differentiated thrice in the first diagram and
    twice in the second).  {\bf Thin ``edges''} mark contractions by $\eta$.
    {\bf Fuzzy ``ties''} denote independence relationships by connecting
    identical loss functions (here, $l_c$ with $l_c$): nodes not connected by a
    path of fuzzy ties are independent.  The colors are redundant with the
    fuzzy ties and used only so that we may concisely refer to a specific node
    in prose.  Crucially, for a fixed, i.i.d. distribution over $(l_c, l_b,
    l_a)$, {\bf the topology of a diagram determines its expected value}.  For
    instance, $\expc \sdia{(01-2-3)(02-12-23)} = \expc
    \sdia{(01-2-3)(03-13-23)}$ because both are trees with two leaves tied.
    Thus follows the simplification on the second line above.  As shown with
    braces, we may convert back to explicit tensor expressions, invoking
    independence between untied nodes to factor the expression.  However, as we
    will see, the diagrammatic form of a tensor expression offers us physical
    intuition and guides us toward constructing unbiased estimators of the
    statistics they represent.  

    We define a diagram with fuzzy outlines instead of fuzzy ties to be the
    difference between the fuzzy tied version and the completely untied
    version: $\sdia{c(01-2)(01-12)} =
    \sdia{(01-2)(01-12)}-\sdia{(0-1-2)(01-12)}$. 
    
    The recipes for writing down test (or train) losses of SGD and its variants
    are straight-forward in the diagram notation because they reduce the
    problem of evaluating the previous dynamical expressions to the problem of
    counting isomorphic graphs.  The more complicated the direct computation,
    the greater the savings of using diagrams.  An appendix provides details
    and proofs for a variety of situations.  For now, we focus on the test loss
    of SGD.

\subsection{Recipe for the Test Loss of SGD}
    Our results all follow from this theorem and its analogues:
    \begin{thm}
        The order $\eta^d$ contribution to the expected test loss of SGD is:
        \begin{equation}\label{eq:sgdcoef}
            (-1)^d \sum_D \sum_{f: D\to\Free(S)} \prod_{i\in S} \frac{1}{|f^{-1}(i)|!} 
            D
        \end{equation}
        where $D$ ranges over (isomorphism classes of) diagrams with $d$ edges
        and $f$ ranges over morphisms in $\Pp$.
    \end{thm}

    In the special case of $B=1, M=1$:
    \begin{prop}
        The order $\eta^d$ contribution to the expected test loss of one-epoch
        SGD with singleton batches is:
        \begin{equation}\label{eq:sgdbasiccoef}
            \frac{(-1)^d}{d!} \sum_D |\Pp(D\to [P])| {N \choose P-1} {d \choose d_0,\cdots,d_{P-1}}
            D
        \end{equation}
        where $D$ ranges over $d$-edged diagrams whose equivalence classes
        are each totally disconnected (else, the coefficient is $0$) and have
        sizes $d_p: 0\leq p\leq P$, with $d_P=1$.
    \end{prop}
    A $P$-part, $d$-edged diagram then contributes $\Theta\left((\eta N)^d
    N^{P-d-1}\right)$ to the loss.  For example, there are six diagrams to
    third order, and they have $(4+2)+(2+2+3)+(1)$ many orderings.  See Table
    \ref{tab:scatthree}.  Intuitively, $\eta N$ measures the {\bf physical
    time} of optimization, and $1/N$ measures {\bf coarseness} of time
    discretization.  More precisely, we have a double-series in $(\eta N)^d
    N^{P-d-1}$, where $d$ counts thin edges and $d+1-P$ counts fuzzy ties; the
    $P=d+1$ terms give us a discretization-agnostic (hence continuous-time,
    noiseless) ODE approximation to SGD, while $P\leq d$ gives correction terms
    modeling time-discretization and hence noise.  
    \begin{cor}
        For one-epoch SGD on singleton batches through fixed physical time $T$:
        the order $N^{-1}$ deviation of SGD's test loss from ODE's is
        $
            \frac{T^2 N^{-1}}{2} \dia{c(01-2)(02-12)}
        $.
        The order $N^{-2}$ deviation of SGD's test loss due to non-gaussian
        noise is
        $
            \frac{T^3 N^{-2}}{6} \wrap{\sdia{c(012-3)(03-13-23)} - 3 \sdia{c(01-2-3)(03-13-23)}}
        $.
    \end{cor}
    For finite $N$, these effects make SDE different from SGD.  SDE also fails
    to model the correlations between updates in multiepoch SGD.  On the other
    hand, in the $N=\infty$ limit for which SDE matches SGD, optimization and
    generalization become computationally intractable and trivial,
    respectively. 

    \begin{table}[h!]
        \centering 
        \resizebox{\columnwidth}{!}{%
        \begin{tabular}{c|c|c}
            {\LARGE $\Theta\left((\eta N)^3 N^{-0}\right)$} &
            {\LARGE $\Theta\left((\eta N)^3 N^{-1}\right)$} &
            {\LARGE $\Theta\left((\eta N)^3 N^{-2}\right)$} \\ \hline
            \begin{tabular}{c}
                \begin{tabular}{LL}
                    \bdia{(0-1-2-3)(01-12-23)} & \bdia{(0-1-2-3)(01-13-23)}
                \end{tabular} \\
                \begin{tabular}{LL}
                    \bdia{(0-1-2-3)(02-13-23)} & \bdia{(0-1-2-3)(03-12-23)}
                \end{tabular} \\ \hline
                \begin{tabular}{LL}
                    \bdia{(0-1-2-3)(03-13-23)} & \bdia{(0-1-2-3)(02-12-23)}
                \end{tabular}
            \end{tabular}
            &
            \begin{tabular}{c}
                \begin{tabular}{LL}
                    \bdia{(01-2-3)(02-13-23)} & \bdia{(01-2-3)(03-12-23)}
                \end{tabular} \\ \hline
                \begin{tabular}{LL}
                    \bdia{(0-12-3)(01-13-23)} & \bdia{(0-12-3)(02-13-23)}
                \end{tabular} \\ \hline
                \begin{tabular}{LLL}
                    \bdia{(01-2-3)(03-13-23)} & \bdia{(0-12-3)(03-13-23)} & \bdia{(01-2-3)(02-12-23)} 
                \end{tabular}
            \end{tabular}
            &
            \begin{tabular}{c}
                \begin{tabular}{L}
                    \bdia{(012-3)(03-13-23)}
                \end{tabular}
            \end{tabular}
        \end{tabular}
        }
        \caption{
            Degree-$3$ scattering diagrams for $B=M=1$ SGD's test loss.
            {\bf Left:} $(d, P) = (3, 3)$.  Diagrams for ODE behavior.
            {\bf Center:} $(d, P) = (3, 2)$.  $1$st order deviation of SGD away from ODE.
            {\bf Right:} $(d, P) = (3, 1)$.  $2$nd order deviation of SGD from ODE with appearance of
            non-Gaussian statistics.
        }
        \label{tab:scatthree}
    \end{table}
    
    \begin{prop}
        To second order in $\eta$, the test loss of SGD --- on $N$
        samples for $M$ epochs with batch size $B$ dividing $N$ and with any
        shuffling scheme --- has expectation
        {\small
        \begin{align*}
                                                    \dia{(0)()}
            &- MN                                   \dia{(0-1)(01)}
             + MN\wrap{MN - \frac{1}{2}}            \dia{(0-1-2)(01-12)} \\
            &+ MN\wrap{\frac{M}{2}}                 \dia{c(01-2)(02-12)}  
             + MN\wrap{\frac{M-\frac{1}{B}}{2}}     \dia{c(01-2)(01-12)}
        \end{align*}
        }
    \end{prop}

    \begin{cor}
        To second order in $\eta$, inter-epoch shuffling doesn't affect SGD's
        expected test loss.
    \end{cor}

    \begin{cor}
        To second order in $\eta$, one-epoch SGD has 
        $
             \wrap{\frac{M-1}{M}}\wrap{\frac{B+1}{B}}\wrap{\frac{N}{2}} \mdia{c(01-2)(01-12)}
        $
        less test loss than $M$-epoch SGD with learning rate $\eta/M$. 
    \end{cor}

    Given an unbiased estimator $\hat{C}$ of gradient covariance, we may get
    GD to mimic SGD:
    \begin{cor}
        The expected test loss of pure SGD is, to second order in $\eta$,
        less than that of pure GD by
        $
              \wrap{\frac{M}{2}} \wrap{\frac{N-1}{N}} \sdia{c(01-2)(01-12)}
        $.
        Moreover, GD on a modified loss 
        $
            \tilde l_n = l_n + \wrap{\frac{N-1}{4N^2}} \hat{C}_\nu^\nu(\theta)
        $
        has an expected test loss that agrees with SGD's to second order.
    \end{cor}

\subsection{Renormalization}
    An important idea is that of renormalization, i.e. the summarization of  
    myriad small-scale interactions into an effective large-scale theory. 
    We can use this two ways:
        ({\bf A}) to refine our computations if we know the hessian;  
        ({\bf B}) to refine our computations if we know the ``effective propagator''.
    \lorem{3}
    \lorem{3}

\subsection{Descent as Scattering}
    \lorem{3}
    \lorem{3}
    \begin{figure}[h!]
        \centering
        \plotplace{space time with some diagrams}
        \plotplace{one diagram, many embeddings}
        \plotplace{interepoch shuffling}
        \plotplace{multiepoch vs gd}
        \caption{\lorem{2}}
    \end{figure}

%==============================================================================
%    PREDICTIONS AND APPLICATIONS
%==============================================================================

\section{Consequences and Applications}

\subsection{Vanilla SGD}
    For single-epoch SGD with singleton batches, we sum all relevant diagrams
    through order $3$; the coefficients $4, 2; 2, 2, 3; 1$ come from counting
    the elements of Table \ref{tab:scatthree}, and the other coefficients come
    from analogous tables.  This yields:
    \begin{align*}
            &\mathcal{L}^{\text{SGD}}_\text{test}(T, \eta) \in                   
        \\ 
               \sdia{(0)()}
            &- \frac{\eta}{1!}   {T \choose 1} \wrap{\sdia{(0-1)(01)}}
        \\
            &+ \frac{\eta^2}{1!1!} {T \choose 2} \wrap{2 \sdia{(0-1-2)(01-12)}} 
             + \frac{\eta^2}{2!} {T \choose 1} \wrap{\sdia{(01-2)(02-12)}}
        \\
            &- \frac{\eta^3}{1!1!1!} {T \choose 3} \wrap{
                       4 \sdia{(0-1-2-3)(01-12-23)}+
                       2 \sdia{(0-1-2-3)(03-13-23)}
                   }
        \\
            &- \frac{\eta^3}{2!1!} {T \choose 2} \wrap{
                       2         \sdia{(01-2-3)(03-12-23)}+
                       2    \sdia{(0-12-3)(02-13-23)}+
                       3     \sdia{(01-2-3)(02-12-23)}
                   }
        \\
            &- \frac{\eta^3}{3!} {T \choose 1} \wrap{\sdia{(012-3)(03-13-23)}}
            + o(\eta^3)\footnotemark
    \end{align*}
    \footnotetext{
        We use little-$o(\eta^d)$ instead of big-$O(\eta^{d+1})$ to avoid specializing to analytic functions.
        Error terms depend on the loss landscape and on $T$.  When gradients are uniformly bounded,
        the $T$ dependence is at most linear.
    }
    By contrast, the generalization gap $\mathcal{L}^{\text{SGD}}_\text{gen} =
    \mathcal{L}^{\text{SGD}}_\text{test} -
    \mathcal{L}^{\text{SGD}}_\text{train}$ is suppressed by a factor $1/N$ ($N
    \leq T$):
    \begin{align*}
        &N \cdot \mathcal{L}^{\text{SGD}}_\text{gen}(T, \eta) \in
        \\
        &+ \eta   {T \choose 1} \wrap{\sdia{(01)(01)} - \sdia{(0-1)(01)}} 
        - \eta^2 {T \choose 2} \wrap{\sdia{(01-2)(01-12)} + \sdia{(02-1)(01-12)}- 2\sdia{(0-1-2)(01-12)}} \\
        &- \frac{\eta^2}{2!} {T \choose 1} \wrap{\sdia{(012)(02-12)} - \sdia{(01-2)(02-12)}} 
         + o(\eta^2) 
        %\\
        %=&+ \eta^{\lambda \mu} {T \choose 1} C_{\lambda \mu} 
        %- \eta^{\lambda \mu} \eta^{\nu \xi} {T \choose 2} \wrap{ \frac{1}{2} G_\lambda \nabla_\mu C_{\nu \xi} + C_{\mu \nu} H_{\xi \lambda} }
        %\\
        %&- \frac{\eta^{\lambda \mu} \eta^{\nu \xi}}{2!} {T \choose 1} \wrap{ \wang{l_\lambda l_\nu l_{\mu \xi}} - (G_{\mu}G_{\nu} + C_{\mu\nu})H_{\xi \lambda} }
        % + o(\eta^2) \\
    \end{align*}
    The leading order term is $N \cdot \mathcal{L}^{\text{SGD}}_\text{gen}(T, \eta) \approx \eta T \wrap{\sdia{(01)(01)} - \sdia{(0-1)(01)}} = T \cdot \eta^{\lambda\mu} C_{\lambda\mu}$,
    where $C$ is the covariance of gradients.  We thus recover a main result of \citet{ro18}.
    \lorem{3}
    \lorem{3}
    \begin{figure}[h!]
        \centering
        \plotplace{small T: eta curve} \plotplace{test loss decrease near minimum} \\
        \plotplace{small T gen gap: actual vs predicted} \plotplace{nongaussian example}
        \caption{\lorem{2}}
    \end{figure}

\subsection{Emulating Small Batches with Large Ones}
    \lorem{3}
    \lorem{3}
    \lorem{3}
    \begin{figure}[h!]
        \centering
        \plotplace{batch matching over one init}
        \plotplace{batch matching over multiple inits} \\
        \plotplace{scan over betas} 
        \plotplace{summary over many models}
        \caption{\lorem{2}}
    \end{figure}

\subsection{Analyzing Second Order Methods}
    We demonstrate how our approach extends to more sophisticated optimizers by
    analyzing momentum and a hessian-based method. 

    \lorem{3}
    momentum

    Now consider a hessian-based update parameterized by a scalar $\lambda$: 
    $$
        \theta \leftsquigarrow
        \theta -
        (\eta^{-1} + \lambda \nabla \nabla l_t(\theta))^{-1} \nabla l_t(\theta)
    $$

    invhess
    \begin{figure}[h!]
        \centering
        \plotplace{momentum} 
        \plotplace{invhess}
        \caption{\lorem{2}}
    \end{figure}

\subsection{Epochs and Overfitting}
    \lorem{3}
    \begin{figure}[h!]
        \centering
        \plotplace{multiepoch vs sgd limit}
        \plotplace{multiepoch vs gd limit}
        \caption{\lorem{2}}
    \end{figure}

\subsection{Myopic Model Selection}
    \lorem{3}
    \begin{figure}[h!]
        \centering
        \plotplace{rankings: actual vs predicted}
        \plotplace{architecture vs optimization ease}
        \caption{\lorem{2}}
    \end{figure}

\subsection{Comparison to Continuous Time}
    \lorem{3}
    \lorem{3}
    Also, sgd interepoch correlations
    \begin{figure}[h!]
        \centering
        \plotplace{distinguishing landscape}
        \plotplace{ode vs sde vs sgd performance on landscape}
        \caption{\lorem{2}}
    \end{figure}

\subsection{Thermodynamic Engine}
    We clarify  
    \lorem{3}
    \begin{figure}[h!]
        \centering
        \plotplace{loss landscape: mean and covariance}
        \plotplace{net theta vs time: ours, chaudhari, naive}
        \caption{\lorem{2}}
    \end{figure}
    We constructed a counter-intuitive loss landscape wherein, for arbitrarily
    small learning rates, SGD cycles counterclockwise around a circle of
    minima.  Our mechanism differs from that discovered by \citet{ch18}
    {\color{red}  discuss the thermodynamic significance of both}


%==============================================================================
%    CONCLUSION      
%==============================================================================

\section{Conclusion}
    We presented a novel diagrammatic tool for analyzing gradient-based
    descent.  By presenting a new regularizing term, we showed that {\bf
    large-batch GD can be made to emulate small-batch SGD}, thus completing a
    project suggested by \citet{ro18}.  This is significant because, while
    small batch sizes can lead to better generalization \citep{bo91}, modern
    infrastructure increasingly rewards large batch sizes \citep{go18}.      
    We showed also that in multi-epoch SGD, inter-epoch shuffling induces only
    a $3$rd order effect on test loss.  Intuitively, we proved that {\bf the
    hessian matters asymptotically more than  shuffling order}.

    The diagram method is also a rich source of intuitions and physical
    analogies.  For example, it offers a clearer understanding of the
    empirically verified limit cycles found in Chaudhari.  As our physical
    analogy emphasizes the underlying metric, it reconciles competing views
    of whether sharp or flat minima generalize.  Further exploration of this
    bridge to particle physics, especially within the framework of
    renormalization theory, pose a promising direction for future research.

    Variances

%==============================================================================
%    ACKNOWLEDGEMENTS
%==============================================================================

\subsection{Acknowledgements}
    We thank Dan A. Roberts and Sho Yaida for patient introductions to their
    work and for precisely posing several of the questions we answer here.  We
    feel deeply grateful to Sho Yaida and Josh B. Tenenbaum for their
    compassionate guidance.  We appreciate the generosity of
        Andrzej Banburski
        and
        Wenli Zhao
    in offering crucial feedback on writing.

%==============================================================================
%    REFERENCES      
%==============================================================================

%\section*{References}
    \bibliography{perturb}
    \bibliographystyle{icml2019}

    \lorem{3}
    \lorem{3}

%==============================================================================
%    APPENDICES      
%==============================================================================

\section*{A. Derivation of Diagram Rules}

\subsection{Dyson Series for Iterative Optimizers}
    If a density $\rho$ governs a point $\theta$ in weight space, then after a
    sequence of updates $\theta \mapsto \theta - \eta^{\mu\nu} \nabla_\mu
    l(\theta)$ on losses $(l_t: 0\leq t < T)$, the following density (up to an
    error term whose Taylor series vanishes; all perturbative results will
    implicitly carry such terms) will govern the new point:
    \begin{equation}\label{eq:descexp}
        \exp\left(+\eta^{\mu\nu} \nabla_\mu l_{T-1}(\theta) \nabla_\nu\right) \cdots \exp\left(+\eta^{\mu\nu} \nabla_\mu l_0(\theta) \nabla_\nu\right) \rho
    \end{equation}
    or
    $
        \prod \exp\left(+\eta \nabla l \nabla\right) \rho
    $
    for short.
    The exponent above is a linear operator that acts on a space of
    sufficiently smooth maps; in particular, the $\nabla_\nu$ does not act on
    the $\nabla_\mu l(\theta)$ with which it pairs.  Integrating by parts, we
    write the expectation over initial values after $T$ steps of a function $s$
    of weight space (e.g. $s$ may be test or train loss) as:
    \begin{align}\label{eq:contraexp}
        %&\int_\theta \left(\prod_{T > t \geq 0} \exp\left(+\eta^{\mu\nu} \nabla_\mu l(\theta) \nabla_\nu\right) \rho\right)(\theta) s(\theta)
        %= \\
        &\int_\theta \rho(\theta) \left(\prod_{0 \leq t \leq T} \exp\left(-\eta^{\mu\nu} \nabla_\mu l(\theta) \nabla_\nu\right) s\right)(\theta)
    \end{align}
    Since the exponentials above might not commute, we may not compose
    the product of exponentials into an exponential of a sum.  We instead
    compute an expansion in powers of $\eta$.  Setting the initialization
    $\rho(\theta) = \delta(\theta-\theta_0)$ to be deterministic, and labeling
    as $\theta_t$ the weight after $t$ steps, we find:
    \begin{equation}\label{eq:dyson}
        s(\theta_T) =
        %\left(\prod_{T > t \geq 0} \exp\left(-\eta^{\mu\nu} \nabla_\mu l_t(\theta) \nabla_\nu\right) s\right) (\theta_0)
        %= 
        \sum_{0\leq d < \infty} (-\eta)^d \sum_{\substack{(d_t: 0\leq t<T) \\ \sum_t d_t = d}}
        \left(\prod_{0 \leq t < T} \frac{(\nabla l_t(\theta) \nabla)^{d_t}}{d_t!}\right) s (\theta_0)
    \end{equation}

    \lorem{3}
    \lorem{3}
    \lorem{3}
    \lorem{3}

\section*{B. Tutorial on Diagram Rules}
    \lorem{3}
    \lorem{3}
    \lorem{3}

\section*{C. Derivations of Perturbative Results}
    \lorem{3}
    \lorem{3}
    \lorem{3}
    \lorem{3}
    \lorem{3}

\section*{D. Diagram Rules vs Direct Perturbation}
    \lorem{3}
    \lorem{3}
    \lorem{3}
    \lorem{3}
    \lorem{3}

\section*{E. The $\eta$-Series' Domain of Convergence}
    \lorem{3}
    \lorem{3}
    \lorem{3}

\section*{F. Autonomous ODE Fitting}
    We fit a .
    These have the benefit 
    In particular:
    \begin{align}
        y\prime(t) =               a     &\to&   y(t) =  y(0) + a t                      \\
        y\prime(t) =         b y + a     &\to&   y(t) = (y(0) - (a/b)) \exp(b t) + (a/b) \\
        y\prime(t) = c y^2 + b y + a     &\to&                                             
    \end{align}

    \lorem{3}

\section*{G. Generalized Bessel Factors}
    \lorem{3}
    \lorem{3}
    \lorem{3}



\end{document}

