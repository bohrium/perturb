%==============================================================================
%    LATEX PREAMBLE  
%==============================================================================

\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{hyperref, xcolor}
\usepackage{amsmath, amssymb, amsthm, hanging, graphicx, txfonts, ifthen}
        
\usepackage[percent]{overpic}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newtheorem{klem}{Key Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{prop}{Proposition}

\usepackage{icml2019}
%\usepackage[accepted]{icml2019}

\usepackage{array}   % for \newcolumntype macro
\newcolumntype{L}{>{$}l<{$}}

\definecolor{moor}{rgb}{0.8,0.2,0.2}
\definecolor{moog}{rgb}{0.2,0.8,0.2}
\definecolor{moob}{rgb}{0.2,0.2,0.8}

\newcommand{\Free}{\mathcal{F}}
\newcommand{\Forg}{\mathcal{G}}
\newcommand{\Mod}{\mathcal{M}}
\newcommand{\Hom}{\text{\textnormal{Hom}}}
\newcommand{\image}{\text{\textnormal{im}}}
\newcommand{\Ss}{\mathcal{S}}
\newcommand{\Pp}{\mathcal{P}}
\newcommand{\SGD}{\text{\textnormal{SGD}}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\expc}{\mathbb{E}}
\newcommand{\expct}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\wrap}[1]{\left( #1 \right)}
\newcommand{\wang}[1]{\left\langle #1 \right\rangle}
\newcommand{\wive}[1]{\left\llbracket #1 \right\rrbracket}
\newcommand{\worm}[1]{\left\| #1 \right\|}

\newcommand{\plotplace}[3]{
    \begin{overpic}[width=#2, height=#3]{../plots/blank.png}
        \put( 5, 85){
            \begin{tabular}{p{#2-1.0cm}}
                #1
            \end{tabular}
        }
    \end{overpic}
}
\newcommand{\plotmoo}[3]{
    \includegraphics[width=#2           ]{../#1.png}
}

\newcommand{\bdia}[1]{\begin{gathered}\includegraphics[scale=0.22]{../diagrams/#1.png}\end{gathered}}
\newcommand{\dia} [1]{\begin{gathered}\includegraphics[scale=0.18]{../diagrams/#1.png}\end{gathered}}
\newcommand{\mdia}[1]{\begin{gathered}\includegraphics[scale=0.14]{../diagrams/#1.png}\end{gathered}}
\newcommand{\sdia}[1]{\begin{gathered}\includegraphics[scale=0.10]{../diagrams/#1.png}\end{gathered}}

\newcommand{\half}{\frac{1}{2}}
\newcommand{\sixth}{\frac{1}{6}}

\newcommand{\ofsix}[1]{
    {\tiny $\substack{
        \ifthenelse{\equal{#1}{0}}{\blacksquare}{\square}
        \ifthenelse{\equal{#1}{1}}{\blacksquare}{\square} \\
        \ifthenelse{\equal{#1}{2}}{\blacksquare}{\square} 
        \ifthenelse{\equal{#1}{3}}{\blacksquare}{\square} \\
        \ifthenelse{\equal{#1}{4}}{\blacksquare}{\square}
        \ifthenelse{\equal{#1}{5}}{\blacksquare}{\square}
    }$}
}


\newcommand{\lorem}[1]{
    Lorem ipsum dolor sit amet, consectetur adipiscing elit...\\
    \nopagebreak\vspace{#1cm} \ \\
    ...sunt in culpa qui officia deserunt mollit anim id est laborum.
}


\begin{document}

%==============================================================================
%    TITLE AND AUTHOR
%==============================================================================

\icmltitlerunning{Descent as Scattering}

\twocolumn[
    \icmltitle{A Space-Time Approach to Analyzing Stochastic Gradient Descent}
    
    \begin{icmlauthorlist}
        \icmlauthor{Samuel C.~Tenka}{mit}
    \end{icmlauthorlist}
    \icmlaffiliation{mit}{
        Computer Science and Artificial Intelligence Lab,
        Massachusetts Institute of Technology,
        Cambridge, Massachusetts, USA
    }
    \icmlcorrespondingauthor{Samuel C.~Tenka}{coli@mit.edu}
    
    \icmlkeywords{Machine Learning, SGD, ICML}
    
    \vskip 0.3in
]
\printAffiliationsAndNotice{}

%==============================================================================
%    ABSTRACT        
%==============================================================================

\begin{abstract}
    We present a diagrammatic calculus for reasoning about the behavior, at
    small learning rates, of SGD and its variants.  We interpret the diagrams
    as histories of scattering events, thus offering a new physical analogy for
    descent.  Illustrating this technique, we construct a regularizing term
    that causes large-batch GD to emulate small-batch SGD, present a
    model-selection heuristic that depends only on statistics measured before
    optimization, and exhibit a counter-intuitive loss landscape wherein SGD
    eternally cycles counterclockwise around a circle of minima. 
\end{abstract}

%==============================================================================
%    INTRODUCTION    
%==============================================================================

IDEA: ASCENT?

Fashion Mnist and CIFAR 10

Correct Thm 1 to address nonconstant batch size 

\section{Introduction}
    Stochastic gradient descent (SGD) decreases an unknown objective $l$ by
    performing discrete-time steepest descent on noisy estimates of $l$.  A key
    question is how the noise affects the final objective value.  We connect
    SGD dynamics to physical scattering theory, thus providing a quantitative
    and qualitative toolkit for answering this question.

    Specifically, we derive a diagram-based formalism for reasoning about SGD
    via a path integral over possible interactions between weights and data.
    The formalism permits perturbative analysis, leading to predictions of
    learning curves for small $\eta$.  Unlike the continuous-time limits of
    previous work, this framework models discrete time, and with it, the
    potential {\bf non-Gaussianity} of noise.  We thus obtain new results
    quantifying the {\bf effect of epoch number, batch size, and momentum} on
    SGD test loss.  We also contrast SGD against popular continuous-time
    approximations such as ordinary or stochastic differential equations (ODE,
    SDE).
    
    Path integrals offer not only quantitative predictions but also an exciting
    new viewpoint --- that of iterative optimization as a {\bf scattering
    process}.  Much as individual Feynman diagrams (see \citet{dy49a}) depict
    how local particle interactions compose into global outcomes, our diagrams
    depict how individual SGD updates influence each other before affecting a
    final test loss.  In fact, we import from physics tools such as {\bf
    crossing symmetries} (see \citet{dy49b}) and {\bf re-normalization} (see
    \citet{ge54}) to simplify our calculations and refine our estimates.  The
    diagrams' combinatorial properties yield precise qualitative conclusions as
    well, for instance that to order $\eta^2$, {\bf inter-epoch} shuffling does
    not affect expected test loss.


%==============================================================================
%    BACKGROUND AND NOTATION
%==============================================================================

\section{Background and Notation}

\subsection{A Smooth Stage: Tensor Conventions}
    We adopt summation notation for Greek, suppressing
    indices when clear.  To expedite dimensional analysis, we 
    regard the learning rate as an inverse metric $\eta^{\mu\nu}$ that
    converts a gradient into a displacement (\cite{bo13}).  We use $\eta$
    to raise indices; for example, with $C$ denoting the covariance of
    gradients, its ``trace'' will be $C^{\mu}_{\mu} = \eta^{\mu\nu}
    C_{\mu\nu}$.  Standard syntactic constraints make manifest which
    expressions transform naturally with respect to optimization dynamics.
    
    We assume that all polynomials of the $0$th and higher derivatives of the
    losses $l_n$, considered as random functions on weight space, have
    infinitely differentiable expectations.
    
    Kol\'{a}\u{r} gives a careful introduction to these differential geometric
    ideas \yrcite{ko93}.

\subsection{Combinatorial Costumes: Structure Sets}
    We make use of \emph{structure sets}, i.e. sets $S$ equipped with a
    preorder $\leq$ and an equivalence relation $\sim$.  The morphisms of
    structure sets are strictly increasing maps that preserve $\sim$ and its
    negation.  A structure set is \emph{pointed} if it has a unique maximum
    element and this element forms a singleton $\sim$-class.  The categories
    $\Ss$ of structure sets and $\Pp$ of pointed structure sets enjoy a
    free-forgetful adjunction $\Free, \Forg$.

    A \emph{diagram} is a rooted tree equipped with an equivalence relation
    $\sim$ on nodes.  We draw the tree with thin edges, with the root at the
    far right, and we indicate $\sim$ with fuzzy ties.  By reading the tree as
    a Hasse graph, we see that each diagram $D$ induces a structure set, by
    abuse of notation also named $D$.  An $\Ss$-map from $D$ to
    $[P]=(\Forg\circ\Free)^P(\text{empty set})$ is an \emph{ordering} of $D$,
    where $P$ counts $D$'s equivalence classes.  Let $o(D)$ count orderings of
    $D$.

    Fong gives a swift introduction to these category theoretic and
    diagrammatic ideas \yrcite{fo19}.
        
\subsection{The Parameterized \emph{Personae}: Forms of SGD}
    SGD decreases an objective $l$ by updating on smooth, unbiased i.i.d.
    estimates $(l_n: 0\leq n<N)$ of $l$.  The pattern of updates is determined
    by a structure set $S$ whose preorder is a total preorder with element $i$ 
    inside strongly connected component $C(i)$: for a map
    $\pi:S\to [N]$ that induces $\sim$, we define SGD inductively as
    $\text{SGD}_{S}(\theta) = \theta$ when $S$ is empty and otherwise
    $$
        \SGD_S(\theta) =
            \SGD_{S\setminus M}(\theta^\mu - \eta^{\mu\nu} \nabla_\nu l_{M}(\theta))
    $$
    where $M = \min S \subseteq S$ specifies a batch and $l_M =
    \frac{1}{M} \sum_{m\in M} l_{\pi(m)}$ is a batch average.  Since the
    distribution of $l_n$ is permutation invariant, the non-canonical choice
    of $\pi$ does not affect the distribution of output $\theta$s.

    Of special interest are structure sets that divide into $M\times B$ many
    \emph{epochs} each with $N/B$ many disjoint \emph{batches} of size $B$.  An
    SGD instance is then determined by $N, B, M$, and an \emph{inter-epoch
    shuffling scheme}.  The cases $B=1$ and $B=N$ we call \emph{pure SGD} and
    \emph{pure GD}.

%\subsection{The Tempting Tool: Taylor Series}
%    Intuitively, each descent step displaces $\theta$ by $-\eta \nabla l$ and
%    hence decreases the loss $l(\theta)$ by $\eta (\nabla l)^2$; thus, we
%    expect after $T$ steps a net decrease of $T \eta (\nabla l)^2$:
%    \begin{equation} \label{eq:motone}
%        l(\theta_T) \approx l(\theta_0) - T \cdot \eta \cdot (\nabla l(\theta_0))^2
%    \end{equation}
%    This intuition fails to capture two crucial facts: {\bf curvature} --- that
%    as $\theta$ changes during training, so may $\nabla l(\theta)$ --- and {\bf
%    noise} --- that $l_n$ and $l$ may differ.
%
%    To account for noise, we should replace each $(\nabla l_t)(\nabla l)$ by
%    an expectation.  If we are interested in train instead of test loss, 
%    We get some expectations of the form $(\nabla l_t)(\nabla l_t)$, and hence
%    obtain a different result than for test loss.
%
%    To account for curvature, {\color{red} FILL IN}
       
%==============================================================================
%    DIAGRAM CALCULUS FOR SGD
%==============================================================================

\section{Diagram Calculus for SGD}
\subsection{Role of Diagrams}
    Suppose $s$ is smooth on weight space; e.g. $s$ may be a test
    loss.  We track $s(\theta)$ as $\theta$ evolves by SGD:
    \begin{klem}
        The Maclaurin series of $s(\theta_T)$ with respect to $\eta$ is:
        \begin{equation}\label{eq:dyson}
            \sum_{(d_t: 0\leq t<T)}
            (-\eta)^{\sum_t d_t}
            \left(
                \prod_{0 \leq t < T}
                    \left.  \frac{(g \nabla)^{d_t}}{d_t!} \right|_{g=\nabla l_t(\theta)}
            \right)
            (s) (\theta_0)
        \end{equation}
    \end{klem}
    In averaging over training sets $(l_t: 0\leq t<T)$ we may factor the
    expectation of the above product according to independence relations
    between the $l_t$.  We view various training procedures (e.g. pure GD, pure
    SGD) as {\bf prescribing different independence relations} that lead to
    different factorizations and hence to potentially different generalization
    behavior at each order of $\eta$.

    An instance of the above product (for $s=l_a$ drawn from a test set and
    $0\leq c\leq b<T$) is
    $-\eta^3 (\nabla l_c \nabla)^2 (\nabla l_b \nabla) l_a$, which is
    {\small
    \begin{align*}
        - (\nabla^\lambda l_c) (\nabla^\mu l_c) (\nabla_\lambda \nabla_\mu \nabla^\nu l_b) (\nabla_\nu l_a)   
        - (\nabla^\lambda l_c) (\nabla^\mu l_c) (\nabla_\lambda \nabla^\nu l_b) (\nabla_\mu \nabla_\nu l_a) \\
        - (\nabla^\lambda l_c) (\nabla^\mu l_c) (\nabla_\mu \nabla^\nu l_b) (\nabla_\lambda \nabla_\nu l_a)   
        - (\nabla^\lambda l_c) (\nabla^\mu l_c) (\nabla^\nu l_b) (\nabla_\lambda \nabla_\mu \nabla_\nu l_a)
    \end{align*}
    }
    To reduce clutter, we adapt the string notation of \citet{pe71}.  Then, in
    expectation over $(l_c, l_b, l_a)$ drawn i.i.d.:
    \begin{align}
        \cdots
        &= 
             \sdia{(01-2-3)(02-12-23)}
            +\sdia{(01-2-3)(02-13-23)}
            +\sdia{(01-2-3)(03-12-23)}
            +\sdia{(01-2-3)(03-13-23)} \\
            \label{eq:simpl}
        &=
            \underbrace{2\sdia{(01-2-3)(02-12-23)}}_{
               -2~\expct{{\color{moor}(\nabla l)(\nabla l)}}~\expct{{\color{moog}\nabla\nabla\nabla l}}~\expct{{\color{moob} \nabla l}}
            }
            +
            \underbrace{2\sdia{(01-2-3)(02-13-23)}}_{
               -2~\expct{{\color{moor}(\nabla l)(\nabla l)}}~\expct{{\color{moog}\nabla \nabla l}}~\expct{{\color{moob}\nabla \nabla l}}
            }
    \end{align}
    Above, each node corresponds to an $l_n$ (here, red for $l_c$, green
    for $l_b$, blue for $l_a$), differentiated $g$ times for a degree-$g$ node
    (for instance, $l_b$ is differentiated thrice in the first diagram and
    twice in the second).  Thin \emph{edges} mark contractions by $-\eta$.
    Fuzzy \emph{ties} denote correlations by connecting identical loss
    functions (here, $l_c$ with $l_c$).  The colors are redundant with the
    fuzzy ties.  The value $v(D)$ of a diagram $D$ is the expected value of the
    corresponding tensor expression.
    
    Crucially, for a fixed, i.i.d.  distribution over $(l_c, l_b, l_a)$, {\bf
    the topology of a diagram determines its value}.  For instance,
    $\sdia{(01-2-3)(02-12-23)} = \sdia{(01-2-3)(03-13-23)}$.  Thus follows the
    simplification of equation \ref{eq:simpl}.  We may convert back to explicit
    tensor expressions, invoking independence between untied nodes to factor
    the expression.  However, as we will see, the diagrams offer physical
    intuition, streamline computations, and determine useful unbiased
    estimators of the statistics they represent.  

    We define a diagram with fuzzy outlines to be the difference between the
    fuzzy tied and untied versions : $\sdia{c(01-2)(01-12)} =
    \sdia{(01-2)(01-12)}-\sdia{(0-1-2)(01-12)}$. 
    
    The recipes for writing down test (or train) losses of SGD and its variants
    are straight-forward in the diagram notation because they reduce the
    problem of evaluating the previous dynamical expressions to the problem of
    counting isomorphic graphs.  The more complicated the direct computation,
    the greater the savings of using diagrams.  An appendix provides details
    and proofs for a variety of situations.  For now, we focus on the test loss
    of SGD.

\subsection{Recipe for the Test Loss of SGD}
    Our results all follow from this theorem and its analogues.
    Throughout, the $d$-edged diagrams give the order $\eta^d$ terms. 
    \begin{thm} \label{thm:test}
        SGD's expected test loss has a Maclaurin series
        given as a weighted sum of diagrams:
        \begin{equation}\label{eq:sgdcoef}
            \sum_{D \in \image(\Free)}
            \left(
                \sum_{f: D\to\Free(S)}
                \prod_{i\in S} \frac{|C(i)|^{|f^{-1}(i)|}}{|f^{-1}(i)|!}
            \right)
            v(D) 
        \end{equation}
        Here, $D$ is (an isomorphism class of) a diagram of form
        $\Free(T)$ and $f$ is a morphism in $\Pp$.
    \end{thm}
    \begin{thm}
        SGD's expected generalization gap (test loss minus train loss) is
        formula \ref{eq:sgdcoef} with $v(D)$ replaced by
        %\begin{equation}
        $
            \sum_{p \in D/\sim_D} v(D_p)/N
        $.
        %\end{equation}
        Here, $p$ ranges through equivalance classes of $D$, and $D_p$ is $D$
        with a fuzzy outline connecting $D$'s maximal node to $p$,
        e.g. $(\sdia{(0-1)(01)})_{p=\sdia{(0)()}} = \sdia{c(01)(01)}$.
    \end{thm}

    In the special case of $B=1, M=1$:
    \begin{prop}
        The order $\eta^d$ contribution to the expected test loss of one-epoch
        SGD with singleton batches is:
        \begin{equation}\label{eq:sgdbasiccoef}
            \frac{(-1)^d}{d!} \sum_{D\in \image(\Free)} 
            o(D) {N \choose P-1} {d \choose d_0,\cdots,d_{P-1}}
            v(D)
        \end{equation}
        where $D$ ranges over $d$-edged diagrams whose equivalence classes
        have sizes $d_p: 0\leq p\leq P$, with $d_P=1$
        and, without loss, are each antichains.
    \end{prop}
    A $P$-part, $d$-edged diagram then contributes $\Theta\left((\eta N)^d
    N^{P-d-1}\right)$ to the loss.  For example, there are six diagrams to
    third order, and they have $(4+2)+(2+2+3)+(1)$ many orderings --- see Table
    \ref{tab:scatthree}.  Intuitively, $\eta N$ measures the {\bf physical
    time} of descent, and $1/N$ measures {\bf coarseness} of time
    discretization.  So we have a double-series in $(\eta N)^d
    N^{P-d-1}$, where $d$ counts thin edges and $d+1-P$ counts fuzzy ties; the
    $P=d+1$ terms correspond to a discretization-agnostic (hence
    continuous-time, noiseless) ODE approximation to SGD, while $P\leq d$ gives
    correction terms modeling time-discretization and hence noise.  
    \begin{cor}
        For one-epoch SGD on singleton batches through fixed physical time $T$:
        the order $N^{-1}$ deviation of SGD's test loss from ODE's is
        $
            ({{T^2 N^{-1}}/{2}}) \sdia{c(01-2)(02-12)}
        $.
        The order $N^{-2}$ deviation of SGD's test loss due to non-gaussian
        noise is
        $
            -({{T^3 N^{-2}}/{6}}) (\sdia{c(012-3)(03-13-23)} - 3 \sdia{c(01-2-3)(03-13-23)})
        $.
    \end{cor}
    For finite $N$, these effects make SDE different from SGD.  SDE also fails
    to model the correlations between updates in multiepoch SGD.  On the other
    hand, in the $N=\infty$ limit for which SDE matches SGD, optimization and
    generalization become computationally intractable and trivial,
    respectively. 

    \begin{table}[h!]
        \centering 
        \resizebox{\columnwidth}{!}{%
        \begin{tabular}{c|c|c}
            {\LARGE $\Theta\left((\eta N)^3 N^{-0}\right)$} &
            {\LARGE $\Theta\left((\eta N)^3 N^{-1}\right)$} &
            {\LARGE $\Theta\left((\eta N)^3 N^{-2}\right)$} \\ \hline
            \begin{tabular}{c}
                \begin{tabular}{LL}
                    \bdia{(0-1-2-3)(01-12-23)} & \bdia{(0-1-2-3)(01-13-23)}
                \end{tabular} \\
                \begin{tabular}{LL}
                    \bdia{(0-1-2-3)(02-13-23)} & \bdia{(0-1-2-3)(03-12-23)}
                \end{tabular} \\ \hline
                \begin{tabular}{LL}
                    \bdia{(0-1-2-3)(03-13-23)} & \bdia{(0-1-2-3)(02-12-23)}
                \end{tabular}
            \end{tabular}
            &
            \begin{tabular}{c}
                \begin{tabular}{LL}
                    \bdia{(01-2-3)(02-13-23)} & \bdia{(01-2-3)(03-12-23)}
                \end{tabular} \\ \hline
                \begin{tabular}{LL}
                    \bdia{(0-12-3)(01-13-23)} & \bdia{(0-12-3)(02-13-23)}
                \end{tabular} \\ \hline
                \begin{tabular}{LLL}
                    \bdia{(01-2-3)(03-13-23)} & \bdia{(0-12-3)(03-13-23)} & \bdia{(01-2-3)(02-12-23)} 
                \end{tabular}
            \end{tabular}
            &
            \begin{tabular}{c}
                \begin{tabular}{L}
                    \bdia{(012-3)(03-13-23)}
                \end{tabular}
            \end{tabular}
        \end{tabular}
        }
        \caption{
            Degree-$3$ scattering diagrams for $B=M=1$ SGD's test loss.
            {\bf Left:} $(d, P) = (3, 3)$.  Diagrams for ODE behavior.
            {\bf Center:} $(d, P) = (3, 2)$.  $1$st order deviation of SGD away from ODE.
            {\bf Right:} $(d, P) = (3, 1)$.  $2$nd order deviation of SGD from ODE with appearance of
            non-Gaussian statistics.
        }
        \label{tab:scatthree}
    \end{table}

    A quick combinatorial argument shows:
    \begin{cor}
        To order $\eta^2$, inter-epoch shuffling doesn't affect SGD's
        expected test loss.
    \end{cor}
    Indeed, for any inter-epoch shuffling scheme: 
    \begin{prop}\label{prop:ordtwo}
        To order $\eta^2$, the test loss of SGD --- on $N$
        samples for $M$ epochs with batch size $B$ dividing $N$ and with any
        shuffling scheme --- has expectation
        {\small
        \begin{align*}
                                                    \mdia{(0)()}
            &+ MN                                   \mdia{(0-1)(01)}
             + MN\wrap{MN - \frac{1}{2}}            \mdia{(0-1-2)(01-12)} \\
            &+ MN\wrap{\frac{M}{2}}                 \mdia{c(01-2)(02-12)}  
             + MN\wrap{\frac{M-\frac{1}{B}}{2}}     \mdia{c(01-2)(01-12)}
        \end{align*}
        }
    \end{prop}

    \begin{cor}
        To order $\eta^2$, one-epoch SGD has 
        $
             \wrap{\frac{M-1}{M}}\wrap{\frac{B+1}{B}}\wrap{\frac{N}{2}} \mdia{c(01-2)(01-12)}
        $
        less test loss than $M$-epoch SGD with learning rate $\eta/M$. 
    \end{cor}

    Given an unbiased estimator $\hat{C}$ of gradient covariance, we may get
    GD to mimic SGD:
    \begin{cor}
        The expected test loss of pure SGD is, to order $\eta^2$,
        less than that of pure GD by
        $
              \wrap{\frac{M}{2}} \wrap{\frac{N-1}{N}} \sdia{c(01-2)(01-12)}
        $.
        Moreover, GD on a modified loss 
        $
            \tilde l_n = l_n + \wrap{\frac{N-1}{4N^2}} \hat{C}_\nu^\nu(\theta)
        $
        has an expected test loss that agrees with SGD's to second order.
    \end{cor}

\subsection{Renormalization}
    An important idea is that of renormalization, i.e. the summarization of  
    myriad small-scale interactions into an effective large-scale theory. 
    We can use this two ways:
        ({\bf A}) to refine our computations if we know the hessian;  
        ({\bf B}) to refine our computations if we know the ``effective propagator''.
    \lorem{3}
    \lorem{3}

\subsection{Descent as Scattering}
    In sum, SGD's test loss is a weighted sum of diagrams, each $d$-edged
    diagram contributing to order $\eta^d$.  We depict the $\Ss$-maps of
    $f:D\to S$ Theorem \ref{thm:test} as an embedding of the graph $D$ into
    the structure set or ``spacetime'' $S$.  Thus, SGD's test loss is a sum
    over all embeddings in spacetime; see Figure (\ref{fig:spacetime}).  This
    loss depends on the shape of spacetime, which in encodes correlations
    between updates.  To compute a test loss, we simply count embeddings.
    For instance, the order $\eta^2$ diagrams in (\ref{fig:spacetime}) all 
    contribute the same amount to test loss, so we just need to find out how
    many such embedded diagrams there are.   
    \begin{figure}[h!] \label{fig:spacetime}
        \centering  
        \plotmoo{diagrams/spacetime}{\columnwidth}{3.0cm}  
        \caption{
            Some diagrams embedded in spacetime.  The left four diagrams
            give order $\eta^1$, $\eta^1$, $\eta^3$, and $\eta^5$
            contributions to pure GD's test loss.  The right four each
            contribute $\eta^2 \expct{\nabla^2} \expct{\nabla}^2$; their
            equivalence demonstrates crossing symmetry.  Only diagrams
            whose nodes fall within shaded diagonals contribute to pure SGD's
            test loss (with an extra factor $N$ per edge). 
        }
    \end{figure}
    \begin{figure}[h!] \label{fig:vsmulti}
        \centering  
        \plotmoo{diagrams/spacetime-b}{\columnwidth}{3.0cm}
        \caption{
            Comparison of pure GD's vs pure SGD's test loss.  We may
            normalize almost every order $\eta^2$ GD diagram to an
            equivalent SGD diagram by horizontal or vertical shifts (see
            left five diagrams).  By contrast, $M{N\choose 2}$ many
            $\sdia{(01-2)(01-12)}$s turn into $\sdia{(0-1-2)(01-12)}$s (see
            rightmost diagram).  So pure GD's test loss exceeds pure SGD's
            test loss by $M ((N-1)/2N) \sdia{c(01-2)(01-12)}$.
        }
    \end{figure}
    Likewise, as shown in Figure
    (\ref{fig:vsmulti}), the order $\eta^2$ diagrams of pure GD
    and pure SGD are nearly in correspondence, except for a discrepancy that
    shows the two test losses differ by $M ((N-1)/2N) \sdia{c(01-2)(01-12)}$.
    %This argument generalizes to yield proposition \ref{prop:ordtwo}.


%==============================================================================
%    PREDICTIONS AND APPLICATIONS
%==============================================================================

\pagebreak
\section{Consequences and Applications}

\subsection{Vanilla SGD}
    \lorem{3}
    \lorem{3}
    \begin{figure}[h!]
        \centering
        \plotplace{small T: eta curve}{4.0cm}{4.0cm} \plotplace{test loss decrease near minimum}{4.0cm}{4.0cm} \\
        \plotplace{small T gen gap: actual vs predicted}{4.0cm}{4.0cm} \plotplace{nongaussian example}{4.0cm}{4.0cm}
        \caption{\lorem{2}}
    \end{figure}

\subsection{Emulating Small Batches with Large Ones}
    \lorem{3}
    \lorem{3}
    \lorem{3}
    \begin{figure}[h!]
        \centering
        \plotplace{batch matching over one init}{4.0cm}{4.0cm}
        \plotplace{batch matching over multiple inits}{4.0cm}{4.0cm} \\
        \plotplace{scan over betas}{4.0cm}{4.0cm}
        \plotplace{long time comparison}{4.0cm}{4.0cm}
        \caption{\lorem{2}}
    \end{figure}

\subsection{Analyzing Second Order Methods}
    We demonstrate how our approach extends to more sophisticated optimizers by
    analyzing momentum and a hessian-based method. 

    \lorem{3}
    momentum

    Now consider a hessian-based update parameterized by a scalar $\lambda$: 
    $$
        \theta \leftsquigarrow
        \theta -
        (\eta^{-1} + \lambda \nabla \nabla l_t(\theta))^{-1} \nabla l_t(\theta)
    $$

    invhess
    \begin{figure}[h!]
        \centering
        \plotplace{momentum}{4.0cm}{4.0cm} 
        \plotplace{invhess}{4.0cm}{4.0cm}
        \caption{\lorem{2}}
    \end{figure}

\subsection{Epochs and Overfitting}
    \lorem{3}
    \begin{figure}[h!]
        \centering
        \plotplace{multiepoch vs sgd limit}{4.0cm}{4.0cm}
        \plotplace{multiepoch vs gd limit}{4.0cm}{4.0cm}
        \caption{\lorem{2}}
    \end{figure}

\subsection{Myopic Model Selection}
    \lorem{3}
    \begin{figure}[h!]
        \centering
        \plotplace{rankings: actual vs predicted}{4.0cm}{4.0cm}
        \plotplace{architecture vs optimization ease}{4.0cm}{4.0cm}
        \caption{\lorem{2}}
    \end{figure}

\subsection{Comparison to Continuous Time}
    \lorem{3}
    \lorem{3}
    Also, sgd interepoch correlations
    \begin{figure}[h!]
        \centering
        \plotplace{distinguishing landscape}{4.0cm}{4.0cm}
        \plotplace{ode vs sde vs sgd performance on landscape}{4.0cm}{4.0cm}
        \caption{\lorem{2}}
    \end{figure}

\subsection{Thermodynamic Engine}
    We clarify  
    \lorem{3}
    \begin{figure}[h!]
        \centering
        \plotplace{loss landscape: mean and covariance}{4.0cm}{4.0cm}
        \plotplace{net theta vs time: ours, chaudhari, naive}{4.0cm}{4.0cm}
        \caption{\lorem{2}}
    \end{figure}
    We constructed a counter-intuitive loss landscape wherein, for arbitrarily
    small learning rates, SGD cycles counterclockwise around a circle of
    minima.  Our mechanism differs from that discovered by \citet{ch18}
    {\color{red}  discuss the thermodynamic significance of both}

%==============================================================================
%    RELATED WORK    
%==============================================================================

\section{Related Work}
    It was \citet{ki52} who, in uniting gradient descent \citep{ca47} with
    stochastic approximation \citep{ro51}, invented SGD.  Since the development
    of back-propagation for efficient differentiation \citep{we74}, SGD
    has been used to train connectionist models including neural
    networks \citep{bo91}, in recent years to remarkable success \citep{le15}.

    Several lines of work quantify the overfitting of SGD-trained networks
    \citep{ne17a}.  For instance, \citet{ba17} controls the Rademacher
    complexity of deep hypothesis classes, leading to generalization bounds
    that are optimizer-agnostic.  However, since networks trained via SGD
    generalize despite their seeming ability to shatter large sets
    \citep{zh17}, one infers that generalization arises from the aptness to
    data of not only architecture but also optimization \citep{ne17b}.  Others
    have focused on the implicit regularization of SGD itself, for instance by
    modeling descent via stochastic differential equations (SDEs) (e.g.
    \citet{ch18}).  However, per \citet{ya19}, such continuous-time analyses
    cannot treat covariance correctly, and so they err when interpreting
    results about SDEs as results about SGD for finite trainsets.

    Following \citet{ro18}, we avoid making a continuous-time approximation by
    instead Taylor-expanding around the learning rate $\eta=0$.  In fact, we
    develop a diagrammatic method for evaluating each Taylor term that is
    inspired by the field theory methods popularized by \citet{dy49a}.  Using
    this technique, we quantify the overfitting effects of batch size and epoch
    number, and based on this analysis, propose a regularizing term that causes
    large-batch GD to emulate small-batch SGD, thus establishing a precise
    version of the Covariance-BatchSize-Generalization relationship conjectured
    in \citet{ja18}.  
    
    While we make rigorous, architecture-agnostic predictions of learning
    curves, these predictions become vacuous for large $\eta$.  In particular,
    while our work does not assume convexity of the loss landscape, it also is
    blind to large-$\eta T$ convergence of SGD.  Other discrete-time dynamical
    analyses allow large $\eta$ by treating deep generalization
    phenomenologically, whether by fitting to an empirically-determined
    correlate of Rademacher bounds \citep{li18}, by exhibiting generalization
    of local minima {\bf flat} with respect to the standard metric (see
    \citet{ho17}, \citet{ke17}, citet{wa18}), or by exhibiting generalization
    of local minima {\bf sharp} with respect to the standard metric (see
    \citet{st56}, \citet{di17}, \citet{wu18}).  Our work, which makes explicit
    the dependence of generalization on the underlying metric and on the form
    of gradient noise, reconciles those latter, seemingly clashing claims.
    
    Others have imported the perturbative methods of physics to analyze descent
    dynamics:  \citet{dy19} perturb in inverse network width, employing 't
    Hooft diagrams to compute deviations of non-infinitely-wide deep learning
    from Gaussian processes.  Meanwhile, \cite{ch18} and \citet{li17} perturb
    in learning rate to second order by approximating noise between updates as
    gaussian and uncorrelated.  This approach does not generalize to higher
    orders, and, because correlations and heavy tails are essential obstacles
    to concentration of measure and hence of generalization, it does not model
    the generalization behavior of SGD.  By contrast, we use Penrose diagrams
    to compute test and train losses to arbitrary order in learning rate,
    quantifying the effect of non-gaussian and correlated noise.  We hence
    extend \citet{ro18} beyond leading order and beyond $2$ time steps,
    allowing us to compare, for instance, the expected test losses of
    multi-epoch and single-epoch SGD.



%==============================================================================
%    CONCLUSION      
%==============================================================================

\section{Conclusion}
    We presented a novel diagrammatic tool for analyzing gradient
    descent.  We introduced a novel regularizing term, thus showing that {\bf
    large-batch GD can be made to emulate small-batch SGD} and completing a
    project suggested by \citet{ro18}.  This is significant because, while
    small batch sizes can lead to better generalization \citep{bo91}, modern
    infrastructure increasingly rewards large batch sizes \citep{go18}.      
    We showed also that in multi-epoch SGD, inter-epoch shuffling induces only
    a $3$rd order effect on test loss.  Intuitively, we proved that {\bf the
    hessian matters asymptotically more than shuffling order}.

    The diagram method is also a rich source of intuitions and physical
    analogies.  For example, it offers a clearer understanding of the
    empirically verified limit cycles found in Chaudhari.  As our physical
    analogy emphasizes the underlying metric, it reconciles competing views
    of whether sharp or flat minima generalize.  Further exploration of this
    bridge to particle physics, especially within the framework of
    renormalization theory, pose a promising direction for future research.

    VARIANCES

%==============================================================================
%    ACKNOWLEDGEMENTS
%==============================================================================

\subsection{Acknowledgements}
    We thank Dan A. Roberts and Sho Yaida for patient introductions to their
    work and for precisely posing several of the questions we answer here.  We
    feel deeply grateful to Sho Yaida and Josh B. Tenenbaum for their
    compassionate guidance.  We appreciate the generosity of
        Andrzej Banburski
        and
        Wenli Zhao
    in offering crucial feedback on writing.

%==============================================================================
%    REFERENCES      
%==============================================================================

%\section*{References}
    \bibliography{perturb}
    \bibliographystyle{icml2019}

    \lorem{3}
    \lorem{3}

%==============================================================================
%    APPENDICES      
%==============================================================================

\section*{A. Derivation of Diagram Rules}

\subsection{Dyson Series for Iterative Optimizers}
    If a density $\rho$ governs a point $\theta$ in weight space, then after a
    sequence of updates $\theta \mapsto \theta - \eta^{\mu\nu} \nabla_\mu
    l(\theta)$ on losses $(l_t: 0\leq t < T)$, the following density (up to an
    error term whose Maclaurin series vanishes; all perturbative results will
    implicitly carry such terms) will govern the new point:
    \begin{equation}\label{eq:descexp}
        \exp\left(+\eta^{\mu\nu} \nabla_\mu l_{T-1}(\theta) \nabla_\nu\right) \cdots \exp\left(+\eta^{\mu\nu} \nabla_\mu l_0(\theta) \nabla_\nu\right) \rho
    \end{equation}
    or
    $
        \prod \exp\left(+\eta \nabla l \nabla\right) \rho
    $
    for short.
    The exponent above is a linear operator that acts on a space of
    sufficiently smooth maps; in particular, the $\nabla_\nu$ does not act on
    the $\nabla_\mu l(\theta)$ with which it pairs.  Integrating by parts, we
    write the expectation over initial values after $T$ steps of a function $s$
    of weight space (e.g. $s$ may be test loss) as:
    \begin{align}\label{eq:contraexp}
        %&\int_\theta \left(\prod_{T > t \geq 0} \exp\left(+\eta^{\mu\nu} \nabla_\mu l(\theta) \nabla_\nu\right) \rho\right)(\theta) s(\theta)
        %= \\
        &\int_\theta \rho(\theta) \left(\prod_{0 \leq t \leq T} \exp\left(-\eta^{\mu\nu} \nabla_\mu l(\theta) \nabla_\nu\right) s\right)(\theta)
    \end{align}
    Since the exponentials above might not commute, we may not compose
    the product of exponentials into an exponential of a sum.  We instead
    compute an expansion in powers of $\eta$.  Setting the initialization
    $\rho(\theta) = \delta(\theta-\theta_0)$ to be deterministic, and labeling
    as $\theta_t$ the weight after $t$ steps, we find:
    \begin{equation}\label{eq:dyson}
        s(\theta_T) =
        %\left(\prod_{T > t \geq 0} \exp\left(-\eta^{\mu\nu} \nabla_\mu l_t(\theta) \nabla_\nu\right) s\right) (\theta_0)
        %= 
        \sum_{0\leq d < \infty} (-\eta)^d \sum_{\substack{(d_t: 0\leq t<T) \\ \sum_t d_t = d}}
        \left(\prod_{0 \leq t < T} \frac{(\nabla l_t(\theta) \nabla)^{d_t}}{d_t!}\right) s (\theta_0)
    \end{equation}

    \lorem{3}
    \lorem{3}
    \lorem{3}
    \lorem{3}

\section*{B. Tutorial on Diagram Rules}
    \lorem{3}
    \lorem{3}
    \lorem{3}

\section*{C. Derivations of Perturbative Results}

    \lorem{3}
    \lorem{3}
    \lorem{3}
    \lorem{3}
    \lorem{3}

\section*{D. Diagram Rules vs Direct Perturbation}
    Diagram methods from Stueckelberg to Peierls have flourished in physics
    because they enable swift computations and immediate intuition that would
    otherwise require laborious algebraic manipulation.  We exhibit how our
    diagram formalism likewise streamlines analysis of descent by comparing
    direct perturbation and the new formalism on some sample problems.

    \subsection*{One-Epoch, Singleton-Batch SGD Test Loss to 3rd Order}
        For single-epoch SGD with singleton batches, we sum all relevant diagrams
        through order $3$; the coefficients $4, 2; 2, 2, 3; 1$ come from counting
        the elements of Table \ref{tab:scatthree}, and the other coefficients come
        from analogous tables.  This yields:
        \begin{align*}
                &\mathcal{L}^{\text{SGD}}_\text{test}(T, \eta) \in                   
            \\ 
                   \sdia{(0)()}
                &+ \frac{\eta}{1!}   {T \choose 1} \wrap{\sdia{(0-1)(01)}}
            \\
                &+ \frac{\eta^2}{1!1!} {T \choose 2} \wrap{2 \sdia{(0-1-2)(01-12)}} 
                 + \frac{\eta^2}{2!} {T \choose 1} \wrap{\sdia{(01-2)(02-12)}}
            \\
                &+ \frac{\eta^3}{1!1!1!} {T \choose 3} \wrap{
                           4 \sdia{(0-1-2-3)(01-12-23)}+
                           2 \sdia{(0-1-2-3)(03-13-23)}
                       }
            \\
                &+ \frac{\eta^3}{2!1!} {T \choose 2} \wrap{
                           2         \sdia{(01-2-3)(03-12-23)}+
                           2    \sdia{(0-12-3)(02-13-23)}+
                           3     \sdia{(01-2-3)(02-12-23)}
                       }
            \\
                &+ \frac{\eta^3}{3!} {T \choose 1} \wrap{\sdia{(012-3)(03-13-23)}}
                + o(\eta^3)\footnotemark
        \end{align*}
        \footnotetext{
            We use little-$o(\eta^d)$ instead of big-$O(\eta^{d+1})$ to avoid
            specializing to analytic loss landscapes.  Error terms depend on
            the loss landscape and on $T$.  When gradients are uniformly
            bounded, the $T$ dependence is at most linear.
        }

    \subsection*{Test Loss of One-Epoch SGD vs Multi-Epoch SGD}

    \subsection*{Generalization Gap of a Hessian-Based Method}

    \subsection*{Test Loss of SGD vs GD}
        The generalization gap $\mathcal{L}^{\text{SGD}}_\text{gen} =
        \mathcal{L}^{\text{SGD}}_\text{test} -
        \mathcal{L}^{\text{SGD}}_\text{train}$ is suppressed by a factor $1/N$ ($N
        \leq T$):
        \begin{align*}
            &N \cdot \mathcal{L}^{\text{SGD}}_\text{gen}(T, \eta) \in
            \\
            &+ \eta   {T \choose 1} \sdia{c(01)(01)} 
            + \eta^2 {T \choose 2} \wrap{\sdia{c(01-2)(01-12)} + \sdia{c(02-1)(01-12)}} \\
            &+ \frac{\eta^2}{2!} {T \choose 1} \wrap{\sdia{(012)(02-12)} - \sdia{(01-2)(02-12)}} 
             + o(\eta^2) 
            \\
            =&+ \eta^{\lambda \mu} {T \choose 1} C_{\lambda \mu} 
            + \eta^{\lambda \mu} \eta^{\nu \xi} {T \choose 2} \wrap{ \frac{1}{2} G_\lambda \nabla_\mu C_{\nu \xi} + C_{\mu \nu} H_{\xi \lambda} }
            \\
            &+ \frac{\eta^{\lambda \mu} \eta^{\nu \xi}}{2!} {T \choose 1} \wrap{ \wang{l_\lambda l_\nu l_{\mu \xi}} - (G_{\mu}G_{\nu} + C_{\mu\nu})H_{\xi \lambda} }
             + o(\eta^2)
        \end{align*}
        The leading order term is $N \cdot \mathcal{L}^{\text{SGD}}_\text{gen}(T, \eta) \approx \eta T \wrap{\sdia{(01)(01)} - \sdia{(0-1)(01)}} = T \cdot \eta^{\lambda\mu} C_{\lambda\mu}$,
        where $C$ is the covariance of gradients.  We thus recover a main result of \citet{ro18}.



\section*{E. The $\eta$-Series' Domain of Convergence}
    \lorem{3}
    \lorem{3}
    \lorem{3}

\section*{F. Autonomous ODE Fitting}
    We fit a .
    These have the benefit 
    In particular:
    \begin{align}
        y\prime(t) =               a     &\to&   y(t) =  y(0) + a t                      \\
        y\prime(t) =         b y + a     &\to&   y(t) = (y(0) - (a/b)) \exp(b t) + (a/b) \\
        y\prime(t) = c y^2 + b y + a     &\to&                                             
    \end{align}

    \lorem{3}

\section*{G. Generalized Bessel Factors}
    \lorem{3}
    \lorem{3}
    \lorem{3}
\end{document}

