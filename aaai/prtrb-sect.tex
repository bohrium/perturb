

After explaining the diagram-based Taylor series approach to SGD dynamics, we
state our main result: that diagram-based analysis is correct.

        \subsection{Notation and assumptions, II}\label{sect:background}

            %--  names of sgd parameters  -------------------------------------

\textsc{Regularity Conditions} ---
We assume the following throughout.
%
\textbf{Derivative bounds}:
there are compact sets $(K_k: k\geq 0)$ so that
$\nabla^n l_x(\theta)\in K_k$ for all $\theta,l_x$. 
Here $\nabla^k
l_x(\theta)$ is a $k$th derivative, a $k$-axis tensor.
%
\textbf{Analytic moments}:
any polynomial $p$ of $l_x$ and its higher derivatives
induces a random variable so that
$\expct{p}:\Mm\to\RR$ (exists and) is analytic in $\theta$;
moreover, $\expct{p}$'s radii of convergence are strictly bounded from $0$,
even as $\theta$ varies.
%
Consequently, $\nabla \expct{p}=\expct{\nabla p}$.


%            \subsubsection{Loss Landscape, SGD}
            \textsc{Batches and Epochs} --- Generalizing the previous section,
            our theory describes SGD with
            any number
                {$\mathbf{N}$ of training points},
                {$\mathbf{T}$ of updates}, and 
                {$\mathbf{B}$ of points per batch}.
            SGD runs $T$ updates (hence
                {$\mathbf{E}=TB/N$ epochs} or
                {$\mathbf{M}=T/N$ updates per training point}) of the form
            %    \translucent{moolime}{$\mathbf{N}$ of training points},
            %    \translucent{moolime}{$\mathbf{T}$ of updates}, and 
            %    \translucent{moolime}{$\mathbf{B}$ of points per batch}.
            %Specifically, SGD runs $T$ many updates (hence
            %    \translucent{moolime}{$\mathbf{E}=TB/N$ epochs} or
            %    \translucent{moolime}{$\mathbf{M}=T/N$ updates per training point}) of the form
            $$
                \textstyle
                \theta_{t+1}^\mu
                \coloneqq
                \theta_t^\mu -
                \sum_\nu
                \eta^{\mu\nu} \nabla_\nu
                    \sum_{n\in \Bb_t} l_n(\theta) / B
            $$
            where in each epoch %$(k(N/B)\leq t<(k+1)(N/B))$,
            we sample the
            $t$th batch $\Bb_t$ without replacement from the training sequence.
            %
            %We especially study
            %the \textbf{final testing loss} $\expc[l(\theta_T)]$.
            %and the \textbf{final displacement} $\expc[\theta_T-\theta_0]$.

            %--  tensor conventions  ------------------------------------------

            \textsc{Comparing Tensors} --- A (potentially tensor)
            quantity $q$ \emph{vanishes to order $\eta^d$} when
            for some homogeneous degree-$d$ polynomial $p$
            $\lim_{\eta\to 0} q/p(\eta) = 0$; we then say $q\in o(\eta^d)$.
            %
            We write $A \preceq B$ ($\prec$) for symmetric bilinear forms $A,
            B$ when $A(v,v) \leq B(v,v)$ ($<$) for all $v\neq 0$. 

\textsc{Continuous Time} ---
            %\subsubsection{Continuous Time}
                Ordinary and stochastic differential equations (ODE, SDE) are
                popular models of SGD \citep{li18, ba21}.  They correspond to
                continuous-time limits of large-training-set SGD with
                independent noise ($E=B=1$, $N=T=kT_0$, $\eta=\eta_0/k$, $k\to
                \infty$) ODE descends on a noiseless version of the landscape
                $l_x(\theta)$, while SDE descends on a version whose gradient
                noise is independent gaussian of shape $C(\theta) =
                \expc[\nabla l_x \nabla l_x] - \expc[\nabla l_x] \expc[\nabla
                l_x]$, scaled as in a Wiener process:
                $$
                    l^{\text{ODE}}_x(\theta) - l(\theta) = 0
                    \hspace{1cm}
                    l^{\text{SDE}}_x(\theta) - l(\theta)
                        \sim \Nn(0, k\cdot \theta^T C(\theta)\theta)
                $$
                In this paper we shall take $k$ to be large but finite and
                express results about ODE, SDE with error terms such as
                $o(1/k)$.  We emphasize that the constants in these little-$o$s
                are permitted to depend on the loss landscape and optimization
                parameters such as $\eta, T$.  It is physical intuition and
                experiment that determine when such error terms are negligible.

\subsection{Diagrams arise from and organize Taylor series}\label{sect:challenges}
        %\subsection{Diagrams overcome Taylor Series challenges}
        \label{sect:using}\label{sect:diagrams}


    %\subsubsection{Proving Prop \ref{prop:nest}}
    %Let's study $\expct{\theta_T}, \expct{l(\theta_T)}$.  To warm
    %up,
    %we'll prove Prop \ref{prop:nest} (c.f.\ \cite{ne04,ro18}). 

    \subsubsection{Structure of the Taylor Expansion} %---
    We discuss how to analyze SGD by expanding in powers of $\eta$.  We begin
    by proving Prop \ref{prop:nest} (c.f.\ \cite{ne04,ro18}). 

    \begin{proof} %(of Prop \ref{prop:nest}).
        By our gradient bound assumption: $\theta_T - \theta_0$ is $O(\eta^1)$.
        We {claim} that $(\theta_T - \theta_0)^\mu =
        -\sum_t \sum_\nu \eta^{\mu\nu} \nabla_\nu l_{n_t}(\theta_0) + o(\eta^1)$.
        %
        The claim holds when $T=0$.  Say the claim holds for
        ${\tilde T}$-step SGD with
        $T = {\tilde T}+1$.  The displacement
        $\wrap{\theta_{T} - \theta_{{\tilde T}}}^\mu$
        is:%evaluates to:
        \begin{align*}
               & - \textstyle\sum_{\nu} \eta^{\mu\nu} \nabla_\nu l_{n_{\tilde T}}(\theta_{{\tilde T}})   
            \\=& - \textstyle\sum_{\nu} \eta^{\mu\nu} \nabla_\nu \wrap{
                       l_{n_{\tilde T}}(\theta_0)
                       + \text{\translucent{moosky}{$\sum_{\xi} \nabla_\xi l_{n_{\tilde T}}(\theta_0) (\theta_{{\tilde T}} - \theta_0)^\xi$}}
                       + o(\theta_{{\tilde T}} - \theta_0)
                   }    
            \\=& - \textstyle\sum_{\nu} \eta^{\mu\nu} \nabla_\nu \wrap{
                          l_{n_{\tilde T}}(\theta_0)
                          + \nabla l_{n_{\tilde T}}(\theta_0) \cdot O(\eta^1) + o(O(\eta^1))
                      }    
            \\=& \textstyle\text{\translucent{moolime}{$- \sum_\nu \eta^{\mu\nu} \nabla_\nu l_{n_{\tilde T}}(\theta_0)$}} + o(\eta^1)
        \end{align*}
        Applying the induction hypothesis proves the claim.
        %
        We plug the claim into $l$'s Taylor series:
        \begin{align*}
            \expc[l(\theta_T) - l(\theta_0)]
            &= \textstyle \sum_\mu \nabla_\mu l(\theta_0) \text{\translucent{moopink}{$\expc[\theta_T - \theta_0]^\mu$}} + \expc[o(\theta_T - \theta_0)] \\
            &= \textstyle \sum_\mu \nabla_\mu l(\theta_0) (\text{\translucent{moogold}{$-T\eta G + o(\eta^1)$}}) + o(O(\eta^1)) \\
            &= \textstyle - \sum_{\mu\nu} T G_\mu \eta^{\mu\nu} G_\nu + o(\eta^1)
        \end{align*}
        Indeed, due our assumption of analytic moments, the above
        expectations of $o(\eta^1)$ terms are still $o(\eta^1)$.
    \end{proof}

    The above proof gives an order-$1$ result.  %Correction terms involve
At higher order, higher derivatives correct
${\translucent{moosky}{$\cdots$}}$
%${\translucent{moosky}{$\nabla l_{n_{\tilde T}}(\theta_0) \cdot (\theta_{{\tilde T}} - \theta_0)$}}$
and higher
moments augment
\translucent{moopink}{$\cdots$}.
%\translucent{moopink}{$\expc[\theta_T - \theta_0]$}.
%
%%Whereas \translucent{moogold}{$-TG\eta G$}
%%we multiplied
%%$l$'s derivatives by the expectations of such summands.
%
Whereas above the displacement is a sum over $\tilde T$s of
\translucent{moolime}{$\cdots$}s, due to ${\translucent{moosky}{$\cdots$}}$'s
corrections the displacement at higher order is a sum over \emph{tuples} of
times with summands such as $\nabla\nabla l_{n_{T^\prime}} \nabla l_{n_{\tilde
T}}$ instead of $\nabla l_{n_{\tilde T}}$.
%
When we then take expectations of \translucent{moolime}{$\cdots$} to evaluate 
\translucent{moopink}{$\cdots$} as  
\translucent{moogold}{$\cdots$},
some summands (e.g.\ $\expc[\nabla\nabla l_{5}
\nabla l_{2}]=\expc[\nabla\nabla l_{5}]
\expc[\nabla l_{2}]$) are uncorrelated and thus factor; others (e.g.\ $\expc[\nabla\nabla
l_{5} \nabla l_{5}]$) do not.  This is how $\nabla l_x$'s higher cumulants such
as $C, S$ appear in our analysis.
%In short, a Taylor
%expansion even to low degrees yields a combinatorial explosion of
%terms.
%
    %\textsc{Diagrams in brief} ---
    \label{sect:diagrams-in-brief}
%That development begins with the observation that each $\eta$
%`connects' two $\nabla$ operators as indices prescribe.  So we
%draw
%$\eta$s as edges, $\nabla^k l$s as nodes, and each
%summand of the following form 

  Overall, a general summand in \translucent{moolime}{$\cdots$} has the following form (evaluated at $\theta=\theta_0$):
$$
    \sum_{\text{all Greek indices}} \wrap{\prod_{j\in J} \eta^{\mu_j\nu_j}}
    \wrap{\prod_{i\in I} \wrap{\prod_{k\in K_i} \nabla_{\xi_{i,k}}}
    l_{x_i}} \wrap{\prod_{k\in K_\star} \nabla_{\xi_{\star,k}}}
    l
$$
We represent such a summand
as an undirected graph equipped with a partition of nodes, with
edges indexed by $j\in J$, nodes
indexed by $i\in I\sqcup \{\star\}$, an edge $j$ incident
to a node $i$ when $\{\xi_{i,k}:k\in K_i\}$ meets
$\{\mu_j,\nu_j\}$, and nodes $i,i^\prime$ grouped in the partition when
$x_i=x_j$.

%Such diagrams offer the same advantages over direct notation that %of compactness and of clarity as
%decimal numerals offer over unary numerals.  More importantly,
%their topology has dynamical significance and leads to a
%physical interpretation of SGD.




        %\subsection{Diagrams overcome Taylor Series challenges}\label{sect:using}\label{sect:diagrams}

            %%\subsubsection{Diagrams organize the combinatorial explosion of terms}
            %%We formalize \S\ref{sect:diagrams-in-brief}'s identification of
            %%terms (in $\expc[l(\theta_T)]$'s Taylor expansion) with diagrams.
            %%(Throughout, colors help us refer to parts of diagrams; colors lack
            %%mathematical meaning.)
            \begin{dfn}%[\S\ref{appendix:toward-diagrams}]
                \emph{%A \translucent{moolime}{\textbf{diagram}}
                A {\textbf{diagram}}
                is a rooted
                tree equipped with an equivalence relation on (i.e.\ a
                partition of) its non-root nodes.  We orient the tree
                left-to-right so that children precede parents; the root is
                thus rightmost.  We draw the partition structure with 
                fuzzy outlines.%
                %minimally
                %many outlined ties.
                }\mend 
                \squash
            \end{dfn}
            Valid diagrams include
            \squash\squash
            $\sdia{c(0-1)(01)}$,
            $\sdia{c(0-1-2)(02-12)}$,
            $\sdia{c(012-3)(02-12-23)}$,
            $\sdia{c(01-2-3)(02-12-23)}$,
            $\sdia{c(0-1-2-3)(01-12-23)}$,
            $\sdia{MOOc(03-12-4)(03-13-23-34)}$
            but not \squash\squash $\sdia{MOOc(01)(0-1)}$,  
                 $\sdia{MOOc(01)(01)}$, 
                 $\sdia{MOOc(0-1-2)(01-02-12)}$,
                 $\sdia{MOOc(0-1-2)(01-02)}$, 
                 $\sdia{MOOc(0-1-2)(02)}$,
                 $\sdia{MOOc(02-012-3)(02-12-23)}$.
            \squash\squish
            Since a diagram is just a rooted tree and partition,
            $
                \sdia{c(01-2-3)(01-13-23)} = 
                \sdia{c(02-1-3)(02-13-23)} = 
                \sdia{c(0-12-3)(03-12-23)} 
            $ are the same diagrams.

            \squash
            \begin{dfn}%[\S\ref{appendix:evaluate-histories}]\label{dfn:uvalue-body}
                \emph{
                    A diagram $D$'s \squish \emph{un-resummed value} 
                    %(\translucent{moolime}{\textbf{uvalue}})
                    ({\textbf{uvalue}})
                    is a product with one factor of $l_x$'s $d$th derivative
                    for each degree-$d$ node, grouped
                    under cumulant symbols $\CC$ (think: expectation symbols $\expc$)\footnote{
                        Inconsequential technicality: uvalues are products of
                        \emph{cumulants} such as $C$, not of un-centered moments such as
                        $GG+C$.
                        %
                        The symbol $\CC[a]$ gives $a$'s mean; $\CC[a\cdot
                        b]$, $a,b$'s covariance; 
                        we center higher cumulants 
                        $\CC[\prod_i a_i]$ with respect to lower
                        cumulants.
                        %
                        %%See \S\ref{appendix:evaluate-histories}.
                    }
                    per $D$'s fuzzy groups, and tensor-contracted 
                    via a %per $D$'s edges.  We thus have a
                    factor $\eta^{\mu\nu}$ for each edge.
                }\mend
            \end{dfn}
            E.g.\
            $\text{uvalue}(\sdia{c(01-2-3)(02-12-23)})$ is:\squash
                \newcommand{\AAA}{{\color{black}\nabla_\mu}}
                \newcommand{\BBB}{{\color{black}\nabla_\nu}}
                \newcommand{\CCC}{{\color{black}\nabla_\xi}}
                \newcommand{\DDD}{{\color{black}\nabla_\omicron}}
                \newcommand{\EEE}{{\color{black}\nabla_\pi}}
                \newcommand{\FFF}{{\color{black}\nabla_\rho}}
                \newcommand{\ww}[1]{\,\CC[#1]\,}%\wasq{#1}}
                \newcommand{\rRr}[1]{{\color{moor}#1}}
                \newcommand{\gGg}[1]{{\color{moog}#1}}
                \newcommand{\bBb}[1]{{\color{moob}#1}}
                \newcommand{\sixsum}{\textstyle\sum_{{\mu\nu\xi\omicron\pi\rho}} }
            %\begin{figure}[H]
            %    \centering  
            %    \crunch
            %    \begin{tabular}{rcl}
            %        %%$\text{uvalue}(\mdia{c(01-2-3)(02-13-23)})$ &
            %        %%$=$ &
            %        %%$\sixsum \eta^{\mu\xi} \eta^{\nu\pi} \eta^{\omicron\rho} \, \ww{(\rRr{\AAA l_x}) \cdot (\rRr{\BBB l_x})}       \ww{(\gGg{\CCC \DDD      l_x})} \ww{(\bBb{\EEE \FFF l_x})}$\\
            %        %%\crunch\squash\squash
            %        %%\\                   
            %        %%$\text{uvalue}(\mdia{c(01-2-3)(02-12-23)})$ &
            %        %%$=$ &                                         
                    $$\sixsum \eta^{\mu\xi} \eta^{\nu\omicron} \eta^{\pi\rho} \, \ww{(\rRr{\AAA l_x}) \cdot (\rRr{\BBB l_x})}       \ww{(\gGg{\CCC \DDD \EEE l_x})} \ww{(\bBb{     \FFF l_x})}$$
                    %\\
                    %\crunch\squash\squash
                    %%\\
                    %%$\text{uvalue}(\mdia{c(012-3)(02-12-23)})$ &
                    %%$=$ &
                    %%$\sixsum \eta^{\mu\xi} \eta^{\nu\omicron} \eta^{\pi\rho} \,\, \ww{(\rRr{\AAA l_x}) \cdot (\rRr{\BBB l_x})  \cdot     (\rRr{\CCC \DDD \EEE l_x})} \ww{(\gGg{     \FFF l_x})}$
            %    \end{tabular}
            %    \crunch
            %    %\caption{
            %    %    Illustration of \textbf{Def.\ 
            %    %    \ref{dfn:uvalue-body}}.
            %    %}
            %    \label{fig:uvalue-example}
            %\end{figure}

            There are dozens of small diagrams.  In many analyses, only a few
            diagrams are relevant.  Examples: for fixed $T$, \textbf{to order
            $d$ we may neglect diagrams with more than $d$ edges};
            %
            if $E=B=1$ (\S\ref{sect:background}), each diagram with an ancestor-descendant pair in the
            same part contributes zero to $\expc[l(\theta_T)]$; for $\theta_0$ a minimum of
            $l$, all diagrams vanish that contain a leaf node
            not fuzzily grouped.%\footnote{
                %Recall that $E,T,B,M,N;G,H,J,C,S;\Dd,\Mm,l_x,\eta,\theta$
                %are defined in \S\ref{sect:background}.
            %}

            \subsubsection{SGD as a Sum over Histories}\label{sect:histories}
            %\textsc{SGD as a Sum over Histories}\label{sect:histories} ---
            %-----  histories and patterns of influence  ---------------------
            %\subsubsection{Embeddings}
            Having expressed terms in $\expc[l(\theta_T)]$'s Taylor expansion %(\ref{eq:dyson})
            as uvalues of
            diagrams, we seek the coefficient for each uvalue.  Intuitively, a
            diagram represents a process (as in Figure \ref{fig:paradigm}) and
            a diagram's contribution scales with the number of ways that
            process may occur.
            Specifically, the Key Lemma in \S{\ref{appendix:key-lemma}
            establishes that a diagram's coefficient in the expansion is the
            number of its \emph{histories} (weighted by symmetry factors to
            counter overcounting), where we define \emph{histories}
            below with
            respect to given SGD hyperparameters $N,E,B$.\footnote{
                and w.r.t.\ a deterministic selector of the
                $t$th batch.  One may take expectations over such
                algorithms --- \S\ref{appendix:draw-spacetime}. 
            }

            \begin{dfn}
                \emph{A \textbf{history} of a diagram is an assignment of non-root
                nodes to $\emph{(n,t)}$ pairs such that: the $\emph{n}$th
                training point participates in the $t$th batch; parents'
                $\emph{t}$s strictly exceed their children's $\emph{t}$s; and
                any two nodes' $\emph{n}$s are equal if the nodes
                are in the same part of the partition.}\mend 
            \end{dfn}

            E.g.\ $\sdia{(0-1)(01)}$ has just one non-root node.  It has
            as many histories as there are $(n,t)$ cells where $n$ participates
            in the $t$th update, i.e.,
            $B\cdot T$ histories.  Since $\sdia{(0-1)(01)}$ is the only
            $1$-edged diagram, it gives the full $\eta^1$ contribution to
            final testing loss.  
            %%We thus recover Prop
            %%\ref{prop:nest}:
            %%    \vspace{-0.15cm}
            %%$$
            %%    -(\text{\# of histories of~}\sdia{(0-1)(01)}) \cdot \uvalue(\sdia{(0-1)(01)}) / B
            %%    =
            %%    -T \cdot \textstyle\sum_{\mu\nu} G_\mu \eta^{\mu\nu} G_\nu
            %%$$
            %%    \vspace{-0.65cm}

            %-----  isolate effect of tensors; crossing symmetries  -----------
            Diagrams streamline analysis of SGD because it is in practice
            straightforward to count a diagram's histories.  
            %
            Also, the topology of diagrams has dynamical significance: the $t^d
            T^{-p}$-th order correction\footnote{We compare ODE integrated to
            time $t$ to $T$ steps of SGD with $\eta = \eta_\star t/T$ and
            $E=B=1$, and we assume $p\neq 0$.} to the ODE approximation of SGD
            is given by diagrams with $d$ edges and $d+1-p$ many fuzzy groups
            (counting each node not fuzzily grouped as its own group).
            Likewise, if we seek to isolate the effect, say, of $C$ or $H$ or
            $S$ or $J$, we may consider only those diagrams that contain the
            corresponding subgraph.%in Figure \ref{fig:tensor}.

            %
            %Moreover, we may
            %compute the effect of the skewness of gradient noise in isolation
            %by evaluating only those diagrams containing
            %$\sdia{MOOc(012)(0-1-2)}$.    

            %We interpret edges as carrying influence from the training set
            %toward the test measurement (figure \ref{fig:paradigm}). 

            %\begin{wrapfigure}{r}{0.4\textwidth}
            %    \centering  
            %    \plotmoow{diagrams/spacetime-f}{0.99\linewidth}{}
            %    \caption{
            %        %\textbf{Edges carry information}.
            %        Embedding of $\mdia{MOOc(01-2-3-4)(04-13-23-34)}$.
            %    }
            %    \vspace{-0.20cm}
            %    \label{fig:intuition1}
            %\end{wrapfigure}

            %    Moreover, a diagram's uvalue depends only on its
            %    graph and partition structures (not on its root), so, e.g.:\footnote{The physics-oriented reader
            %    will recognize this as a \emph{crossing symmetry}.}
            %    $$\uvalue(\sdia{c(0-1-2)(01-12)})=
            %    \uvalue(\sdia{c(0-1-2)(02-12)})$$ 
            %    %These relations reduce the effort of evaluating (\ref{eq:dyson}).
            %    These relate the contribution of a process (e.g.\
            %    $\sdia{c(0-1-2)(01-12)}$, whose degree-two node temporally
            %    separates its neighbors) to a time-distorted version of the process
            %    (e.g.\ $\sdia{c(0-1-2)(02-12)}$, whose
            %    degree-two node succeeds its neighbors). 


            %\newpage
            %-----  resummation  ----------------------------------------------
            \subsection{Re-summation cures large-$T$ divergences}\label{sect:resummation}
            So far, diagrams have been a convenient but dispensible
            book-keeping tool; so far, \S\ref{sect:challenges}'s polynomial
            divergence remains in (\ref{eq:sgdcoef}).  We now show how
            diagrams enable us to tame this divergence.

            Let us collect similar diagrams, where our notion of
            `similar' permits chains to grow or shrink (see Definition
            \ref{dfn:link}).  We obtain lists (each conveniently represented by
            its smallest member) such as %: \squash
            %%%%\vspace{-0.30cm}
            %%%%\begin{align*}
            %%    $\sdia{c(0-1)(01)}$,
            %%    $\sdia{c(0-1-2)(01-12)}$,
            %%    $\sdia{c(0-1-2-3)(01-12-23)}, \cdots$ or
            %%    %%\sdia{MOOc(0-1-2-3-4)(01-12-23-34)},\cdots
            %%%%    &&
                $\sdia{c(01-2)(02-12)}$,
                $\sdia{c(02-1-3)(01-13-23)}$,
                $\sdia{MOOc(03-1-2-4)(01-12-24-34)},\cdots$.\squash
                %$\sdia{MOOc(02-1-3-4)(01-14-23-34)},\cdots
            %%\end{align*}
            %%\vspace{-0.60cm}
            \noindent
            We will express in closed form the total contribution to
            (\ref{eq:sgdcoef}) of all diagrams in such a list.  The idea is that
            the uvalues of chains are powers of hessians --- e.g.\
            $\uvalue(\sdia{MOOc(0-1-2-3-4)(01-12-23-34)}) = (\eta)^4 GH^3G$ --- so we
            may sum over chain lengths via geometric series.

            \begin{figure}%{r}{0.2\linewidth}
                \centering
                \plotmoow{aaai/anatomy-diagram-b}{4.15cm}{}
                \plotmoow{aaai/resum-cubic-histories}{4.15cm}{}
                %\plotmooh{aaai/chladni-drift}{}{2.7cm}
                \caption{%
                  {\protect\offiveprime{0}}: a diagram consists of nodes,
                  edges, and fuzzy groupings that dictate the diagram's
                  corresponding tensor expression.
                  %Summing 
                  %over all possible histories gives the expected testing loss.
                  %diagram $\sdia{c(01-2-3)(02-12-23)}$ evaluates to
                  %$C(-\eta)^2(\nabla H)(-\eta)G$.
                  {\protect\offiveprime{1}}
                  shows one of a diagram's histories.  Re-summation balances
                  that history topologically related histories of
                  related diagrams such as {\!\!\protect\offiveprime{234}}.
                }
            \end{figure}

            %This is the \emph{resummed value} (\emph{rvalue}) of the list's
            %smallest member.
            We define an embedded diagram's \emph{resummed value} or
            \emph{\textbf{rvalue}} as we defined
            the $\uvalue$, except that we use $(I-\eta H)^{\Delta t-1}\eta$
            instead of $\eta$ to contract a pair of tensors embedded 
            $\Delta t$ timesteps apart.
            %
            For example, take Figure \ref{fig:resumintuition}'s history of
            $\sdia{c(0-1)(01)}$ (topmost of four).  The associated uvalue  is
            $\sum_{\mu\nu} G_\mu\eta^{\mu\nu}G_\nu$: a $G$ for each degree-one node and an
            $\eta$ for each edge.  By contrast, the associated rvalue is
            $\sum_{\mu\nu} G_\mu((I-\eta H)^{11-1}\eta)^{\mu\nu}G_\nu$ since the edge spans
            $11$ timesteps.  Distributing out this expression reveals uvalues for
            histories of $\sdia{c(0-1-2)(01-12)}$, etc.
            %A sum over histories is a sum of terms with a
            %range of exponents in place of $12-1$.

            \squash\squash
            \begin{dfn}\label{dfn:link}
                \emph{A \textbf{link} is a degree-$2$ non-root node that
                is not fuzzily grouped.
                E.g.\ $\sdia{c(02-1-3)(01-13-23)}$ has one link (green).
                To \emph{reduce} at a link, we
                replace the link by a black edge connecting the link's two
                neighbors.  E.g.\ $\sdia{c(02-1-3)(01-13-23)}\rightsquigarrow\sdia{c(01-2)(02-12)}$.  Reduction generates an equivalence relation on
                diagrams. Each equivalence class contains exactly one
                \textbf{linkless} diagram.  }\mend
            \end{dfn}

        \subsection{Main result}\label{sect:main}
    
            %-----  recipe for test loss  -------------------------------------

            \begin{thm} \label{thm:resum}
                $\forall T:\, \exists \eta_0 \succ 0:\,
                \forall 0\preceq \eta \prec \eta_0:\,$
                the final testing loss is
                a sum over \emph{linkless} diagrams: 
                    \squash
                \begin{equation*} \label{eq:resum}
                    \expc[l(\theta_T)]=
                    \sum_{\substack{D~\text{a linkless} \\ \text{diagram}}}
                    ~
                    \sum_{\substack{f~\text{an embed-} \\ \text{-ding of}~D}}
                    ~
                    \frac{1}{\wabs{\Aut_f(D)}}
                    \,
                    \frac{{\rvalue_f}(D)}{(-B)^{|\edges(D)|}}
                    \squash
                \end{equation*}
                Here, $\wabs{\Aut_f(D)}$ counts the graph automorphisms of $D$
                that preserve $f$. (Typically $\wabs{\Aut_f(D)}=1$.)%(see \ref{appendix:sum-histories} for  ).
            \end{thm}
            %-----  simplifications  ------------------------------------------
            \begin{rmk} \label{rmk:integrate}
                \emph{
                %Theorem \ref{thm:resum} expresses SGD's testing loss as a
                %sum over diagrams.
                A diagram with $d$ edges scales as
                $O(\eta^d)$, so the Theorem expresses a series in $\eta$.  In
                practice, we truncate to small $d$ (thus focusing on few-edged
                diagrams) and we replace sums over histories by integrals over
                $t$; $(I-\eta H)^t$, by $\exp(- \eta H t)$, thus reducing to a
                routine integration of exponentials at the cost an error factor
                $1 + o(\eta)$.}\mend
                %\squash
            \end{rmk}
            \begin{rmk}
                \emph{Theorem \ref{thm:resum} gives us the expectation of 
                the testing loss.  Straightforward variations of the theorem permit us
                to compute variances instead of expectations, training
                statistics instead of testing statistics, and weight
                displacements instead of losses.  See \S{appendix:solve-variants}.}\mend
                %\squash
            \end{rmk}
            %-----  convergence  ----------------------------------------------
            \begin{thm} \label{thm:converge}
                For constant-$M=P$ or constant-$N=P$ SGD:
                $\forall \theta_\star,P:\, (G(\theta)=0 \wedge H(\theta)\succ
                0) \implies \exists U\ni \theta_\star\,\,\text{open}:\, \forall
                \theta_0\in U:\,$
                Theorem \ref{thm:resum}'s $d$th-order truncation converges
                as $T\to \infty$.
                %Assuming Local Strength, the truncation's large-time error
                %$\lim_{T\to\infty} \epsilon(\eta,T)$ exists for small $\eta$
                %and is $o(\eta^d)$
            \end{thm}
            {\large\color{red}QUADRATIC EXACTNESS!}
            \begin{rmk}
                \emph{
                Thm \ref{thm:converge} claims only that a limit
                $\lim_{T\to\infty} L_d(T,\eta)$ of the
                $d$th-order truncation $L_d(T,\eta)$ exists.
                %
                It does not compare $\lim_{d\to\infty}
                \lim_{T\to\infty} L_d(T,\eta)$ with $\lim_{T\to\infty}
                \lim_{d\to\infty} L_d(T,\eta)$ (however, we note that we have not in practice
                observed pathologies of non-commuting limits).
                %
                Our theory suggests but does not guarantee that when $d, T$ are
                large (and finite), then computation of $L_d(T,\eta)$ by Taylor
                methods gives insight into SGD's behavior.  It is by empirical
                tests and physical intution that we decide whether in a given
                situation our theory's little-$o$ error terms may be ignored. 
                }
            \end{rmk}

        \subsubsection{Example Computation A}
            \begin{figure}%{r}{0.4\textwidth}
                \crunch
                \begin{tabular}{ccc}
                    diagram                     & \#embed.s         & interpretation            \\\hline
                    $\sdia{c(0-1)(01)}$         & $ T            $  & na\"ive descent           \\
                    $\sdia{c(0-1-2)(02-12)}$    & ${T+1\choose 2}$  & $\theta$-dependent loss   \\
                    $\sdia{c(01-2)(02-12)}$     & $ T            $  & gradient noise            \\
                    $\sdia{c(01-2)(01-12)}$     & $ 0            $  & correl'd batches
                \end{tabular}
                \crunch
            \end{figure}
            We improve on Prop \ref{prop:nest} for SGD with $E=B=1$.
            The one $1$-edged diagram ($\sdia{c(0-1)(01)}$) embeds
            in $T$ ways (one for each timestep) and contributes (let
            $K^\mu_\nu = \sum_{\xi} \eta^{\mu\xi} H_{\xi\nu}$):
            $$
                \sum_{0\leq t<T} \sum_{\mu\nu} G_\mu \wasq{(I-K)^{T-t-1} \eta}^{\mu\nu} G_\nu 
                = 
                \sum_{\mu\nu} G_\mu \wasq{\frac{I - K^T}{I - K} \eta}^{\mu\nu} G_\nu 
            $$
            to the loss.  This is the re-summed %(Hessian-corrected)
            Prop.

        \subsubsection{Example Computation B}
            There are three $2$-edged linkless diagrams (see table
            for intuitive interpretations).
            The two diagrams involving higher cumulants (i.e., gray
            outlines) give the leading order effect of gradient noise. 
            Since $E=1$, $\sdia{c(01-2)(01-12)}$ has no histories; so 
            only $\sdia{c(01-2)(02-12)}$ contributes.  Up to a $1+o(\eta)$
            factor, its rvalues sum to (a sum over all indices of):
            \begin{align*}
                &\int_t
                C_{\mu\nu}
                    \wasq{
                        \exp\wrap{-(T-t)(K\otimes I + I\otimes K)}
                    }^{\mu\nu}_{\xi\omicron}
                    \eta^{\xi\pi}
                    \eta^{\omicron\rho}
                H_{\pi\rho}
                \\=
                &C_{\mu\nu}
                    \wasq{
                        \frac{
                            I-\exp\wrap{-T(K\otimes I + I\otimes K)}
                        }{
                            (K\otimes I + I\otimes K)
                        }
                    }^{\mu\nu}_{\xi\omicron}
                    \eta^{\xi\pi}
                    \eta^{\omicron\rho}
                H_{\pi\rho}
            \end{align*}
            We used Remark \ref{rmk:integrate} to approximate
            $(I-K)^\mu_\xi(I-K)^\nu_\omicron$ by $\exp(-K\otimes I - I\otimes K)^{\mu\nu}_{\xi\omicron}$.
            The above is the leading contribution of the gradient covariance $C$
            to SGD's final testing loss.  We have derived an $E=B=1$ variant of
            Corollary \ref{cor:overfit}'s 
            $E=T$, $B=N$ result (\S\ref{subsect:curvature-and-overfitting}).


        \subsubsection{On Factoring}

