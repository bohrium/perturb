\relax 
\providecommand\hyper@newdestlabel[2]{}
\bibstyle{aaai21}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{bo91,le15}
\citation{go18,wu20}
\providecommand \oddpage@label [2]{}
\providecommand {\FN@pp@footnotehinttrue }{}
\providecommand {\FN@pp@footnote@aux }[2]{}
\FN@pp@footnotehinttrue 
\@writefile{toc}{\contentsline {section}{Introduction}{1}{section*.1}\protected@file@percent }
\newlabel{sect:intro}{{}{1}{Introduction}{section*.1}{}}
\@writefile{toc}{\contentsline {section}{A paradigmatic example}{1}{section*.2}\protected@file@percent }
\newlabel{sect:entropic-example}{{}{1}{A paradigmatic example}{section*.2}{}}
\@writefile{toc}{\contentsline {subsection}{Notation and assumptions, I}{1}{section*.3}\protected@file@percent }
\newlabel{sect:setup}{{}{1}{Notation and assumptions, I}{section*.3}{}}
\citation{mi73}
\citation{cu87}
\citation{ne04}
\citation{ya19b}
\FN@pp@footnote@aux{1}{2}
\newlabel{prop:nest}{{0}{2}{}{prop.0}{}}
\@writefile{toc}{\contentsline {subsection}{Informal statement of result}{2}{section*.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Left}: Diagrams evaluate to tensor expressions: a degree-$d$ node gives a $d$th derivative; an edge gives a $(-\eta )$; expectation brackets enclose each fuzzy group. \textbf  {Right}: Each diagram depicts a class of histories; here is an example history for each of $\sizeddia {c(0-1)(01)}{0.10}, \sizeddia {c(01-2)(02-12)}{0.10}, \sizeddia {c(0-1-2)(02-12)}{0.10}$. Summing diagram values over all possible histories gives the expected testing loss. \relax }}{2}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:recipe-a}{{1}{2}{\textbf {Left}: Diagrams evaluate to tensor expressions: a degree-$d$ node gives a $d$th derivative; an edge gives a $(-\eta )$; expectation brackets enclose each fuzzy group. \textbf {Right}: Each diagram depicts a class of histories; here is an example history for each of $\sdia {c(0-1)(01)}, \sdia {c(01-2)(02-12)}, \sdia {c(0-1-2)(02-12)}$. Summing diagram values over all possible histories gives the expected testing loss. \relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Left}: a diagram consists of nodes, edges, and fuzzy groupings that dictate the diagram's corresponding tensor expression. \textbf  {Right}: Gradient noise pushes SGD toward minima flat w.r.t.\ $C$. A 2D loss near a valley of minima. Red densities show typical $\theta $s, perturbed by noise, in two slices of the valley. The hessian changes across the valley: $J \neq 0$. \relax }}{2}{figure.caption.7}\protected@file@percent }
\newlabel{fig:cubic}{{2}{2}{\textbf {Left}: a diagram consists of nodes, edges, and fuzzy groupings that dictate the diagram's corresponding tensor expression. \textbf {Right}: Gradient noise pushes SGD toward minima flat w.r.t.\ $C$. A 2D loss near a valley of minima. Red densities show typical $\theta $s, perturbed by noise, in two slices of the valley. The hessian changes across the valley: $J \neq 0$. \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{SGD descends on a $C$-smoothed landscape}{2}{section*.6}\protected@file@percent }
\newlabel{sect:entropic-curl}{{}{2}{SGD descends on a $C$-smoothed landscape}{section*.6}{}}
\citation{ch18}
\citation{we19b}
\citation{li18,ba21}
\FN@pp@footnote@aux{2}{3}
\FN@pp@footnote@aux{3}{3}
\newlabel{cor:entropic}{{1}{3}{}{cor.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Subfigure {\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\offour {0}}: \textsc  {Helix}\ is defined on an $\mathcal  {M}=\mathbb  {R}^3$ extending into the page. A helical level surface $S$ (blue) of $l$ winds around a 1D valley of minima orthogonal to the page. Gradient noise (red bi-arrows), parallel to the page, twists out of phase with $l$. {\offour {123}}: SGD's trajectory (green) over cross sections of \textsc  {Helix}\ descend progressively into the page. Dotted curves help compare adjacent panes. Gradient noise kicks $\theta $ from A; $\theta $ then falls ({\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\offour {1}}) to B in {\offour {2}}. At C, noise kicks $\theta $ uphill (\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\offour {3}); $\theta $ thus never settles and the descent persists. \relax }}{3}{figure.caption.8}\protected@file@percent }
\newlabel{fig:archimedes}{{3}{3}{Subfigure {\!\!\protect \offour {0}}: \Helix \ is defined on an $\Mm =\RR ^3$ extending into the page. A helical level surface $S$ (blue) of $l$ winds around a 1D valley of minima orthogonal to the page. Gradient noise (red bi-arrows), parallel to the page, twists out of phase with $l$. {\protect \offour {123}}: SGD's trajectory (green) over cross sections of \Helix \ descend progressively into the page. Dotted curves help compare adjacent panes. Gradient noise kicks $\theta $ from A; $\theta $ then falls ({\!\!\protect \offour {1}}) to B in {\protect \offour {2}}. At C, noise kicks $\theta $ uphill (\!\!\protect \offour {3}); $\theta $ thus never settles and the descent persists. \relax }{figure.caption.8}{}}
\FN@pp@footnote@aux{4}{3}
\@writefile{toc}{\contentsline {section}{Perturbation theory of SGD}{3}{section*.9}\protected@file@percent }
\newlabel{sect:calculus}{{}{3}{Perturbation theory of SGD}{section*.9}{}}
\@writefile{toc}{\contentsline {subsection}{Notation and assumptions, II}{3}{section*.10}\protected@file@percent }
\newlabel{sect:background}{{}{3}{Notation and assumptions, II}{section*.10}{}}
\@writefile{toc}{\contentsline {subsection}{Diagrams arise from and organize Taylor series}{3}{section*.11}\protected@file@percent }
\newlabel{sect:challenges}{{}{3}{Diagrams arise from and organize Taylor series}{section*.11}{}}
\FN@pp@footnote@aux{5}{4}
\newlabel{sect:histories}{{}{4}{SGD as a Sum over Histories}{section*.12}{}}
\@writefile{toc}{\contentsline {subparagraph}{SGD as a Sum over Histories}{4}{section*.12}\protected@file@percent }
\FN@pp@footnote@aux{6}{4}
\FN@pp@footnote@aux{7}{4}
\newlabel{sect:resummation}{{}{4}{Re-summation cures large-$T$ divergences}{section*.13}{}}
\@writefile{toc}{\contentsline {subparagraph}{Re-summation cures large-$T$ divergences}{4}{section*.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Left}: Re-summation evaluates a whole class of topologically related histories at once. Here are example histories that appear in a re-summed value for $\sizeddia {c(01-2-3)(02-12-23)}{0.10}$. \textbf  {Right}: The Taylor expansion presents SGD behavior as ``default trajectories'' in weight space (intuitively represented by diagram edges) punctuated by noise (here, red arrow) and inter-update interactions (nodes and fuzzy groupings). Un-resummed diagrams correspond to straight default trajectories (green). By contrast, re-summed diagrams model default trajectories (purple) that curve along with the hessian. Thus, un-resummed expressions tend to diverge with large time, while re-summed expressions treat trajectories trapped near minima (dark blue). \relax }}{4}{figure.caption.14}\protected@file@percent }
\newlabel{fig:resum-cubic-histories}{{4}{4}{\textbf {Left}: Re-summation evaluates a whole class of topologically related histories at once. Here are example histories that appear in a re-summed value for $\sdia {c(01-2-3)(02-12-23)}$. \textbf {Right}: The Taylor expansion presents SGD behavior as ``default trajectories'' in weight space (intuitively represented by diagram edges) punctuated by noise (here, red arrow) and inter-update interactions (nodes and fuzzy groupings). Un-resummed diagrams correspond to straight default trajectories (green). By contrast, re-summed diagrams model default trajectories (purple) that curve along with the hessian. Thus, un-resummed expressions tend to diverge with large time, while re-summed expressions treat trajectories trapped near minima (dark blue). \relax }{figure.caption.14}{}}
\newlabel{dfn:link}{{5}{4}{}{dfn.5}{}}
\@writefile{toc}{\contentsline {subsection}{Main result}{4}{section*.15}\protected@file@percent }
\newlabel{sect:main}{{}{4}{Main result}{section*.15}{}}
\newlabel{thm:resum}{{1}{4}{}{thm.1}{}}
\newlabel{eq:resum}{{1}{4}{}{thm.1}{}}
\newlabel{rmk:integrate}{{1}{4}{}{rmk.1}{}}
\citation{ch87}
\citation{ro19}
\citation{ro18}
\citation{ba21}
\newlabel{thm:converge}{{2}{5}{}{thm.2}{}}
\@writefile{toc}{\contentsline {subparagraph}{Example Computation A}{5}{figure.caption.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Example Computation B}{5}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Consequences of the theory}{5}{section*.19}\protected@file@percent }
\newlabel{sect:consequences}{{}{5}{Consequences of the theory}{section*.19}{}}
\@writefile{toc}{\contentsline {subsection}{Gradient noise repels SGD}{5}{section*.20}\protected@file@percent }
\newlabel{subsect:epochs-batch}{{}{5}{Gradient noise repels SGD}{section*.20}{}}
\FN@pp@footnote@aux{8}{5}
\newlabel{cor:batch}{{2}{5}{Computed from $\sdia {c(01-2)(01-12)}$}{cor.2}{}}
\newlabel{cor:epochs}{{3}{5}{$\sdia {c(01-2)(01-12)}$}{cor.3}{}}
\@writefile{toc}{\contentsline {subsection}{Time discretization penalizes sloped regions}{5}{section*.21}\protected@file@percent }
\newlabel{cor:epochs}{{4}{5}{$\sdia {c(0-1-2)(01-12)}$}{cor.4}{}}
\newlabel{cor:vsode}{{5}{5}{$\sdia {c(01-2)(02-12)}$}{cor.5}{}}
\@writefile{toc}{\contentsline {subsection}{Jerk distinguishes SDE and SGD}{5}{section*.22}\protected@file@percent }
\citation{ch18}
\citation{di18}
\citation{di18}
\citation{ki52}
\citation{ca47}
\citation{ro51}
\citation{we74}
\citation{bo91}
\citation{le15}
\citation{ne17a}
\citation{ba17}
\citation{zh17}
\citation{ne17b}
\citation{ch18,zh19}
\citation{ya19a}
\citation{ro18}
\citation{be19}
\citation{me20}
\citation{me20}
\citation{mo18b}
\citation{zo20}
\citation{wu20}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Subfigure {\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\ofsix {0}}: {\ofsix {1}}: Chladni drift on $\mathcal  {M}=\mathbb  {R}^2$. Red bi-arrows depict $C(\theta )$'s major axis. SGD updates (green) tend toward small $C$. {\ofsix {2345}}: Both curvature and noise affect overfitting. In each pane, the $\leftrightarrow $ axis represents weight space and the $\updownarrow $ axis represents loss. Noise (blue) transforms the testing loss (thin curve) into the observed loss (thick curve). Red dots mark the testing loss at the arg-min of the observed loss. \ofsix {24}: \emph  {covector}-perturbed landscapes favor large $H$s. \ofsix {35}: \emph  {vector}-perturbed landscapes favor small $H$s. \relax }}{6}{figure.caption.24}\protected@file@percent }
\newlabel{fig:spring}{{5}{6}{Subfigure {\!\!\protect \ofsix {0}}: {\protect \ofsix {1}}: Chladni drift on $\Mm =\RR ^2$. Red bi-arrows depict $C(\theta )$'s major axis. SGD updates (green) tend toward small $C$. {\protect \ofsix {2345}}: Both curvature and noise affect overfitting. In each pane, the $\leftrightarrow $ axis represents weight space and the $\updownarrow $ axis represents loss. Noise (blue) transforms the testing loss (thin curve) into the observed loss (thick curve). Red dots mark the testing loss at the arg-min of the observed loss. \protect \ofsix {24}: \emph {covector}-perturbed landscapes favor large $H$s. \protect \ofsix {35}: \emph {vector}-perturbed landscapes favor small $H$s. \relax }{figure.caption.24}{}}
\newlabel{cor:vsode}{{6}{6}{$\sdia {c(012-3)(03-13-23)}$}{cor.6}{}}
\@writefile{toc}{\contentsline {subsection}{Both flat and sharp minima overfit less}{6}{section*.23}\protected@file@percent }
\newlabel{subsect:curvature-and-overfitting}{{}{6}{Both flat and sharp minima overfit less}{section*.23}{}}
\newlabel{cor:overfit}{{7}{6}{from $\sdia {c(01-2)(02-12)}$, $\sdia {c(01)(01)}$}{cor.7}{}}
\@writefile{toc}{\contentsline {subsection}{Experiments}{6}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Discussion}{6}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Related work}{6}{section*.28}\protected@file@percent }
\newlabel{sect:related}{{}{6}{Related work}{section*.28}{}}
\FN@pp@footnote@aux{9}{6}
\citation{li18}
\citation{ho17,ke17,wa18}
\citation{st56,di17,wu18}
\citation{dy19}
\citation{ch18}
\citation{li17}
\citation{ba21}
\citation{pe71}
\citation{li18,we19b,zh19,ba21}
\citation{fi17}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Subfigure {\tmspace  -\thinmuskip {.1667em}\tmspace  -\thinmuskip {.1667em}\ofsixvert {0}}: Fashion-MNIST convnet's testing loss vs learning rate. For all initializations tested ($1$ shown, $11$ unshown), the order $3$ prediction agrees with experiment through $\eta T \approx 10^0$, corresponding to a decrease in $0\mbox  {-}1$ error of $\approx 10^{-3}$. \ofsixvert {1}: Fashion-MNIST convnet's testing loss. For large $\eta T$, our predictions break down. Here, the order $3$ prediction holds until the $0\mbox  {-}1$ error improves by $5\cdot 10^{-3}$. Beyond this, $2$nd order agreement with experiment is coincidental. \ofsixvert {2}: CIFAR-10 convnet generalization gaps. For all initializations tested ($1$ shown, $11$ unshown), the degree-$2$ prediction agrees with experiment through $\eta T \approx 5\cdot 10^{-1}$. \ofsixvert {3}: Fashion-MNIST convnet. SGD with $2, 3, 5, 8$ epochs incurs greater test loss than one-epoch SGD (difference shown in I bars) by the predicted amounts (predictions shaded) for a range of learning rates. Here, all SGD runs have $N=10$; we scale the learning rate for $E$-epoch SGD by $1/E$ to isolate the effect of inter-epoch correlations away from the effect of larger $\eta T$. \ofsixvert {4}: SGD traverses \textsc  {Helix}' valley of global minima. Note: $H$ and $C$ are bounded across the valley, we see drift for all small $\eta $, and we see displacement exceeding the landscape's period of $2\pi $. So: the drift is not a pathology of well-chosen $\eta $, of divergent noise, or of ephemeral initial conditions. \ofsixvert {5}: For \textsc  {Mean Estimation}\tmspace  +\thinmuskip {.1667em} with fixed $C$ and a range of $H$s, initialized at the truth, the testing losses after fixed-$T$ GD are smallest for very sharp and very flat $H$. Near $H=0$, our predictions improve on AIC,TIC \citep  {di18}. \relax }}{7}{figure.caption.26}\protected@file@percent }
\newlabel{fig:experiments}{{6}{7}{Subfigure {\!\!\protect \ofsixvert {0}}: Fashion-MNIST convnet's testing loss vs learning rate. For all initializations tested ($1$ shown, $11$ unshown), the order $3$ prediction agrees with experiment through $\eta T \approx 10^0$, corresponding to a decrease in $0\mbox {-}1$ error of $\approx 10^{-3}$. \protect \ofsixvert {1}: Fashion-MNIST convnet's testing loss. For large $\eta T$, our predictions break down. Here, the order $3$ prediction holds until the $0\mbox {-}1$ error improves by $5\cdot 10^{-3}$. Beyond this, $2$nd order agreement with experiment is coincidental. \protect \ofsixvert {2}: CIFAR-10 convnet generalization gaps. For all initializations tested ($1$ shown, $11$ unshown), the degree-$2$ prediction agrees with experiment through $\eta T \approx 5\cdot 10^{-1}$. \protect \ofsixvert {3}: Fashion-MNIST convnet. SGD with $2, 3, 5, 8$ epochs incurs greater test loss than one-epoch SGD (difference shown in I bars) by the predicted amounts (predictions shaded) for a range of learning rates. Here, all SGD runs have $N=10$; we scale the learning rate for $E$-epoch SGD by $1/E$ to isolate the effect of inter-epoch correlations away from the effect of larger $\eta T$. \protect \ofsixvert {4}: SGD traverses \Helix ' valley of global minima. Note: $H$ and $C$ are bounded across the valley, we see drift for all small $\eta $, and we see displacement exceeding the landscape's period of $2\pi $. So: the drift is not a pathology of well-chosen $\eta $, of divergent noise, or of ephemeral initial conditions. \protect \ofsixvert {5}: For \MeanEstimation \, with fixed $C$ and a range of $H$s, initialized at the truth, the testing losses after fixed-$T$ GD are smallest for very sharp and very flat $H$. Near $H=0$, our predictions improve on AIC,TIC \citep {di18}. \relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{Conclusion}{7}{section*.29}\protected@file@percent }
\citation{ro18}
\citation{ne04,ro18}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  \textbf  {Resummation propagates information damped by curvature}. Each resummed value (here, for $\sizeddia {c(0-1)(01)}{0.10}$) represents many un-resummed values, four shown here, each modulated by the Hessian ($\sizeddia {MOOc(0)(0-0)}{0.10}$) in a different way. \relax }}{9}{figure.caption.30}\protected@file@percent }
\newlabel{fig:resumintuition}{{7}{9}{\textbf {Resummation propagates information damped by curvature}. Each resummed value (here, for $\sdia {c(0-1)(01)}$) represents many un-resummed values, four shown here, each modulated by the Hessian ($\sdia {MOOc(0)(0-0)}$) in a different way. \relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  \textbf  {Named tensors}, typically evaluated at initialization ($\theta =\theta _0$). Def.\ \ref  {dfn:uvalue-body} explains how diagrams depict tensors. \relax }}{9}{figure.caption.31}\protected@file@percent }
\newlabel{fig:tensor}{{8}{9}{\textbf {Named tensors}, typically evaluated at initialization ($\theta =\theta _0$). Def.\^^M\ref {dfn:uvalue-body} explains how diagrams depict tensors. \relax }{figure.caption.31}{}}
\newlabel{cor:epochs}{{8}{9}{$\sdia {c(01-2)(01-12)}$}{cor.8}{}}
\FN@pp@footnote@aux{10}{9}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  \textbf  {A sub-process of SGD}. Timesteps index columns; training data index rows. The $5$th datum participates in the $2$nd SGD update. This {\color  {spacetimepurple}$(n=5,t=2)$ event} affects the testing loss both directly and via the {\color  {spacetimeteal}$(1,12)$ event}, which is itself modulated by the {\color  {spacetimeindigo}$(2,5)$ event}. \relax }}{9}{figure.caption.32}\protected@file@percent }
\newlabel{fig:paradigm}{{9}{9}{\textbf {A sub-process of SGD}. Timesteps index columns; training data index rows. The $5$th datum participates in the $2$nd SGD update. This {\color {spacetimepurple}$(n=5,t=2)$ event} affects the testing loss both directly and via the {\color {spacetimeteal}$(1,12)$ event}, which is itself modulated by the {\color {spacetimeindigo}$(2,5)$ event}. \relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsection}{Taylor series: method and challenges}{9}{section*.33}\protected@file@percent }
\newlabel{sect:challenges}{{}{9}{Taylor series: method and challenges}{section*.33}{}}
\@writefile{toc}{\contentsline {subparagraph}{What happens when we keep higher order terms?}{9}{section*.34}\protected@file@percent }
\citation{ne04,ro18}
\bibdata{perturb}
\bibcite{ab07}{{1}{2007}{{Absil, Mahony, and Sepulchre}}{{}}}
\bibcite{am98}{{2}{1998}{{Amari}}{{}}}
\bibcite{ba21}{{3}{2021}{{Barrett and Dherin}}{{}}}
\bibcite{ba17}{{4}{2017}{{Bartlett, Foster, and Telgarsky}}{{}}}
\bibcite{be19}{{5}{2019}{{Belkin et~al.}}{{Belkin, Hsu, Ma, and Mandal}}}
\bibcite{bo91}{{6}{1991}{{Bottou}}{{}}}
\bibcite{ca47}{{7}{1847}{{Cauchy}}{{}}}
\FN@pp@footnote@aux{11}{10}
\newlabel{sect:using}{{}{10}{What happens when we keep higher order terms?}{section*.34}{}}
\newlabel{sect:diagrams}{{}{10}{What happens when we keep higher order terms?}{section*.34}{}}
\@writefile{toc}{\contentsline {subparagraph}{Structure of the Taylor expansion}{10}{section*.35}\protected@file@percent }
\newlabel{sect:diagrams-in-brief}{{}{10}{Structure of the Taylor expansion}{section*.35}{}}
\bibcite{ch18}{{8}{2018}{{Chaudhari and Soatto}}{{}}}
\bibcite{ch87}{{9}{1787}{{Chladni}}{{}}}
\bibcite{di17}{{10}{2017}{{Dinh et~al.}}{{Dinh, Pascanu, Bengio, and Bengio}}}
\bibcite{di18}{{11}{2018}{{Dixon and Ward}}{{}}}
\bibcite{dy19}{{12}{2019}{{Dyer and Gur-Ari}}{{}}}
\bibcite{dy49a}{{13}{1949}{{Dyson}}{{}}}
\bibcite{fi17}{{14}{2017}{{Finn, Abbeel, and Levine}}{{}}}
\bibcite{ga23}{{15}{1823}{{Gauss}}{{}}}
\bibcite{go18}{{16}{2018}{{Goyal et~al.}}{{Goyal, Doll\'{a}r, Girshick, Noordhuis, Wesolowski, Kyrola, Tulloch, Jia, and He}}}
\bibcite{ho17}{{17}{2017}{{Hoffer, Hubara, and Soudry}}{{}}}
\bibcite{ke17}{{18}{2017}{{Keskar et~al.}}{{Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang}}}
\bibcite{ki52}{{19}{1952}{{Kiefer and Wolfowitz}}{{}}}
\bibcite{kr09}{{20}{2009}{{Krizhevsky}}{{}}}
\bibcite{la51}{{21}{1951}{{Landau and Lifshitz}}{{}}}
\bibcite{la60}{{22}{1960}{{Landau and Lifshitz}}{{}}}
\bibcite{le15}{{23}{2015}{{LeCun, Bengio, and Hinton}}{{}}}
\bibcite{li17}{{24}{2017}{{Li, Tai, and E}}{{}}}
\bibcite{li18}{{25}{2018}{{Liao et~al.}}{{Liao, Miranda, Banburski, Hidary, and Poggio}}}
\bibcite{cu87}{{26}{1987}{{McCullagh}}{{}}}
\bibcite{me20}{{27}{2020}{{Mei and Montanari}}{{}}}
\bibcite{mi73}{{28}{1973}{{Misner, Thorne, and Wheeler}}{{}}}
\bibcite{mo18b}{{29}{2018}{{Mohri, Rostamizadeh, and Talwalkar}}{{}}}
\bibcite{ne04}{{30}{2004}{{Nesterov}}{{}}}
\bibcite{ne17a}{{31}{2017{a}}{{Neyshabur et~al.}}{{Neyshabur, Bhojanapalli, McAllester, and Srebro}}}
\bibcite{ne17b}{{32}{2017{b}}{{Neyshabur et~al.}}{{Neyshabur, Tomioka, Salakhutdinov, and Srebro}}}
\bibcite{ni17}{{33}{2017}{{Nickel and Kiela}}{{}}}
\bibcite{pa19}{{34}{2019}{{Paszke et~al.}}{{Paszke, Gross, Massa, Lerer, Bradbury, Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala}}}
\bibcite{pe71}{{35}{1971}{{Penrose}}{{}}}
\bibcite{ro51}{{36}{1951}{{Robbins and Monro}}{{}}}
\bibcite{ro18}{{37}{2018}{{Roberts}}{{}}}
\bibcite{ro19}{{38}{2019}{{Roberts}}{{}}}
\bibcite{ro64}{{39}{1964}{{Rota}}{{}}}
\bibcite{st56}{{40}{1956}{{Stein}}{{}}}
\bibcite{wa18}{{41}{2018}{{Wang et~al.}}{{Wang, Keskar, Xiong, and Socher}}}
\bibcite{we19b}{{42}{2019}{{Wei and Schwab}}{{}}}
\bibcite{we74}{{43}{1974}{{Werbos}}{{}}}
\bibcite{wu20}{{44}{2020}{{Wu et~al.}}{{Wu, Hu, Xiong, Huan, Braverman, and Zhu}}}
\bibcite{wu18}{{45}{2018}{{Wu, Ma, and E}}{{}}}
\bibcite{xi17}{{46}{2017}{{Xiao, Rasul, and Vollgraf}}{{}}}
\bibcite{ya19b}{{47}{2019{a}}{{Yaida}}{{}}}
\bibcite{ya19a}{{48}{2019{b}}{{Yaida}}{{}}}
\bibcite{zh17}{{49}{2017}{{Zhang et~al.}}{{Zhang, Bengio, Hardt, Recht, and Vinyals}}}
\bibcite{zh16}{{50}{2016}{{Zhang, Reddi, and Sra}}{{}}}
\bibcite{zh19}{{51}{2019}{{Zhu et~al.}}{{Zhu, Wu, Yu, and Ma}}}
\bibcite{zo20}{{52}{2020}{{Zou et~al.}}{{Zou, Cao, Zhou, and Gu}}}
\FN@pp@footnotehinttrue 
\@writefile{toc}{\contentsline {section}{Tutorial: how to use diagrams}{13}{section*.38}\protected@file@percent }
\newlabel{appendix:tutorial}{{}{13}{Tutorial: how to use diagrams}{section*.38}{}}
\@writefile{toc}{\contentsline {subsection}{Two example calculations}{13}{section*.39}\protected@file@percent }
\newlabel{appendix:example}{{}{13}{Two example calculations}{section*.39}{}}
\newlabel{qst:grad}{{1}{13}{Leading order effect of gradients}{quest.1}{}}
\newlabel{qst:multi}{{2}{13}{Leading order effect of epochs}{quest.2}{}}
\FN@pp@footnotehinttrue 
\newlabel{sect:gridss}{{}{14}{Computations: Grids}{section*.40}{}}
\@writefile{toc}{\contentsline {subparagraph}{Computations: Grids}{14}{section*.40}\protected@file@percent }
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\newlabel{sect:exampleembed}{{}{15}{Computations: Embeddings of diagrams into grids}{section*.41}{}}
\@writefile{toc}{\contentsline {subparagraph}{Computations: Embeddings of diagrams into grids}{15}{section*.41}\protected@file@percent }
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\newlabel{sect:example-eval}{{}{16}{Computations: Evaluating each diagram history}{section*.42}{}}
\@writefile{toc}{\contentsline {subparagraph}{Computations: Evaluating each diagram history}{16}{section*.42}\protected@file@percent }
\FN@pp@footnote@aux{12}{16}
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\@writefile{toc}{\contentsline {subparagraph}{Computations: Summing the histories' values}{17}{section*.43}\protected@file@percent }
\FN@pp@footnotehinttrue 
\@writefile{toc}{\contentsline {subsection}{How to identify the relevant grid}{18}{section*.44}\protected@file@percent }
\newlabel{appendix:draw-spacetime}{{}{18}{How to identify the relevant grid}{section*.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  \textbf  {The grids of two SGD variants.} Shaded cells show $(n,t)$ pairs (see text). \textbf  {Left}: Two epoch SGD with batch size one. \textbf  {Right}: Four epoch SGD with batch size two. \relax }}{18}{figure.caption.45}\protected@file@percent }
\newlabel{fig:spacetimes}{{10}{18}{\textbf {The grids of two SGD variants.} Shaded cells show $(n,t)$ pairs (see text). \textbf {Left}: Two epoch SGD with batch size one. \textbf {Right}: Four epoch SGD with batch size two. \relax }{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsection}{How to identify the relevant diagram histories}{18}{section*.47}\protected@file@percent }
\newlabel{appendix:draw-histories}{{}{18}{How to identify the relevant diagram histories}{section*.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  Embeddings, legal and illegal. \textbf  {Left}: illegal history of $\sizeddia {c(0-1-2)(01-12)}{0.10}$, since the time-ordering condition is not obeyed. For the same reason, not a legal history of $\sizeddia {c(01-2)(01-12)}{0.10}$. \textbf  {Middle}: an history of $\sizeddia {c(0-1-2)(01-12)}{0.10}$. Also an history of $\sizeddia {c(01-2)(01-12)}{0.10}$, since the correlation condition is obeyed. \textbf  {Right}: a legal history of $\sizeddia {c(0-1-2)(01-12)}{0.10}$. Not an history of $\sizeddia {c(01-2)(01-12)}{0.10}$, since the correlation condition is not obeyed. \relax }}{19}{figure.caption.48}\protected@file@percent }
\newlabel{fig:histories}{{11}{19}{Embeddings, legal and illegal. \textbf {Left}: illegal history of $\sdia {c(0-1-2)(01-12)}$, since the time-ordering condition is not obeyed. For the same reason, not a legal history of $\sdia {c(01-2)(01-12)}$. \textbf {Middle}: an history of $\sdia {c(0-1-2)(01-12)}$. Also an history of $\sdia {c(01-2)(01-12)}$, since the correlation condition is obeyed. \textbf {Right}: a legal history of $\sdia {c(0-1-2)(01-12)}$. Not an history of $\sdia {c(01-2)(01-12)}$, since the correlation condition is not obeyed. \relax }{figure.caption.48}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  \textbf  {Multiple ways to draw the $6$ distinct degree-$3$ diagrams for $B=E=1$ SGD's testing loss.} Because the grid of $B=E=1$ SGD has only one cell per row and one cell per column, the only diagrams that have a non-zero number of histories are the diagrams such that each ancestor-descendant pair in the rooted tree occupie two different parts of the partition. We show $(4+2)+(2+2+3)+(1)$ ways to draw the $6$ diagrams. In fact, these drawings show all of the time-orderings of the diagrams' nodes that are consistent with the time-ordering condition. \textbf  {Organization}: We organize the diagrams into columns by the number of parts in their partitions. Because partitions (fuzzy outlines) indicate correlations between nodes (i.e. noise), diagrams with fuzzy outlines show deviations of SGD away from deterministic ODE. The big-$\Theta $ notation that heads the columns gives the asymptotics of the sum-over-histories of each diagram's $\text  {\textnormal  {uvalue}}$s (for $N$ large and $\eta $ small even relative to $1/N$). {\bf  Left}: Diagrams for ODE behavior. {\bf  Center}: $1$st order deviation of SGD away from ODE. {\bf  Right}: $2$nd order deviation of SGD from ODE with appearance of non-Gaussian statistics. \relax }}{19}{table.caption.49}\protected@file@percent }
\newlabel{tab:scatthree}{{1}{19}{\textbf {Multiple ways to draw the $6$ distinct degree-$3$ diagrams for $B=E=1$ SGD's testing loss.} Because the grid of $B=E=1$ SGD has only one cell per row and one cell per column, the only diagrams that have a non-zero number of histories are the diagrams such that each ancestor-descendant pair in the rooted tree occupie two different parts of the partition. We show $(4+2)+(2+2+3)+(1)$ ways to draw the $6$ diagrams. In fact, these drawings show all of the time-orderings of the diagrams' nodes that are consistent with the time-ordering condition. \textbf {Organization}: We organize the diagrams into columns by the number of parts in their partitions. Because partitions (fuzzy outlines) indicate correlations between nodes (i.e. noise), diagrams with fuzzy outlines show deviations of SGD away from deterministic ODE. The big-$\Theta $ notation that heads the columns gives the asymptotics of the sum-over-histories of each diagram's $\uvalue $s (for $N$ large and $\eta $ small even relative to $1/N$). {\bf Left}: Diagrams for ODE behavior. {\bf Center}: $1$st order deviation of SGD away from ODE. {\bf Right}: $2$nd order deviation of SGD from ODE with appearance of non-Gaussian statistics. \relax }{table.caption.49}{}}
\@writefile{toc}{\contentsline {subsection}{How to evaluate each history}{19}{section*.50}\protected@file@percent }
\newlabel{appendix:evaluate-histories}{{}{19}{How to evaluate each history}{section*.50}{}}
\citation{ro64}
\@writefile{toc}{\contentsline {subparagraph}{Un-resummed values: $\text  {\textnormal  {uvalue}}(D)$}{20}{section*.51}\protected@file@percent }
\FN@pp@footnote@aux{13}{20}
\newlabel{exm:unresum}{{2}{21}{Un-resummed value}{exm.2}{}}
\@writefile{toc}{\contentsline {subparagraph}{Resummed values: $\text  {\textnormal  {rvalue}}_f(D)$}{21}{section*.52}\protected@file@percent }
\newlabel{exm:resum}{{3}{21}{Re-summed value}{exm.3}{}}
\@writefile{toc}{\contentsline {subparagraph}{Overall}{21}{section*.53}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{How to sum the histories' values}{21}{section*.54}\protected@file@percent }
\newlabel{appendix:sum-histories}{{}{21}{How to sum the histories' values}{section*.54}{}}
\@writefile{toc}{\contentsline {subsection}{How to solve variant problems}{22}{section*.55}\protected@file@percent }
\newlabel{appendix:solve-variants}{{}{22}{How to solve variant problems}{section*.55}{}}
\@writefile{toc}{\contentsline {subsection}{Do diagrams streamline computation?}{23}{section*.59}\protected@file@percent }
\newlabel{appendix:diagrams-streamline}{{}{23}{Do diagrams streamline computation?}{section*.59}{}}
\FN@pp@footnote@aux{14}{23}
\@writefile{toc}{\contentsline {subparagraph}{Effect of batch size}{23}{section*.60}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Effect of non-Gaussian noise at a minimum.}{25}{section*.61}\protected@file@percent }
\citation{dy49a}
\@writefile{toc}{\contentsline {section}{Mathematics of the theory}{27}{section*.62}\protected@file@percent }
\newlabel{appendix:math}{{}{27}{Mathematics of the theory}{section*.62}{}}
\@writefile{toc}{\contentsline {subsection}{Assumptions and Definitions}{27}{section*.63}\protected@file@percent }
\newlabel{appendix:assumptions}{{}{27}{Assumptions and Definitions}{section*.63}{}}
\FN@pp@footnote@aux{15}{27}
\newlabel{dfn:diagrams}{{6}{27}{Diagrams}{dfn.6}{}}
\@writefile{toc}{\contentsline {subsection}{A key lemma \`a la Dyson}{27}{section*.64}\protected@file@percent }
\newlabel{appendix:key-lemma}{{}{27}{A key lemma \`a la Dyson}{section*.64}{}}
\newlabel{lem:dyson}{{}{27}{}{section*.64}{}}
\newlabel{eq:dyson2}{{1}{27}{}{equation.Alph0.1}{}}
\newlabel{eq:smalleta}{{2}{27}{A key lemma \`a la Dyson}{equation.Alph0.2}{}}
\FN@pp@footnote@aux{16}{27}
\newlabel{eq:expansion}{{3}{28}{A key lemma \`a la Dyson}{equation.Alph0.3}{}}
\@writefile{toc}{\contentsline {subsection}{From Dyson to diagrams}{28}{section*.65}\protected@file@percent }
\newlabel{appendix:toward-diagrams}{{}{28}{From Dyson to diagrams}{section*.65}{}}
\newlabel{thm:pathint}{{3}{28}{Test Loss as a Path Integral}{thm.3}{}}
\newlabel{eq:sgdcoef}{{3}{28}{Test Loss as a Path Integral}{thm.3}{}}
\newlabel{eq:toprove}{{4}{29}{From Dyson to diagrams}{equation.Alph0.4}{}}
\newlabel{rmk:leibniz}{{4}{29}{Differentiating Products}{rmk.4}{}}
\newlabel{rmk:leibniz}{{}{29}{From Dyson to diagrams}{rmk.4}{}}
\citation{ro64}
\newlabel{prop:vanilla}{{1}{30}{}{prop.1}{}}
\newlabel{eq:sgdbasiccoef}{{1}{30}{}{prop.1}{}}
\@writefile{toc}{\contentsline {subsection}{Proof of Theorem \ref  {thm:resum}}{30}{section*.66}\protected@file@percent }
\newlabel{appendix:resum}{{}{30}{Proof of Theorem \ref {thm:resum}}{section*.66}{}}
\newlabel{eq:hard}{{5}{30}{Proof of Theorem \ref {thm:resum}}{equation.Alph0.5}{}}
\@writefile{toc}{\contentsline {subsection}{Proof of Theorem \ref  {thm:converge}}{31}{section*.67}\protected@file@percent }
\newlabel{appendix:converge}{{}{31}{Proof of Theorem \ref {thm:converge}}{section*.67}{}}
\@writefile{toc}{\contentsline {subsection}{Proofs of corollaries}{31}{section*.68}\protected@file@percent }
\newlabel{appendix:corollaries}{{}{31}{Proofs of corollaries}{section*.68}{}}
\@writefile{toc}{\contentsline {subparagraph}{Corollary \ref  {cor:entropic}}{31}{section*.69}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Corollary \ref  {cor:overfit}'s first part}{31}{section*.70}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Corollary \ref  {cor:overfit}'s second part}{31}{section*.71}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Corollaries \ref  {cor:epochs} and \ref  {cor:batch}}{31}{section*.72}\protected@file@percent }
\newlabel{prop:ordtwo}{{2}{31}{}{prop.2}{}}
\citation{la60,la51}
\citation{ab07,zh16}
\citation{am98}
\citation{ni17}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Terms used in proof of Prop \ref  {prop:ordtwo}\relax }}{32}{table.caption.73}\protected@file@percent }
\newlabel{tbl:ordtwo}{{2}{32}{Terms used in proof of Prop \ref {prop:ordtwo}\relax }{table.caption.73}{}}
\@writefile{toc}{\contentsline {subparagraph}{Corollary \ref  {cor:vsode}}{32}{section*.74}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Future topics}{32}{section*.75}\protected@file@percent }
\newlabel{appendix:future}{{}{32}{Future topics}{section*.75}{}}
\FN@pp@footnote@aux{17}{32}
\FN@pp@footnote@aux{18}{32}
\citation{am98}
\citation{kr09}
\citation{xi17}
\@writefile{toc}{\contentsline {section}{Experimental methods}{33}{section*.76}\protected@file@percent }
\newlabel{appendix:experiments}{{}{33}{Experimental methods}{section*.76}{}}
\@writefile{toc}{\contentsline {subsection}{What artificial landscapes did we use?}{33}{section*.77}\protected@file@percent }
\newlabel{appendix:artificial}{{}{33}{What artificial landscapes did we use?}{section*.77}{}}
\@writefile{toc}{\contentsline {subparagraph}{\textsc  {Gauss}}{33}{section*.78}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\textsc  {Helix}}{33}{section*.79}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\textsc  {Mean Estimation}}{33}{section*.80}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{What image-classification landscapes did we use?}{33}{section*.81}\protected@file@percent }
\newlabel{appendix:natural}{{}{33}{What image-classification landscapes did we use?}{section*.81}{}}
\@writefile{toc}{\contentsline {subparagraph}{Architectures}{33}{section*.82}\protected@file@percent }
\citation{pa19}
\citation{ga23}
\@writefile{toc}{\contentsline {subparagraph}{Datasets}{34}{section*.83}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Measurement process}{34}{section*.84}\protected@file@percent }
\newlabel{appendix:measure}{{}{34}{Measurement process}{section*.84}{}}
\@writefile{toc}{\contentsline {subparagraph}{Diagram evaluation on real landscapes}{34}{section*.85}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Descent simulations}{34}{section*.86}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Implementing optimizers}{34}{section*.87}\protected@file@percent }
\newlabel{appendix:optimizers}{{}{34}{Implementing optimizers}{section*.87}{}}
\@writefile{toc}{\contentsline {subsection}{Software frameworks and hardware}{34}{section*.88}\protected@file@percent }
\newlabel{appendix:frameworks}{{}{34}{Software frameworks and hardware}{section*.88}{}}
\@writefile{toc}{\contentsline {subsection}{Unbiased estimators of landscape statistics}{34}{section*.89}\protected@file@percent }
\newlabel{appendix:bessel}{{}{34}{Unbiased estimators of landscape statistics}{section*.89}{}}
\newlabel{eq:bessel}{{6}{34}{Unbiased estimators of landscape statistics}{equation.Alph0.6}{}}
\citation{di18}
\citation{di18}
\newlabel{eq:newbessel}{{7}{35}{Unbiased estimators of landscape statistics}{equation.Alph0.7}{}}
\@writefile{toc}{\contentsline {subsection}{Additional figures}{35}{section*.90}\protected@file@percent }
\newlabel{appendix:figures}{{}{35}{Additional figures}{section*.90}{}}
\FN@pp@footnotehinttrue 
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces  {\bf  Experiments on natural and artificial landscapes.} \texttt  {rvalue} refers to Theorem \ref  {thm:resum}'s predictions, approximated as in Remark \ref  {rmk:integrate}. \texttt  {uvalue}s are simpler but (see\ofsix {4}) less accurate. \newline  {\bf  Left: Perturbation models SGD for small $\eta T$.} Fashion-MNIST convnet's testing loss vs learning rate. In this small $T$ setting, we choose to use our theory's simpler un-resummed values (\ref  {appendix:evaluate-histories}) instead of the more precise $\text  {\textnormal  {rvalue}}$s. \ofsix {0}: For all initializations tested ($1$ shown, $11$ unshown), the order $3$ prediction agrees with experiment through $\eta T \approx 10^0$, corresponding to a decrease in $0\mbox  {-}1$ error of $\approx 10^{-3}$. \ofsix {1}: For large $\eta T$, our predictions break down. Here, the order $3$ prediction holds until the $0\mbox  {-}1$ error improves by $5\cdot 10^{-3}$. Beyond this, $2$nd order agreement with experiment is coincidental. \newline  {\bf  Center: $C$ controls generalization gap.}With equal-scaled axes, \ofsix {2} shows that GDC matches SGD (small vertical variance) better than GD matches SGD (large horizontal variance) in testing loss for a range of $\eta $ ($\approx 10^{-3}-10^{-1}$) and initializations (zero and several Xavier-Glorot trials) for logistic regression and convnets. Here, $T=10$. \ofsix {3}: CIFAR-10 generalization gaps. For all initializations tested ($1$ shown, $11$ unshown), the degree-$2$ prediction agrees with experiment through $\eta T \approx 5\cdot 10^{-1}$. \newline  {\bf  Right: Predictions near minima excel for large $\eta T$.} \ofsix {4}: SGD with $2, 3, 5, 8$ epochs incurs greater test loss on Fashiion-MNIST than one-epoch SGD (difference shown in I bars) by the predicted amounts (predictions shaded) for a range of learning rates. Here, all SGD runs have $N=10$; we scale the learning rate for $E$-epoch SGD by $1/E$ to isolate the effect of inter-epoch correlations away from the effect of larger $\eta T$. \ofsix {5}: For \textsc  {Mean Estimation}\tmspace  +\thinmuskip {.1667em} with fixed $C$ and a range of $H$s, initialized at the truth, the testing losses after fixed-$T$ GD are smallest for very sharp and very flat $H$. Near $H=0$, our predictions improve on AIC,TIC \citep  {di18}. \relax }}{36}{figure.caption.91}\protected@file@percent }
\newlabel{fig:vanilla}{{12}{36}{{\bf Experiments on natural and artificial landscapes.} \texttt {rvalue} refers to Theorem \ref {thm:resum}'s predictions, approximated as in Remark \ref {rmk:integrate}. \texttt {uvalue}s are simpler but (see\protect \ofsix {4}) less accurate. \newline {\bf Left: Perturbation models SGD for small $\eta T$.} Fashion-MNIST convnet's testing loss vs learning rate. In this small $T$ setting, we choose to use our theory's simpler un-resummed values (\ref {appendix:evaluate-histories}) instead of the more precise $\rvalue $s. \protect \ofsix {0}: For all initializations tested ($1$ shown, $11$ unshown), the order $3$ prediction agrees with experiment through $\eta T \approx 10^0$, corresponding to a decrease in $0\mbox {-}1$ error of $\approx 10^{-3}$. \protect \ofsix {1}: For large $\eta T$, our predictions break down. Here, the order $3$ prediction holds until the $0\mbox {-}1$ error improves by $5\cdot 10^{-3}$. Beyond this, $2$nd order agreement with experiment is coincidental. \newline {\bf Center: $C$ controls generalization gap.}With equal-scaled axes, \protect \ofsix {2} shows that GDC matches SGD (small vertical variance) better than GD matches SGD (large horizontal variance) in testing loss for a range of $\eta $ ($\approx 10^{-3}-10^{-1}$) and initializations (zero and several Xavier-Glorot trials) for logistic regression and convnets. Here, $T=10$. \protect \ofsix {3}: CIFAR-10 generalization gaps. For all initializations tested ($1$ shown, $11$ unshown), the degree-$2$ prediction agrees with experiment through $\eta T \approx 5\cdot 10^{-1}$. \newline {\bf Right: Predictions near minima excel for large $\eta T$.} \protect \ofsix {4}: SGD with $2, 3, 5, 8$ epochs incurs greater test loss on Fashiion-MNIST than one-epoch SGD (difference shown in I bars) by the predicted amounts (predictions shaded) for a range of learning rates. Here, all SGD runs have $N=10$; we scale the learning rate for $E$-epoch SGD by $1/E$ to isolate the effect of inter-epoch correlations away from the effect of larger $\eta T$. \protect \ofsix {5}: For \MeanEstimation \, with fixed $C$ and a range of $H$s, initialized at the truth, the testing losses after fixed-$T$ GD are smallest for very sharp and very flat $H$. Near $H=0$, our predictions improve on AIC,TIC \citep {di18}. \relax }{figure.caption.91}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  \textbf  {Further experimental results}. \textbf  {Left}: SGD with $2, 3, 5, 8$ epochs incurs greater test loss than one-epoch SGD (difference shown in I bars) by the predicted amounts (predictions shaded) for a range of learning rates. Here, all SGD runs have $N=10$; we scale the learning rate for $E$-epoch SGD by $1/E$ to isolate the effect of inter-epoch correlations away from the effect of larger $\eta T$. \textbf  {Center}: SGD's difference from SDE after $\eta T \approx 10^{-1}$ with maximal coarseness on \textsc  {Gauss}. Two effects not modeled by SDE --- time-discretization and non-Gaussian noise oppose on this landscape but do not completely cancel. Our theory approximates the above curve with a correct sign and order of magnitude; we expect that the fourth order corrections would improve it further. \textbf  {Right}: Blue intervals show regularization using Corollary \ref  {cor:overfit}. When the blue intervals fall below the black bar, this proposed method outperforms plain GD. For \textsc  {Mean Estimation}\ with fixed $C$ and a range of $H$s, initialized a fixed distance \emph  {away} from the true minimum, descent on an $l_2$ penalty coefficient $\lambda $ improves on plain GD for most Hessians. The new method does not always outperform GD, because $\lambda $ is not perfectly tuned according to STIC but instead descended on for finite $\eta T$. \relax }}{37}{figure.caption.92}\protected@file@percent }
\newlabel{fig:takreg}{{13}{37}{\textbf {Further experimental results}. \textbf {Left}: SGD with $2, 3, 5, 8$ epochs incurs greater test loss than one-epoch SGD (difference shown in I bars) by the predicted amounts (predictions shaded) for a range of learning rates. Here, all SGD runs have $N=10$; we scale the learning rate for $E$-epoch SGD by $1/E$ to isolate the effect of inter-epoch correlations away from the effect of larger $\eta T$. \textbf {Center}: SGD's difference from SDE after $\eta T \approx 10^{-1}$ with maximal coarseness on \Gauss . Two effects not modeled by SDE --- time-discretization and non-Gaussian noise oppose on this landscape but do not completely cancel. Our theory approximates the above curve with a correct sign and order of magnitude; we expect that the fourth order corrections would improve it further. \textbf {Right}: Blue intervals show regularization using Corollary \ref {cor:overfit}. When the blue intervals fall below the black bar, this proposed method outperforms plain GD. For \MeanEstimation \ with fixed $C$ and a range of $H$s, initialized a fixed distance \emph {away} from the true minimum, descent on an $l_2$ penalty coefficient $\lambda $ improves on plain GD for most Hessians. The new method does not always outperform GD, because $\lambda $ is not perfectly tuned according to STIC but instead descended on for finite $\eta T$. \relax }{figure.caption.92}{}}
