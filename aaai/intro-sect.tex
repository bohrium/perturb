%-----  object of study  ------------------------------------------------------

  Gradient estimates, measured on minibatches and thus noisy, form the primary
learning signal when training deep neural nets.  While users of deep learning
benefit from the intuition that such \emph{stochastic gradient descent} (SGD)
approximates deterministic descent (GD) \citep{bo91,le15}, SGD's
gradient noise in practice alters training dynamics and testing losses
\citep{go18,wu20}.  We develop a general theory of SGD on short timescales or
near minima.  As an application of this theory, we show that \textbf{gradient
noise biases learning} toward low-curvature, low-noise regions of the loss
landscape.

  Specifically, we study the expectation over training sets of the testing loss
by Taylor expanding that loss in the learning rate $\eta$.  By
induction on $T$, the loss decrease due to $T$ steps of training is 
$-T G\eta G + o(\eta)$, where $G=\expc_x[\nabla l_x(\theta_0)]$ is the
expectation over testing samples $x$ of the loss gradient at initialization.
%%%This matches the intuition that if the gradient doubles, then training
%%%displaces the weight twice as far and each unit displacement corresponds to
%%%twice the loss decrease, for an overall quadrupling of loss-decrease.
%
This estimate is exact for loss landscapes that are deterministic and linear,
that is, when $\nabla l_x(\theta)$ depends neither on $x$ nor on $\theta$.
We derive the corrections to that estimate due to noise and curvature.

  A Taylor series analysis of SGD presents three technical challenges.
%
\emph{First}, the variety of resulting terms explodes combinatorially even for
the main correction to $-T G\eta G$; indeed, that correction represents the
diverse ways that some past update may affects a future weight $\theta_t$ and
thus a future update involving $\nabla l_{x_t}(\theta_t)$.  So our
analysis must describe \textbf{interactions} between updates.
%
\emph{Second}, the SGD's gradient noise is correlated between timesteps.  For
example, the same training sample reappears in each epoch.  Whereas the
aforementioned induction on $T$ had an induction hypothesis of the same form
as its conclusion, to analyze \textbf{finite-sample} effects
we must extend the induction hypothesis to keep track of joint moments.  
%
\emph{Third}, the $d$th order truncation of the exact Taylor expansion \textbf{diverges}
as $T$ grows.  A simple qualitative argument shows why this must be: on typical
landscapes such as least squares linear regression, the loss grows
exponentially with time for any negative $\eta$.  We thus do not expect a
Taylor trunction that --- even when restricted to $\eta$ in a neighborhood of
$0$ --- has error uniform in $T$. 

  Our theory addresses all three challenges.  The key idea is
a new diagram-based notation to organize and evaluate many terms at once.  We
physically interpret SGD as a superposition of many concurrent weight-data
interactions, each depicted by a diagram.  We discuss how diagrams clarify
the effect of correlated noise.  We show how to balance each term in
a $d$th order truncation by a collection of higher
order terms, thus taming the large-$T$ divergence.\footnote{%
  Analogous uses of geometric series appear in condensed physics'
  \emph{ladder summation} and in programming languages' \emph{rational data
  types}
}
In contrast to direct truncation, these `\textbf{re-summed}' expressions have
vanishing large-$T$ error for quadratic landscapes (with correlated,
potentially non-Gaussian gradient noise) and they have provably finite and
empirically small errors for convolutional landscapes on CIFAR-10 and
Fashion-MNIST.
 
%-----  soft benefits: retrospective  -----------------------------------------


