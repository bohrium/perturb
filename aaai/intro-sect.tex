  Gradient estimates, measured on minibatches and thus noisy, form the primary
learning signal in deep learning.  While users of deep learning
benefit from the intuition that \emph{stochastic gradient descent} (SGD)
approximates deterministic descent (GD) \citep{bo91,le15}, SGD's gradient noise
%in practice
alters training dynamics and testing losses \citep{go18,wu20}.  We
analyze these dynamics on short timescales or near minima.  We apply
our theory to find that \textbf{gradient noise biases
learning} toward low-curvature, low-noise regions of the loss landscape.

  Specifically, we study the expectation $\expc[l(\theta_T)]$ over training
sets of the post-training testing loss by Taylor expanding that loss in the
learning rate $\eta$.  By induction on $T$, the loss changes by $-T G\eta G +
o(\eta)$ after $T$ training steps, where $G=\expc_x[\nabla l_x(\theta_0)]$ is
the expected gradient at initialization.
%the expectation over testing samples $x$ of the gradient at initialization.
%
This estimate is exact when $\nabla l_x(\theta)$ depends on neither $x$ nor
$\theta$, i.e., for deterministic linear loss landscapes.  We compute how noise
and curvature correct this estimate.

  A Taylor series analysis of SGD presents three challenges.
%
\emph{First}, the terms explode in variety.  Even the main correction to $-T
G\eta G$ represents the diverse ways that some past update may affect a future
weight $\theta_t$ and thus a future update involving $\nabla
l_{x_t}(\theta_t)$.  That is, updates \textbf{interact}.
%
\emph{Second}, SGD's gradient noise is correlated between timesteps: the same
training sample reappears in each epoch.  Such \textbf{finite-sample} effects
complicate the simple induction that led to $-T G\eta G$.
%
\emph{Third}, the series' $d$th order truncation \textbf{diverges} as $T$
grows.  E.g.\ for linear least squares, the loss
grows exponentially with time for $\eta<0$; so on no neighorhood of $\eta=0$
does a $d$th Taylor truncation suffer an error uniform in $T$. 

  We address the three challenges by using diagrams such as
$\sdia{c(0-1)(01)}$, $\sdia{c(01-2)(02-12)}$, $\sdia{c(0-1-2)(02-12)}$ to
organize and evaluate many Taylor terms at once.   
%
%Roughly, the analytical procedure we develop is this.
We interpret each diagram as depicting a class of interactions or
``\textbf{histories}'' between updates.
%
E.g.\ $\sdia{c(01-2-3)(02-12-23)}$ depicts an update's (red's) double effect on
a future update (green) that in turn affects the testing loss (blue).  The
rightmost (``root'') node always represents a post-training measurement.
%
We show how to compute each diagram's effect and we show that
small diagrams dominate the testing loss.
%
That is: up to $o(\eta^d)$ error, \emph{the testing loss is a sum --- over all
histories of all diagrams with $\leq d$ edges --- of certain diagram-dependent
tensor expressions.} 

  More formally, each $d$-edged diagram represents multiple $O(\eta^d)$ terms
in $\expc[l(\theta_T)]$'s Taylor series.  E.g.\ $\sdia{c(0-1)(01)}$'s terms sum
to $-TG\eta G$, which diverges as $T$ grows.  However, we find that each
diagram's (e.g.\ $\sdia{c(0-1)(01)}$'s) divergence is countered by those of
higher-order topologically related diagrams (e.g.\ $\sdia{c(0-1-2)(01-12)}$,
$\sdia{c(0-1-2-3)(01-12-23)}, \cdots$).
%
%%We thus modify plain truncation to tame the large-$T$ divergence.
%
These `\textbf{re-summed}' expressions' errors
are uniform in $T$ for quadratic landscapes (with 
%potentially
non-Gaussian gradient noise) %---
and are provably finite and
empirically small for convolutional landscapes. %on CIFAR-10.%and
%Fashion-MNIST.



%%  Each diagram
%%represents a kind of process that might `happen' during training in a number of
%%ways.  Diagrams evaluate to tensor expressions.  Summing diagram values over
%%all possible diagrams and histories gives the expected testing loss.
%%Considering only diagrams with $d$ or fewer edges leads to $o(\eta^d)$ error. 

%%E.g.\ $\sdia{c(0-1)(01)}$ depicts some update's (red node's)
%%direct effect on the testing loss (the righmost node).  So $\sdia{c(0-1)(01)}$
%%%can happen in $T$ ways; it
%%``represents $T$ many histories''.
%

%\vfill
%\pagebreak

%-----  soft benefits: retrospective  -----------------------------------------



%%%%%Each diagram contributes a sum over its `histories', where a history is
%%%%%an assignment of a timestep to each fuzzy group of nodes.  We demand
%%%%%that each  edge's left node temporally precedes its right node and that the
%%%%%rightmost node is assigned timestep $T$.
%%%%%%
%%%%%Each history contributes a tensor expression constructed as follows: for each
%%%%%node touching $d$  edges, write a $d$th derivative of $l_x$.  For
%%%%%each  edge
%%%%%%%spanning $\Delta$ many timesteps,
%%%%%write a $-\eta$.
%%%%%And wrap each fuzzy group of nodes in expectation brackets. 
%%%%%%matrix $-\eta(I - \eta H)^{\Delta-1}$, where $H=\expc[\nabla\nabla_x(\theta_0)]$ is the hessian.
%%%%%Then dot product those factors together per the diagram's
%%%%%topology.\footnote{
%%%%%  Inconsequential technicalities:
%%%%%  we actually use cumulants instead of uncentered expectations.  And
%%%%%  we divide by ``symmetry factors'' such as the
%%%%%  factorial denominators in a Taylor series.
%%%%%}


%%  Since $\sdia{c(0-1)(01)}$ has only one non-rightmost node, it has as many
%%histories as there are timesteps.  And $\sdia{c(0-1)(01)}$'s has two
%%degree-$1$ nodes, each in its own fuzzy group, so each history's contribution
%%$G(-\eta)G$ involves two $1$st derivatives $\expc[\nabla l]=G$.  Since $T$ histories contribute
%%$G(-\eta)G$ each, we have recovered the $-TG\eta G$ from above.    

%%  The same procedure gives us the second order terms.  The diagram
%%$\sdia{c(01-2)(02-12)}$ has $T$ many histories.  The fuzzy group of two
%%degree-$1$ nodes gives a factor $\expc[\nabla l_x \nabla l_x]$, which is the
%%gradient covariance $C$.  The single degree-$2$ node gives a factor
%%$\expc[\nabla \nabla l_x]$, which is the hessian $H$.  So the contribution
%%is $T C\eta^2 H/2!$.


%%\footnote{%
%%  Analogous uses of geometric series appear in condensed physics'
%%  \emph{ladder summation} and in programming languages' \emph{rational data
%%  types}
%%}
%In contrast to plain truncation,

%%%This matches the intuition that if the gradient doubles, then training
%%%displaces the weight twice as far and each unit displacement corresponds to
%%%twice the loss decrease, for an overall quadrupling of loss-decrease.

