  Gradient estimates, measured on minibatches and thus noisy, form the primary
learning signal in deep learning.  While users of deep learning
benefit from the intuition that \emph{stochastic gradient descent} (SGD)
approximates deterministic descent (GD) \citep{bo91,le15}, SGD's gradient noise
%in practice
alters training dynamics and testing losses \citep{go18,wu20}.  We
analyze these dynamics on short timescales or near minima.  We apply
our theory to find that \textbf{gradient noise biases
learning} toward low-curvature, low-noise regions of the loss landscape.

  Specifically, we study \emph{the expectation over training sets of the
testing loss after $T$ updates} by Taylor expanding that loss in the learning
rate $\eta$.  While the leading term is exact for deterministic linear
landscapes, it is higher order terms that quantify the effects of noise and
curvature.
%
It may seem that for small $\eta$ we may neglect higher $O(\eta^d)$ terms.
However, the latter have coefficients that scale like $T^d/d!$, since such
terms intuitively represent the joint effect of the ${T\choose d}$ many
size-$d$ subsets of the %length-$T$
update sequence.   We thus need higher order
terms to analyze SGD for small but finite values of $\eta T$.  

%%  {\color{red}HOW HIGHER ORDER TERMS MATTER: explosion of terms with $T$.
%%simplest nontrivial term?} 

  A Taylor series analysis of SGD presents three challenges.
%
\emph{First}, the terms explode in variety.  The sub-leading terms
represent the diverse ways that some past update may affect a future
weight $\theta_t$ and thus a future update involving $\nabla
l_{x_t}(\theta_t)$.  That is, updates \textbf{interact}.
%
\emph{Second}, SGD's gradient noise is correlated between timesteps: the same
training sample reappears in each epoch.  Such \textbf{finite-sample} effects
complicate inductive analyses because one's induction hypothesis must keep
track of joint moments.%the simple induction-on-$T$ that led to $-T G\eta G$.
%
\emph{Third}, the series' $d$th order truncation \textbf{diverges} as $T$
grows.  E.g.\ for linear least squares, the loss
grows exponentially with time for $\eta<0$; so on no neighorhood of $\eta=0$
does a $d$th Taylor truncation suffer an error uniform in $T$. 

  To address the three challenges, we exploit a graphical representation of
Taylor terms.  These \textbf{diagrams} naturally reflect the combinatorics of
interactions and correlations.
%; each diagram helps to evaluate many terms at once.
Diagrams thus enable the book-keeping necessary to uncover the subtle
phenomena expressed in higher order terms. 

  More formally, each $d$-edged diagram represents multiple $O(\eta^d)$ terms
in the Taylor series.  For instance, a single diagram gives all the leading
order terms.  Though the latter's sum is $O(\eta)$, it diverges as $T$ grows
(c.f. Prop \ref{prop:nest}), intuitively because it does not detect that
learning slows after many updates.  Such divergences plague higher terms, too.
%
In our key technical contribution, we cancel each diagram's divergence against
those of larger, \emph{topologically-related} diagrams.  The graphical notation makes
plain which terms cancel when grouped and what each group's remaining sum is. 
%
These `\textbf{re-summed}' expressions' errors are uniform in $T$ for quadratic
landscapes (with non-Gaussian gradient noise) and are provably finite and
empirically small for convolutional landscapes.

%$\sdia{c(0-1)(01)}$, $\sdia{c(01-2)(02-12)}$, $\sdia{c(0-1-2)(02-12)}$ to
%
%E.g.\ $\sdia{c(01-2-3)(02-12-23)}$ depicts an update's (red's) double effect on
%a future update (green) that in turn affects the testing loss (blue).  The
%rightmost (``root'') node always represents a post-training measurement.
%%
%
%We interpret each diagram as depicting a class of interactions or
%``\textbf{histories}'' between updates.
%%
%We show how to compute each diagram's effect and we show that
%small diagrams dominate the testing loss.
%%
%That is: up to $o(\eta^d)$ error, \emph{the testing loss is a sum --- over all
%histories of all diagrams with $\leq d$ edges --- of certain diagram-dependent
%tensor expressions.} 
%
%
