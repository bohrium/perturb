\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi:\discretionary{}{}{}#1}\else
  \providecommand{\doi}{doi:\discretionary{}{}{}\begingroup
  \urlstyle{rm}\Url}\fi

\bibitem[{Absil, Mahony, and Sepulchre(2007)}]{ab07}
Absil, P.-A.; Mahony, R.; and Sepulchre, R. 2007.
\newblock Optimization Algorithms on Matrix Manifolds, Chapter 4.
\newblock \emph{Princeton University Press} .

\bibitem[{Amari(1998)}]{am98}
Amari, S.-I. 1998.
\newblock Natural Gradient Works Efficiently.
\newblock \emph{Neural Computation} .

\bibitem[{Barrett and Dherin(2021)}]{ba21}
Barrett, D.; and Dherin, B. 2021.
\newblock Implicit Gradient Regularization.
\newblock \emph{ICLR} .

\bibitem[{Bartlett, Foster, and Telgarsky(2017)}]{ba17}
Bartlett, P.; Foster, D.; and Telgarsky, M. 2017.
\newblock Spectrally-Normalized Margin Bounds for Neural Networks.
\newblock \emph{NeurIPS} .

\bibitem[{Belkin et~al.(2019)Belkin, Hsu, Ma, and Mandal}]{be19}
Belkin, M.; Hsu, D.; Ma, S.; and Mandal, S. 2019.
\newblock Reconciling Modern Machine Learning Practice and the Bias-Variance
  Trade-off.
\newblock \emph{PNAS} .

\bibitem[{Bottou(1991)}]{bo91}
Bottou, L. 1991.
\newblock Stochastic Gradient Learning in Neural Networks.
\newblock \emph{Neuro-N\^imes} .

\bibitem[{Cauchy(1847)}]{ca47}
Cauchy, A.-L. 1847.
\newblock M\'ethode g\'en\'erale pour la r\'esolution des syst\'emes
  d'\'equations simultan\'ees.
\newblock \emph{Comptes rendus de l'Acad\'emie des Sciences} .

\bibitem[{Chaudhari and Soatto(2018)}]{ch18}
Chaudhari, P.; and Soatto, S. 2018.
\newblock SGD performs variational inference, converges to limit cycles for
  deep networks.
\newblock \emph{ICLR} .

\bibitem[{Chladni(1787)}]{ch87}
Chladni, E. 1787.
\newblock Entdeckungen \"uber die Theorie des Klanges.
\newblock \emph{Leipzig} .

\bibitem[{Comon(2014)}]{co14}
Comon, P. 2014.
\newblock An Introduction to Tensors.
\newblock \emph{IEEE Signal Processing Magazine} .

\bibitem[{Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio}]{di17}
Dinh, L.; Pascanu, R.; Bengio, S.; and Bengio, Y. 2017.
\newblock Sharp Minima Can Generalize For Deep Nets.
\newblock \emph{ICLR} .

\bibitem[{Dixon and Ward(2018)}]{di18}
Dixon, M.; and Ward, T. 2018.
\newblock Takeuchi Information as a form of Regularization.
\newblock \emph{Arxiv Preprint} .

\bibitem[{Dyer and Gur-Ari(2019)}]{dy19}
Dyer, E.; and Gur-Ari, G. 2019.
\newblock Asymptotics of Wide Networks from Feynman Diagrams.
\newblock \emph{ICML Workshop} .

\bibitem[{Dyson(1949)}]{dy49a}
Dyson, F. 1949.
\newblock The Radiation Theories of Tomonaga, Schwinger, and Feynman.
\newblock \emph{Physical Review} .

\bibitem[{Finn, Abbeel, and Levine(2017)}]{fi17}
Finn, C.; Abbeel, P.; and Levine, S. 2017.
\newblock Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.
\newblock \emph{ICML} .

\bibitem[{Gauss(1823)}]{ga23}
Gauss, C. 1823.
\newblock Theoria Combinationis Obsevationum Erroribus Minimis Obnoxiae,
  section 39.
\newblock \emph{Proceedings of the Royal Society of Gottingen} .

\bibitem[{Goyal et~al.(2018)Goyal, Doll\'{a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He}]{go18}
Goyal, P.; Doll\'{a}r, P.; Girshick, R.; Noordhuis, P.; Wesolowski, L.; Kyrola,
  A.; Tulloch, A.; Jia, Y.; and He, K. 2018.
\newblock Accurate, Large Minibatch SGD.
\newblock \emph{Data @ Scale} .

\bibitem[{Hoffer, Hubara, and Soudry(2017)}]{ho17}
Hoffer, E.; Hubara, I.; and Soudry, D. 2017.
\newblock Train Longer, Generalize Better.
\newblock \emph{NeurIPS} .

\bibitem[{Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang}]{ke17}
Keskar, N.; Mudigere, D.; Nocedal, J.; Smelyanskiy, M.; and Tang, P. 2017.
\newblock On Large-Batch Training for Deep Learning: Generalization Gap and
  Sharp Minima.
\newblock \emph{ICLR} .

\bibitem[{Kiefer and Wolfowitz(1952)}]{ki52}
Kiefer, J.; and Wolfowitz, J. 1952.
\newblock Stochastic Estimation of the Maximum of a Regression Function.
\newblock \emph{Annals of Mathematical Statistics} .

\bibitem[{Kol\'{a}\u{r}, Michor, and Slov\'{a}k(1993)}]{ko93}
Kol\'{a}\u{r}, I.; Michor, P.; and Slov\'{a}k, J. 1993.
\newblock Natural Operations in Differential Geometry.
\newblock \emph{Springer} .

\bibitem[{Krizhevsky(2009)}]{kr09}
Krizhevsky, A. 2009.
\newblock Learning Multiple Layers of Features from Tiny Images.
\newblock \emph{UToronto Thesis} .

\bibitem[{Kunstner, Hennig, and Balles(2019)}]{ku19}
Kunstner, F.; Hennig, P.; and Balles, L. 2019.
\newblock Limitations of the empirical Fisher approximation for natural
  gradient descent.
\newblock \emph{NeurIPS} .

\bibitem[{Landau and Lifshitz(1951)}]{la51}
Landau, L.; and Lifshitz, E. 1951.
\newblock The Classical Theory of Fields.
\newblock \emph{Addison-Wesley} .

\bibitem[{Landau and Lifshitz(1960)}]{la60}
Landau, L.; and Lifshitz, E. 1960.
\newblock Mechanics.
\newblock \emph{Pergamon Press} .

\bibitem[{LeCun, Bengio, and Hinton(2015)}]{le15}
LeCun, Y.; Bengio, Y.; and Hinton, G. 2015.
\newblock Deep Learning.
\newblock \emph{Nature} .

\bibitem[{Li, Tai, and E(2017)}]{li17}
Li, Q.; Tai, C.; and E, W. 2017.
\newblock Stochastic Modified Equations and Adaptive Stochastic Gradient
  Algorithms I.
\newblock \emph{PMLR} .

\bibitem[{Liao et~al.(2018)Liao, Miranda, Banburski, Hidary, and Poggio}]{li18}
Liao, Q.; Miranda, B.; Banburski, A.; Hidary, J.; and Poggio, T. 2018.
\newblock A Surprising Linear Relationship Predicts Test Performance in Deep
  Networks.
\newblock \emph{Center for Brains, Minds, and Machines Memo 91} .

\bibitem[{McCullagh(1987)}]{cu87}
McCullagh, P. 1987.
\newblock Tensor Methods in Statistics.
\newblock \emph{Chemical Rubber Company Press} .

\bibitem[{Mei and Montanari(2020)}]{me20}
Mei, S.; and Montanari, A. 2020.
\newblock The Generalization Error of Random Features Regression.
\newblock \emph{Arxiv Preprint} .

\bibitem[{Misner, Thorne, and Wheeler(1973)}]{mi73}
Misner, C.; Thorne, K.; and Wheeler, J. 1973.
\newblock Gravitation.
\newblock \emph{W.H. Freeman and Company} .

\bibitem[{Mohri, Rostamizadeh, and Talwalkar(2018)}]{mo18b}
Mohri, M.; Rostamizadeh, A.; and Talwalkar, A. 2018.
\newblock Foundations of Machine Learning, Section 6.3.2.
\newblock \emph{MIT Press} .

\bibitem[{Nesterov(2004)}]{ne04}
Nesterov, Y. 2004.
\newblock Lectures on Convex Optimization: Minimization of Smooth Functions.
\newblock \emph{Springer Applied Optimization 87, Section 2.1} .

\bibitem[{Neyshabur et~al.(2017{\natexlab{a}})Neyshabur, Bhojanapalli,
  McAllester, and Srebro}]{ne17a}
Neyshabur, B.; Bhojanapalli, S.; McAllester, D.; and Srebro, N.
  2017{\natexlab{a}}.
\newblock Exploring Generalization in Deep Learning.
\newblock \emph{NeurIPS} .

\bibitem[{Neyshabur et~al.(2017{\natexlab{b}})Neyshabur, Tomioka,
  Salakhutdinov, and Srebro}]{ne17b}
Neyshabur, B.; Tomioka, R.; Salakhutdinov, R.; and Srebro, N.
  2017{\natexlab{b}}.
\newblock Geometry of Optimization and Implicit Regularization in Deep
  Learning.
\newblock \emph{Chapter 4 from Intel CRI-CI: Why and When Deep Learning Works
  Compendium} .

\bibitem[{Nickel and Kiela(2017)}]{ni17}
Nickel, M.; and Kiela, D. 2017.
\newblock Poincar\'e Embeddings for Learning Hierarchical Representations.
\newblock \emph{ICML} .

\bibitem[{Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Killeen,
  Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison, Tejani,
  Chilamkurthy, Steiner, Fang, Bai, and Chintala}]{pa19}
Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Killeen, T.; Lin,
  Z.; Gimelshein, N.; Antiga, L.; Desmaison, A.; Kopf, A.; Yang, E.; DeVito,
  Z.; Raison, M.; Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai, J.;
  and Chintala, S. 2019.
\newblock PyTorch: An Imperative Style, High-Performance Deep Learning Library.
\newblock \emph{NeurIPS} .

\bibitem[{Penrose(1971)}]{pe71}
Penrose, R. 1971.
\newblock Applications of Negative Dimensional Tensors.
\newblock \emph{Combinatorial Mathematics and its Applications} .

\bibitem[{Robbins and Monro(1951)}]{ro51}
Robbins, H.; and Monro, S. 1951.
\newblock A Stochastic Approximation Method.
\newblock \emph{Pages 400-407 of The Annals of Mathematical Statistics.} .

\bibitem[{Roberts(2018)}]{ro18}
Roberts, D. 2018.
\newblock SGD Implicitly Regularizes Generalization Error.
\newblock \emph{NeurIPS: Integration of Deep Learning Theories Workshop} .

\bibitem[{Roberts(2019)}]{ro19}
Roberts, D. 2019.
\newblock SGD.
\newblock \emph{Personal communication} .

\bibitem[{Rota(1964)}]{ro64}
Rota, G.-C. 1964.
\newblock Theory of M\"obius Functions.
\newblock \emph{Zeitschrift f\"ur Wahrscheinlichkeitstheoriei und Verwandte
  Gebiete} .

\bibitem[{Roux, Bengio, and Fitzgibbon(2012)}]{ro12}
Roux, N.; Bengio, Y.; and Fitzgibbon, A. 2012.
\newblock Improving First and Second-Order Methods by Modeling Uncertainty.
\newblock \emph{Book Chapter: Optimization for Machine Learning, Chapter 15} .

\bibitem[{Sidiropoulos et~al.(2017)Sidiropoulos, Lathauwer, Fu, Huang,
  Papalexakis, and Faloutsos}]{si17}
Sidiropoulos, N.; Lathauwer, L.; Fu, X.; Huang, K.; Papalexakis, E.; and
  Faloutsos, C. 2017.
\newblock Tensor Decomposition for Signal Processing and Machine Learning.
\newblock \emph{IEEE Transactions on Signal Processing} .

\bibitem[{Stein(1956)}]{st56}
Stein, C. 1956.
\newblock Inadmissibility of the Usual Estimator for the Mean of a Multivariate
  Normal Distribution.
\newblock \emph{Berkeley Symposium on Mathematical Probability} .

\bibitem[{Wang et~al.(2018)Wang, Keskar, Xiong, and Socher}]{wa18}
Wang, H.; Keskar, N.; Xiong, C.; and Socher, R. 2018.
\newblock Identifying Generalization Properties in Neural Networks.
\newblock \emph{Arxiv Preprint} .

\bibitem[{Wei and Schwab(2019)}]{we19b}
Wei, M.; and Schwab, D. 2019.
\newblock How Noise Affects the Hessian Spectrum in Overparameterized Neural
  Networks.
\newblock \emph{Arxiv Preprint} .

\bibitem[{Werbos(1974)}]{we74}
Werbos, P. 1974.
\newblock Beyond Regression: New Tools for Prediction and Analysis.
\newblock \emph{Harvard Thesis} .

\bibitem[{Wu et~al.(2020)Wu, Hu, Xiong, Huan, Braverman, and Zhu}]{wu20}
Wu, J.; Hu, W.; Xiong, H.; Huan, J.; Braverman, V.; and Zhu, Z. 2020.
\newblock On the Noisy Gradient Descent that Generalizes as SGD.
\newblock \emph{ICML} .

\bibitem[{Wu, Ma, and E(2018)}]{wu18}
Wu, L.; Ma, C.; and E, W. 2018.
\newblock How SGD Selects the Global Minima in Over-Parameterized Learning.
\newblock \emph{NeurIPS} .

\bibitem[{Xiao, Rasul, and Vollgraf(2017)}]{xi17}
Xiao, H.; Rasul, L.; and Vollgraf, R. 2017.
\newblock Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine
  Learning Algorithms.
\newblock \emph{Arxiv Preprint} .

\bibitem[{Yaida(2019{\natexlab{a}})}]{ya19b}
Yaida, S. 2019{\natexlab{a}}.
\newblock A First Law of Thermodynamics for SGD.
\newblock \emph{Personal Communication} .

\bibitem[{Yaida(2019{\natexlab{b}})}]{ya19a}
Yaida, S. 2019{\natexlab{b}}.
\newblock Fluctuation-Dissipation Relations for SGD.
\newblock \emph{ICLR} .

\bibitem[{Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and Vinyals}]{zh17}
Zhang, C.; Bengio, S.; Hardt, M.; Recht, B.; and Vinyals, O. 2017.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{ICLR} .

\bibitem[{Zhang, Reddi, and Sra(2016)}]{zh16}
Zhang, H.; Reddi, S.; and Sra, S. 2016.
\newblock Fast stochastic optimization on Riemannian manifolds.
\newblock \emph{NeurIPS} .

\bibitem[{Zhu et~al.(2019)Zhu, Wu, Yu, and Ma}]{zh19}
Zhu, Z.; Wu, J.; Yu, B.; and Ma, J. 2019.
\newblock The Anisotropic Noise in Stochastic Gradient Descent.
\newblock \emph{ICML} .

\bibitem[{Zou et~al.(2020)Zou, Cao, Zhou, and Gu}]{zo20}
Zou, D.; Cao, Y.; Zhou, D.; and Gu, Q. 2020.
\newblock Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU
  Networks.
\newblock \emph{MLJ} .

\end{thebibliography}
