
        %\subsection{Contributions}

            %-----  summarize contributions  ----------------------------------

            This paper presents a new physics-inspired perspective on SGD.  We
            use diagrams to study stochastic optimization on short timescales
            or near minima.  Corollaries \ref{cor:entropic} and
            \ref{cor:overfit} together show that SGD avoids curvature and
            noise, which to leading order control generalization.

            %%Analyzing $\sdia{c(01-2)(02-12)}$, we found that \textbf{flat and
            %%sharp minima both overfit less} than medium minima.  Intuitively, flat
            %%minima are robust to vector noise, sharp minima are robust to covector
            %%noise, and medium minima robust to neither.  We thus proposed a
            %%regularizer enabling gradient-based hyperparameter tuning.
            %%%
            %%Inspecting $\sdia{c(01-2-3)(02-12-23)}$, we extended \cite{we19b} to
            %%nonconstant, non-isotropic covariance to reveal that \textbf{SGD
            %%descends on a landscape smoothed by the current covariance $C$}.  As
            %%$C$ evolves, the smoothed landscape evolves, resulting in
            %%non-conservative dynamics.
            %%%
            %%%Corollaries \ref{cor:entropic} and
            %%%\ref{cor:overfit} together potentially illuminate SGD's success in
            %%%training deep networks: SGD avoids curvature and noise, which
            %%%control generalization.
            %%%
            %%Examining $\sdia{c(01-2)(01-12)}$, we showed that \textbf{GD may
            %%emulate SGD}, as suggested by \cite{ro18}.  This is significant
            %%because, while small batch sizes can lead to better generalization
            %%\citep{bo91}, modern infrastructure increasingly rewards large
            %%batch sizes \citep{go18}.  

            %-----  anticipate criticism of limitations  ----------------------
    
            Since our predictions depend only on loss data near initialization,
            they break down after the weight moves far from initialization.  Our
            theory thus best applies to small-movement contexts, whether for long
            times (large $\eta T$) near an isolated minimum or for short times
            (small $\eta T$) in general.
            %
            Thus, the theory might aid future analysis of fine-tuners such as 
            \cite{fi17}'s MAML.
    
                Much as meteorologists understand how warm and cold fronts
                interact despite long-term forecasting's intractability, we
                quantify how curvature and noise contribute to
                counter-intuitive dynamics governing each short-term interval
                of SGD's trajectory.  Equipped with our theory, users of deep
                learning may refine intuitions --- e.g.\ that SGD descends on
                the training loss --- to account for noise.

        \subsection{Related work}\label{sect:related}
    
            %--  history of sgd  ----------------------------------------------

            \citet{ki52} united gradient descent \citep{ca47} with stochastic
            approximation \citep{ro51} to invent SGD.  Since the development of
            back-propagation \citep{we74}, SGD
            has been used to train connectionist models, e.g.\ neural networks
            \citep{bo91}, recently to remarkable success \citep{le15}.
            %%
            %--  analyzing overfitting; relevance of optimization; sde errs  --
            %%
            Several research programs treat overfitting of SGD-trained
            networks.  \citep{ne17a}.  \citet{ba17} controls the Rademacher
            complexity of deep hypothesis classes, leading to
            optimizer-agnostic generalization bounds.  Yet SGD-trained networks
            generalize despite their ability to shatter large sets
            \citep{zh17}, so generalization must arise from not only
            architecture but also optimization%'s implicit regularization
            \citep{ne17b}.  

            Some analyses of implicit regularization use a Langevin or SDE
            approximation (e.g.\ \citet{ch18,zh19}), but, per \citet{ya19a},
            such continuous-time or uncorrelated-noise analyses treat SGD noise
            incorrectly.
            %
            %--  we extend dan's approach  ------------------------------------
            %
            We avoid these pitfalls by Taylor expanding around $\eta=0$ as in
            \citet{ro18}.  Unlike that work, we generalize beyond order $\eta^1$
            and $T=2$.  %To do so, we develop new summation techniques with
            %improved large-$T$ convergence.
            Our interpretion of the resulting
            terms offers a new qualitative picture of SGD as a superposition of
            simpler information-flow processes.
            %%
            %--  double descent  ----------------------------------------------
            %%
            Other research focuses on \emph{double descent} and suggests that
            some highly overparameterized models share implicit regularization
            properties with linear least-squares models \citep{be19}, for
            example by bounding log-determinants (and hence the effective
            dimensions) of feature matrices and weight spaces
            \citep{me20}.\footnote{
                \cite{me20}'s eq.\ 75 bounds a log-determinant defined in eq.\
                61 of a transformed feature matrix.  Compare to linear
                Representer Theorems \citep{mo18b}.
            }
            %
            Our work reveals new dynamics toward and within valleys of minima,
            dynamics that may also reduce the effective dimension of model space.
            However, our focus on the structure of gradient noise may be
            overspecific, since recent work finds that GD and SGD may both
            converge to the same global minima \citep{zo20} or that
            noise covariance but not higher moments are relevant to
            regularization \citep{wu20}. 

            %--  phenomenology of rademacher correlates such as hessians  -----
        
            Our predictions are vacuous for large $\eta$.  Other work treats
            large-$\eta$ learning phenomenologically, whether by finding
            empirical correlates of the generalization gap \citep{li18}, by
            showing that \emph{flat} minima generalize \citep{ho17,ke17,wa18},
            or by showing that \emph{sharp} minima generalize
            \citep{st56,di17,wu18}.  SGD's implicit regularization mediates
            between these seemingly clashing intuitions (\S
            \ref{subsect:overfit}).
            
            %--  our work vs other perturbative approaches  -------------------
        
            Prior work analyzes SGD perturbatively: \cite{dy19} perturb in
            inverse network width, using 't Hooft diagrams to correct the
            Gaussian Process approximation for specific nets.  Perturbing
            to order $\eta^2$, \cite{ch18} and \cite{li17} assume uncorrelated
            Gaussian noise while \cite{ba21} compares GD to ODE.
            By contrast, we use Penrose diagrams \cite{pe71} to compute
            testing losses and to compare to ODE and SDE to \emph{arbitrary
            order} in $\eta$.  
            We allow correlated,
            non-Gaussian noise and thus \emph{any} smooth architecture.  E.g.\
            we assume no information-geometric relationships between $C$ and
            $H$,\footnote{
                Disagreement of $C$ and
                $H$ is typical in modern learning \citep{ro12, ku19}
            } so we may model VAEs.  

%\moosect{Locating \cit{Ba} in our theory}
%    \cit{Ba}'s \thm{3.1} computes order-$\eta^2$ weight
%    displacements $\theta_T-\theta_0$ in the noiseless case $l_x=l$.  The
%    relevant diagrams are thus those with $\leq 2$ edges and that contain no
%    gray outlines.
%    %
%    Indeed, noiseless $\implies$ cumulants vanish $\implies$ any diagram that
%    contains one or more gray outlines has a uvalue (and rvalue) equal to
%    zero.  So a sum over diagrams is the same as a sum over gray-free diagrams,
%    i.e., over each diagram whose partition (\pag{5}\dfn{1}) is maximally fine.
%
%    Per \S{A.6}, we use `rootless' diagrams, e.g.\
%    $\mdia{MOOc(0-1)(01-1)}, \mdia{MOOc(0-1-2)(01-12-2)}$.  These diagrams look
%    different from ordinary ones because we are computing weight displacements
%    $\Delta_l \triangleq \EE[\theta_T-\theta_0]$, not test losses
%    $\EE[l(\theta_T)]$.  Of course, in the noiseless case, those expectation
%    symbols are redundant.  Likewise, in the noiseless case $\Delta_l$ is a
%    function only of $\eta, T$ (and of the loss landscape $l$ and the
%    initialization $\theta_0$); in particular, we may set $E,B$ as convenient.
%    Let's set $E=B=1$.
%
%\moosect{GD's displacement}
%    So, we seek rootless gray-free diagrams width $\leq 2$ edges. 
%    $\mdia{MOOc(0)(0)}$ and
%    $\mdia{MOOc(0-1)(01-1)}$ are the only such.
%    %
%    Let's use their uvalues as in \pag{36}\thm{3} to compute $\Delta_l(T,\eta)$.
%    We read off:
%    $$
%    \text{uvalue}(\mdia{MOOc(0)(0)}) = G_\mu \eta^{\mu\nu} = h G 
%    \hspace{1cm}
%    \text{uvalue}(\mdia{MOOc(0-1)(01-1)}) = G_\mu \eta^{\mu\sigma} H_{\sigma\rho} \eta^{\rho\nu} = h^2 (H G)   
%    $$
%    The RHSs of the above concretize to the case that $\eta^{\mu\sigma}$ (in our
%    directionality-aware theory a symmetric bilinear form that takes two covectors and outputs a
%    scalar) is $h$ times the standard dot product and that $G, H$ are represented
%    in standard ways as matrices.
%    %
%    The diagrams embed (into an $E=B=1$ grid that looks
%    like the rightmost grid on \pag{18}) in $T$ and in ${T\choose 2}$ many
%    ways, respectively.\footnote{%
%        An embedding of a rootless diagram (e.g.\ $\mdia{MOOc(0-1)(01-1)}$)
%        assigns \emph{every} node to a grid cell.  \pag{19}
%        decrees that we assign only \emph{non-root} nodes when computing
%        $\EE[l(\theta_T)]$; indeed, the root node
%        represents the test-time factor $l$ and thus 
%        corresponds to no training point or training step.  By contrast,
%        every factor of every term in $\EE[\theta_T-\theta_0]$
%        corresponds to some training point $n$ and training step $t$.
%        So we assign \emph{all} nodes to grid cells.  We'll expand \S{A.6}

            \nopagebreak

