  We illustrate our theory by deriving a flatness-seeking tendency of SGD.
This section assumes one-epoch batch-size-one SGD.  We later 
treat batch and multi-epoch effects.

\subsection{Notation and assumptions, I}\label{sect:setup}
    %--  the landscape  -----------------------------------------------

  We formalize the loss --- suffered by a fixed architecture on a random
datapoint --- as a distribution $\Dd$ over functions from a space $\Mm$ of
weights.  The \emph{testing loss} $l:\Mm\to\RR$ is $\Dd$'s mean.  We write
$\theta\in\Mm$, $l_x\sim\Dd$ for generic elements.
%
We consider training sequences $(l_n: 0\leq n<N) \sim \Dd^N$.  We call
$n$ and $l_n$ \emph{training points}.
%
Each initialization $\theta_0 \in \Mm$ then induces via SGD a
distribution over trajectories $(\theta_t: 0\leq t \leq T)$.  Specifically,
SGD runs $T$ steps of $\eta$-steepest descent:
  \begin{equation*}
    \theta_{t+1}^\mu
    \coloneqq
    \theta_t^\mu -
    \sum_{\nu}
    \eta^{\mu\nu} \nabla_\nu l_{n_t}(\theta_t)
  \end{equation*}
Each sequence $(n_t: kN\leq t<kN+N)$ is a permutation of $(n:
0\leq n<N)$.  Our Greek indices name components of
$\theta,\eta,\nabla$ w.r.t.\ a fixed basis.  We view $\eta$
as a bilinear form
so that the only type-correct expressions have geometric significance.
%

We heavily use the
\textbf{gradient} $G_\mu = \expc[\nb_\mu\lx]$, % \leftrightsquigarrow \sdia{MOO(0)(0)}$,
\textbf{hessian} $H_{\mu\nu} = \expc[\nb_\mu\nb_\nu\lx]$, %  \leftrightsquigarrow \sdia{MOO(0)(0-0)}$,
\textbf{jerk} \squish $J_{\mu\nu\xi} = \expc[\nb_\mu\nb_\nu\nb_\xi\lx]$, %  \leftrightsquigarrow \sdia{MOO(0)(0-0-0)}$,
\textbf{covariance} $C_{\mu\nu} = \expc[(\nb\lx - G)^{\otimes 2}]_{\mu\nu}$, %     \leftrightsquigarrow \sdia{MOOc(01)(0-1)}$, and
and
\textbf{skew} \squash $S_{\mu\nu\xi} = \expc[(\nb\lx - G)^{\otimes 3}]_{\mu\nu\xi}$, %  \leftrightsquigarrow \sdia{MOOc(012)(0-1-2)}$,
typically evaluated at
initialization ($\theta=\theta_0$).
So $G, H, J, C, S$ respectively have $1, 2, 3, 2, 3$ many axes, each of which
transforms under change-of-basis like a covector.\footnote{
    Gradients
    (covectors, dollars-per-mile) and displacements (vectors, miles) have geometrically
    distinct types.  We respect this distinction throughout; that's
    why our $\eta$ is a tensor.
    \cite{mi73} (\S2.5) visualizes this geometry;
    \cite{cu87} (\S1.4) discusses vectors vs covectors in statistics.
}

To illustrate our notation
we quote
\citet{ne04}, \S 2.1:
\begin{prop}\label{prop:nest}
    $G = \nabla l(\theta_0)$ controls the loss to leading order.
    Precisely,
    $
        \expc[l(\theta_T)-l(\theta_0)]^\mu =
        - 
        T \sum_{\mu\nu} G_\mu \eta^{\mu\nu} G_\nu
        + o(\eta^1)
    $.
\end{prop}
%(that
%is, $\nabla l_x(\theta)$'s dependence on $x$ and $\theta$) 
%\noindent Our work identifies how noise and curvature correct Prop
%\ref{prop:nest}.

\subsection{Informal statement of result}
  \begin{dfn}
    \emph{
      A {\textbf{diagram}} is a rooted tree equipped with an equivalence relation
      on (i.e.\ a partition of) its non-root nodes.  Convention: we orient the tree
      left-to-right so that children precede parents; the root is thus rightmost.
      We draw the partition with fuzzy outlines.
      Node {\color{moor!90}colors} lack formal meaning but help us refer
      to diagram parts.
      %
      Valid diagrams include
      $\sdia{c(0-1)(01)}$,
      $\sdia{c(0-1-2)(02-12)}$,
      $\sdia{c(012-3)(02-12-23)}$ 
      %$\sdia{c(0-1-2-3)(01-12-23)}$ 
      but not $\sdia{MOOc(01)(0-1)}$,  
      $\sdia{MOOc(01)(01)}$, 
      $\sdia{MOOc(0-1-2)(01-02-12)}$.
    }\mend 
  \end{dfn}
  %$\sdia{MOOc(03-12-4)(03-13-23-34)}$.
  %%And
  %%$\sdia{c(01-2-3)(01-13-23)}$ is the same diagram as
  %%$\sdia{c(02-1-3)(02-13-23)}$.
  %
  We interpret each diagram as depicting a class of interactions or
  ``\textbf{histories}'' between updates.
  %
  E.g.\ $\sdia{c(01-2-3)(02-12-23)}$ depicts an update's (red's) double effect
  on a future update (green) that in turn affects the testing loss (blue).  The
  rightmost (``root'') node always represents a post-training measurement.

  \begin{dfn}
    \emph{A \textbf{history} of a diagram is an assignment of non-root nodes to
      $\emph{(n,t)}$ pairs such that: the $\emph{n}$th training point
      participates in the $t$th batch; parents' $\emph{t}$s strictly exceed
      their children's $\emph{t}$s; and any two nodes' $\emph{n}$s are equal if
      the nodes are in the same part of the partition.}\mend 
  \end{dfn}

  %\begin{dfn}
  %  \emph{
  %    A diagram $D$'s \emph{un-resummed value} ({\textbf{uvalue}})
  %    is a product with one factor of $l_x$'s $d$th derivative
  %    for each degree-$d$ node, grouped
  %    under cumulant symbols $\CC$ (think: expectation symbols $\expc$)\footnote{
  %      Inconsequential technicality: uvalues are products of
  %      \emph{cumulants} such as $C$, not of un-centered moments such as
  %      $GG+C$.
  %      %
  %      The symbol $\CC[a]$ gives $a$'s mean; $\CC[a\cdot
  %      b]$, $a,b$'s covariance; 
  %      we center higher cumulants 
  %      $\CC[\prod_i a_i]$ with respect to lower
  %      cumulants.
  %    }
  %    per $D$'s fuzzy groups, and tensor-contracted 
  %    via a
  %    factor $\eta^{\mu\nu}$ for each edge.
  %  }\mend
  %\end{dfn}

  \begin{figure}[h!]
    \plotmoow{aaai/recipe-a}{0.60\linewidth}{} 
    \hspace{0.04\linewidth}
    \plotmoow{aaai/embeddings-small}{0.33\linewidth}{} 
    \caption{%
        \textbf{Left}: Diagrams evaluate to tensor expressions: a degree-$d$
        node gives a $d$th derivative;  an edge gives a $(-\eta)$; expectation
        brackets enclose each fuzzy group.  \textbf{Right}: Each diagram
        depicts a class of histories; here is an example history for each of
        $\sdia{c(0-1)(01)}, \sdia{c(01-2)(02-12)}, \sdia{c(0-1-2)(02-12)}$.
        Summing diagram values over all possible histories gives the expected
        testing loss. 
    }
    \label{fig:recipe-a}
  \end{figure}
  
  \begin{thm*}[informal]
      The expected testing loss $\expc[l(\theta_T)]$ is a sum over
      all diagram histories, where each diagram is evaluated by a procedure as
      in Figure \ref{fig:recipe-a}.
      If we sum only diagrams with at most $d$ edges, we suffer only
      $o(\eta^d)$ error.
  \end{thm*}

%$\sdia{c(0-1)(01)}$, $\sdia{c(01-2)(02-12)}$, $\sdia{c(0-1-2)(02-12)}$ to

  For example, the only diagram with $1$ edge is $\sdia{c(0-1)(01)}$.  This
diagram evaluates to $-G\eta G$ and, since its red node can be any update while
its green node represents testing, describes $T$ many histories.  We thus
recover Prop \ref{prop:nest}'s (un-resummed value) $-TG\eta G$.  The re-summed value,
which turns out to be
$-TG\frac{1-\exp(-T\eta H)}{H}G$, tempers Prop \ref{prop:nest}'s large-$T$ behavior by
including contributions from $\sdia{c(0-1)(01)}, \sdia{c(0-1-2)(01-12)},
\sdia{c(0-1-2-3)(01-12-23)}, \cdots$.
%and converges as $T$ grows.


\subsection{SGD descends on a $C$-smoothed landscape}\label{sect:entropic-curl}
%An entropic force with curl}

We sketched above how a sum over diagrams gives 
$\expc[l(\theta_T)]$ for the testing loss $l$.  We may likewise compute
$\expc[s(\theta_T)]$ for other $s:\Mm\to\RR$; we need only replace
the $l$ corresponding to each diagram's root by $s$.  For instance, 
$\sdia{c(01-2-3)(02-12-23)}$ ordinarily evaluates 
to $-\eta^3 CJ(\nabla l)$ (see Figure \ref{fig:cubic}), but more generally
evaluates
to $-\eta^3 CJ(\nabla s)$.

Now let's study the displacement $\expc[\theta_T-\theta_0]$ of one-epoch,
batch-size-one SGD initialized at a testing loss minimum.
%
To do this, we let 
$s(\theta)=w\cdot \theta$ be linear; then $\expc[s(\theta_T)]$ is the
displacement's $w$-component. 
%
Since $s$'s higher derivatives vanish, any diagram whose root has degree $>1$
will evaluate to zero.  So only diagrams with degree-$1$ root are relevant to
computing displacements (e.g.\ $\sdia{c(0-1)(01)}, \sdia{c(0-1-2)(01-12)},
\sdia{c(01-2-3)(02-12-23)}$).
%
At a minimum, $G=0$ so we may ignore yet more diagrams: those with a factor of
$\expc[\nabla l]$, i.e., those with a degree-$1$ non-root node that's not
fuzzily grouped.  So we rule out $\sdia{c(0-1)(01)}, \sdia{c(0-1-2)(01-12)},
\cdots$.
%
%It turns out that
Only one fewest-edged diagram remains:
$\sdia{c(01-2-3)(02-12-23)}$, which above we evaluated as
$-\eta^3 (C\nabla H)\cdot (\nabla s)$.
%
So to leading order the displacement is some $T$-dependent number of histories times $-\eta^3 (C\nabla H)$.
%%/2!$ to leading order.\footnote{%
%%    The $2!$ comes 
%%    ``symmetry factors'' that we ignore for now. 
%%}



\begin{figure}%{r}{0.3\textwidth}
    \centering
    \plotmoow{aaai/anatomy-diagram-b}{0.49\linewidth}{}
    \plotmoow{aaai/cubic}{0.49\linewidth}{}
    %%\scalebox{1.0}[1.15]{\plotmoow{aaai/cubic}{0.54\linewidth}{}}
    %%\plotmoow{aaai/spring-b}{0.44\linewidth}{}
    %\rotatebox[origin=c]{-90}{\plotmoow{plots/from-above}{0.17\textwidth}{}}
    \caption{%
        \textbf{Left}:
        a diagram consists of nodes,
                  edges, and fuzzy groupings that dictate the diagram's
                  corresponding tensor expression.
        %This diagram helps quantify...
        \textbf{Right}:         %
        Gradient noise pushes SGD toward minima flat w.r.t.\ $C$.
            A 2D loss near
            a valley of minima.  Red densities show typical
            $\theta$s, perturbed by noise,
            in two slices of the valley.  The hessian
            changes across the valley: $J \neq 0$.  
    }
    \label{fig:cubic}
\end{figure}
Since $-\eta^3 C\nabla H$ points toward small $H$, \textbf{SGD moves
toward flat minima} (Figure \ref{fig:cubic}).
This effect scales with $C$.  
%
%%The bilinear form
%%$F^{\mu\nu}=\textstyle\sum_{\xi\omicron}\eta^{\mu\xi}
%%\eta^{\nu\omicron} C_{\xi\omicron}$ determines which $H$s
%%are `sharp' or `flat'.  E.g.: if $F$
%%degenerates along a covector $v$, then $H$'s 
%%$v$-component does not affect $\expc[\theta_T-\theta_0]$
%\footnote{
%    Explicitly: if
%        $\sum_{\mu\nu} v_\mu F^{\mu\nu} v_\nu = 0$,
%    then replacing $H$ by $\tilde H_{\mu\nu} = H_{\mu\nu} + 42 v_\mu v_\nu$ 
%    will not change $\Delta_\circ$.
%}
%
%Diagram techniques establish
Re-summation more precisely quantifies this `entropic force':\footnote{
    Thermal systems tend toward disorder as if pushed by an
    `entropic force'.
    So arises the tension of rubber
    bands: their polymers can wreathe in
    many ways but be straight in only one.
    Such `forces' characteristically 
    scale with
    temperature (the noise intensity $C$).
}\footnote{
    Our result ($T\gg 1$) is $\Theta(\eta^2)$; \cite{ya19b}'s
    ($T=2$) is $\Theta(\eta^3)$.  We
    integrate noise over time, amplifying $C$'s
    effect. 
}
%%as dominant when $G=0$, $N=T$.
%%Evaluating a single diagram
%%($
%%    \sdia{c(01-2-3)(02-12-23)}
%%$) yields:
%
\begin{cor}\label{cor:entropic}%[Computed from $\sdia{c(01-2-3)(02-12-23)}$]
    Start SGD at a minimum of $l$ with $H\succ 0$, $N=T$.  Use
    an eigenbasis of $K=\eta H$.  
    For any $T$ and
    with ${\mathcal P}_T(s) = (1 - \exp(-Ts))/s$,
    the 
    expected displacement is
    $$
        -
        %{\sum_{\substack{\mu\nu    \\ \omicron\pi\rho}}}
        {\sum_{\mu\nu   \omicron\pi\rho}}
            C_{\mu\omicron}
            {{\mathcal P}_T(K_{\mu\mu} + K_{\omicron\omicron})}
            \eta^{\mu\nu}\eta^{\omicron\pi}
            J_{\nu\pi\xi}
            {{\mathcal P}_T(K_{\xi\xi})}\eta^{\xi\rho}/2
        + o(\eta^3)
    $$
\end{cor}

For instance, consider a 1D valley of near-minima wherein $\eta H$ has spectrum
$\lambda_0 \ll 1/T \ll \lambda_1 \leq \cdots$ for eigenvectors
$v_i$.  Let's ignore diffusion along the valley: $(\eta C) v_0 =
0$.
%
Then ${\mathcal P}_T(\lambda_0)\approx T$ and every
$C_{ij}{\mathcal P}_T(\lambda_i+\lambda_j)$ is $O(T^0)$.  So $\expc[\theta_T-\theta_0]$
scales linearly with $T$.  We thus expect SGD to move with
%velocity $-f(\eta,H,T)\cdot C\nabla H/2$ toward flat minima.  
velocity $\approx -\eta^2 C\nabla H/2$ per timestep toward flat minima.  
Observe that $\nabla ({\frak G}_C \star l) = \nabla l+C\nabla
H/2 + o(C)$, where ${\frak G}_C \star$ denotes convolution with a
\emph{fixed} centered $C$-shaped Gaussian; we conclude with the
intuition that \emph{SGD descends on a $C$-smoothed landscape that
changes as $C$ does}.  %Since the smoothed $l$ may itself evolve,
%SGD might eternally circulate.

%\subsection{Curl in the entropic force}
\begin{figure}[h!]
    \plotmoow{aaai/helix-blueprint}{0.238\linewidth}{}\hspace{-0.04cm}
    \plotmoow{aaai/screw-trajectory-magnified-3}{0.73\linewidth}{}
    %\plotmoow{aaai/screw-trajectory-cropped}{0.47\textwidth}{} 
    \squash
    \caption{%
        {\,\,\protect\offour{0}}: \Helix\ is defined on an $\Mm=\RR^3$ 
        extending into the page.  A helical
        level surface $S$ (blue) of $l$ winds around 
        a 1D valley of minima orthogonal to the
        page.
        %Outside $S$, $l$ is large.
        Gradient noise (red bi-arrows),
        parallel to the page, twists out of phase with $l$.
        %
        {\,\,\protect\offour{123}}: SGD's trajectory (green) over 
        cross sections of \Helix\ descend progressively
        into the page.
        %$l$'s contours are in blue; $l$'s valley
        %intersects each pane's center.
        Dotted
        curves help compare adjacent panes.
        %%Red: %bi-arrows:
        %%$C$'s
        %%major axis.
        %
        Gradient noise kicks $\theta$ from A; $\theta$ then falls
        (\!\!\protect\offour{1}) to B in
        {\!\!\protect\offour{2}}.  At C, noise kicks $\theta$
        uphill (\!\!\protect\offour{3}); $\theta$ thus never
        settles and the descent persists.
    }
    \squash\squash
    \label{fig:archimedes}
\end{figure}

To test Corollary \ref{cor:entropic}'s $C$-dependence,
we %\S\ref{appendix:artificial}
construct a landscape, \Helix, on
whose valley of %global
minima $C$ varies (Figure
\ref{fig:archimedes}).  Each 
$\theta$ has a neighbor that is more attractive (flatter) w.r.t.\
$C(\theta)$.  This induces eternal motion into the page
despite \Helix's discrete translation symmetry.
%in that direction.
Corollary \ref{cor:entropic} predicts a velocity of
$+\eta^2/6$ per timestep, while \cite{ch18}'s SDE-based analysis
predicts\footnote{
    For, \Helix' velocity is $\eta$-perpendicular to the image
    of $(\eta C)^\mu_\nu$.%in tangent space.
} a constant velocity of $0$ (Figure
\ref{fig:experiments}).
%One may add a small linear term to \Helix\ to make SGD eternally
%ascend;
Wrapping \Helix\ in a loop makes SGD circulate. %in
%a velocity field with curl. %(\S\ref{appendix:artificial});
This is
possible because $C\nabla H$, unlike $\nabla(CH)$, is not a total
derivative. 
In avoiding \cite{we19b}'s constant-$C$ assumption, we 
find that SGD's velocity field can have curl. 

