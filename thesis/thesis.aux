\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{bo91}
\citation{fe49}
\citation{pe71}
\citation{ab12}
\pp@pagectr{footnote}{1}{6}{6}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{6}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\pp@pagectr{footnote}{2}{6}{6}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Background on learning}{6}{section.1.1}\protected@file@percent }
\newlabel{sect:background}{{1.1}{6}{Background on learning}{section.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{Generalization, optimization, and approximation}{6}{subsection*.1}\protected@file@percent }
\pp@pagectr{footnote}{3}{6}{6}
\pp@pagectr{footnote}{4}{6}{6}
\pp@pagectr{footnote}{5}{7}{7}
\pp@pagectr{footnote}{6}{7}{7}
\pp@pagectr{footnote}{7}{7}{7}
\pp@pagectr{footnote}{8}{7}{7}
\pp@pagectr{footnote}{9}{7}{7}
\@writefile{toc}{\contentsline {subsection}{Neural networks}{7}{subsection*.2}\protected@file@percent }
\pp@pagectr{footnote}{10}{7}{7}
\citation{bo13}
\pp@pagectr{footnote}{11}{8}{8}
\newpmemlabel{^_1}{8}
\@writefile{toc}{\contentsline {subsection}{Gradient descent}{8}{subsection*.3}\protected@file@percent }
\pp@pagectr{footnote}{12}{8}{8}
\pp@pagectr{footnote}{13}{8}{8}
\pp@pagectr{footnote}{14}{8}{8}
\pp@pagectr{footnote}{15}{8}{8}
\pp@pagectr{footnote}{16}{8}{8}
\@writefile{toc}{\contentsline {subsection}{Stochastic gradient descent}{9}{subsection*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Example of diagram-based computation of SGD's test loss}{9}{section.1.2}\protected@file@percent }
\newlabel{subsect:example}{{1.2}{9}{Example of diagram-based computation of SGD's test loss}{section.1.2}{}}
\pp@pagectr{footnote}{17}{10}{10}
\newlabel{exm:first}{{1}{10}{}{exm.1}{}}
\pp@pagectr{footnote}{18}{10}{10}
\citation{ko93}
\citation{ki52}
\citation{ca47}
\citation{ro51}
\citation{we74}
\citation{bo91}
\citation{le15}
\citation{ne17a}
\citation{ba17}
\citation{zh17}
\citation{ne17b}
\citation{ch18}
\citation{ya19a}
\citation{ro18}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Notation and assumptions}{11}{section.1.3}\protected@file@percent }
\newlabel{sect:notation}{{1.3}{11}{Notation and assumptions}{section.1.3}{}}
\pp@pagectr{footnote}{19}{11}{11}
\pp@pagectr{footnote}{20}{11}{11}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Related work}{11}{section.1.4}\protected@file@percent }
\newlabel{sect:related}{{1.4}{11}{Related work}{section.1.4}{}}
\pp@pagectr{footnote}{21}{11}{11}
\pp@pagectr{footnote}{22}{11}{11}
\citation{li18}
\citation{ho17}
\citation{ke17}
\citation{wa18}
\citation{st56}
\citation{di17}
\citation{wu18}
\citation{dy19}
\citation{ch18}
\citation{li17}
\citation{ro12}
\citation{ku19}
\pp@pagectr{footnote}{23}{12}{12}
\pp@pagectr{footnote}{24}{12}{12}
\pp@pagectr{footnote}{25}{12}{12}
\pp@pagectr{footnote}{26}{13}{13}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Theory, specialized to $E=B=1$ SGD's test loss}{13}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sect:calculus}{{2}{13}{Theory, specialized to $E=B=1$ SGD's test loss}{chapter.2}{}}
\pp@pagectr{footnote}{27}{13}{13}
\citation{ch17}
\citation{gu19}
\citation{we19b}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Main result}{14}{section.2.1}\protected@file@percent }
\newlabel{thm:resum}{{1}{14}{Special case of $E=B=1$}{thm.1}{}}
\newlabel{eq:resum}{{1}{14}{Special case of $E=B=1$}{thm.1}{}}
\newlabel{rmk:integrate}{{2}{14}{}{rmk.2}{}}
\newlabel{thm:converge}{{2}{14}{}{thm.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2} SGD descends on a $C$-smoothed landscape and prefers minima flat with respect to $C$. }{14}{section.2.2}\protected@file@percent }
\pp@pagectr{footnote}{28}{14}{14}
\newpmemlabel{^_2}{14}
\citation{ya19b}
\citation{we19b}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \textbf  {Gradient noise pushes SGD toward flat minima.} The red densities show the typical $\theta $s, perturbed from the minimum due to noise $C$, in two cross sections of the loss valley. $J = \nabla H$ measures how curvature changes across the valley. Our theory does not assume separation between ``fast'' and ``slow'' modes, but we label them in the picture to ease comparison with \citet  {we19b}. Compare with Figure \ref  {fig:archimedes}; see Corollary \ref  {cor:entropic}. }}{15}{figure.2.1}\protected@file@percent }
\newlabel{fig:cubic}{{2.1}{15}{ SGD descends on a $C$-smoothed landscape and prefers minima flat with respect to $C$. }{figure.2.1}{}}
\newlabel{cor:entropic}{{1}{15}{Computed from $\sdia {c(01-2-3)(02-12-23)}$}{cor.1}{}}
\pp@pagectr{footnote}{29}{15}{15}
\citation{ku19}
\citation{di18}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Both flat and sharp minima overfit less}{16}{section.2.3}\protected@file@percent }
\newlabel{subsect:curvature-and-overfitting}{{2.3}{16}{Both flat and sharp minima overfit less}{section.2.3}{}}
\newpmemlabel{^_3}{16}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces \textbf  { Both curvature and the structure of noise affect overfitting.} In each of the four subplots, the $\leftrightarrow $ axis represents weight space and the $\updownarrow $ axis represents loss. \offour {0}: \emph  {covector}-perturbed landscapes favor large $H$s. \offour {1}: \emph  {vector}-perturbed landscapes favor small $H$s. SGD's implicit regularization interpolates between these rows (Corollary \ref  {cor:overfit}). }}{16}{figure.2.2}\protected@file@percent }
\newlabel{fig:spring}{{2.2}{16}{Both flat and sharp minima overfit less}{figure.2.2}{}}
\newlabel{cor:overfit}{{2}{16}{from $\sdia {c(01-2)(02-12)}$, $\sdia {c(01)(01)}$}{cor.2}{}}
\pp@pagectr{footnote}{30}{16}{16}
\pp@pagectr{footnote}{31}{16}{16}
\citation{ch87}
\citation{li17}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}High-$C$ regions repel small-$(E,B)$ SGD more than large-$(E,B)$ SGD}{17}{section.2.4}\protected@file@percent }
\newlabel{sect:epochs-batch}{{2.4}{17}{High-$C$ regions repel small-$(E,B)$ SGD more than large-$(E,B)$ SGD}{section.2.4}{}}
\newpmemlabel{^_4}{17}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces  \textbf  {Chladni plate}. Grains of sand on a vibrating plate tend toward stationary regions. From Pierre Dragicevic and Yvonne Jansen's \href  {http://www.dataphys.org/list/gallery/}{data physicalization project}, Creative Commons BY-SA 3.0. }}{17}{figure.2.3}\protected@file@percent }
\newlabel{fig:chladni}{{2.3}{17}{High-$C$ regions repel small-$(E,B)$ SGD more than large-$(E,B)$ SGD}{figure.2.3}{}}
\pp@pagectr{footnote}{32}{17}{17}
\newlabel{cor:batch}{{3}{17}{$\sdia {c(01-2)(01-12)}$}{cor.3}{}}
\newlabel{cor:epochs}{{4}{17}{$\sdia {c(01-2)(01-12)}$}{cor.4}{}}
\citation{ya19a}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Non-Gaussian noise affects SGD but not SDE}{18}{section.2.5}\protected@file@percent }
\pp@pagectr{footnote}{33}{18}{18}
\newlabel{cor:vsode}{{5}{18}{$\sdia {c(01-2)(02-12)}$, $\sdia {c(012-3)(03-13-23)}$}{cor.5}{}}
\pp@pagectr{footnote}{34}{18}{18}
\pp@pagectr{footnote}{35}{18}{18}
\newlabel{prop:vanilla}{{1}{18}{}{prop.1}{}}
\newlabel{eq:sgdbasiccoef}{{1}{18}{}{prop.1}{}}
\pp@pagectr{footnote}{36}{18}{18}
\pp@pagectr{footnote}{37}{18}{18}
\pp@pagectr{footnote}{38}{18}{18}
\newpmemlabel{^_5}{18}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces -2cm}}{19}{table.2.1}\protected@file@percent }
\citation{ch18}
\pp@pagectr{footnote}{39}{20}{20}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Experiments}{20}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newpmemlabel{^_6}{20}
\newpmemlabel{^_7}{20}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces  \textbf  {Perturbation models SGD for small $\eta T$.} \oftwo {0}: Fashion-MNIST convnet's test loss vs learning rate. Here, the order-$3$ prediction holds until the $0\mbox  {-}1$ error improves by $5\cdot 10^{-3}$. For large $\eta T$, our predictions break down. \oftwo {1}: CIFAR-10 generalization gaps. For all initializations tested ($1$ shown, $11$ unshown), the degree-$2$ prediction agrees with experiment through $\eta T \approx 5\cdot 10^{-1}$. }}{20}{figure.3.1}\protected@file@percent }
\newlabel{fig:vanilla}{{3.1}{20}{Experiments}{figure.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces  \textbf  {Higher order terms improve order-$1$ predictions.} \oftwo {0}: For all Xavier initializations Fashion-MNIST tested ($1$ shown, $11$ unshown), the order $3$ prediction agrees with experiment through $\eta T \approx 10^0$, corresponding to a decrease in $0\mbox  {-}1$ error of $\approx 10^{-3}$. Meanwhile, order $1$ predictions agree only through $\eta T \approx 10^{-1/2}$. \oftwo {1}: We initialize CIFAR-10 near four minima found by pre-training. Order $2$ predictions improve on order $1$ baselines. }}{20}{figure.3.2}\protected@file@percent }
\newlabel{fig:perturb}{{3.2}{20}{Experiments}{figure.3.2}{}}
\citation{ch18}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Epochs and batch size; $C$ repels SGD more than GD}{21}{section.3.1}\protected@file@percent }
\newpmemlabel{^_8}{21}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces  \textbf  {$C$, modulated by batch size and epoch number, controls the generalization gap.} $N=10$ for both plots. With equal-scaled axes, \oftwo {0} shows that GDC matches SGD (small vertical variance) better than GD matches SGD (large horizontal variance) in test loss for a range of $\eta $ ($\approx 10^{-3}-10^{-1}$) and initializations\ (zero and several Xavier-Glorot trials) for logistic regression and convnets. Here, $T=10$. \oftwo {1}: SGD with $2, 3, 5, 8$ epochs. We scale the learning rate for $E$-epoch SGD by $1/E$ to isolate the effect of inter-epoch correlations away from the effect of larger $\eta T$. }}{21}{figure.3.3}\protected@file@percent }
\newlabel{fig:be}{{3.3}{21}{Epochs and batch size; $C$ repels SGD more than GD}{figure.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Minima that are flat \emph  {with respect to} $C$ attract SGD}{21}{section.3.2}\protected@file@percent }
\newlabel{subsect:entropic}{{3.2}{21}{Minima that are flat \emph {with respect to} $C$ attract SGD}{section.3.2}{}}
\newpmemlabel{^_9}{21}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces  \textbf  {\textls  [50]{\textsmallcaps  {Archimedes}}.} A \textbf  {green} level surface of $l$ twists around a valley of minima ($z$ axis) at its center; $l$ is large outside this surface. Due to anisotropic noise, $\theta $ scatters away from the $z$ axis toward the \textbf  {purple} tubes. SGD pushes the scattered $\theta $s toward lower loss, i.e.\ toward the level surface, and so toward larger $z$. The $z$ axis points into the page (\oftwo {0}) or upward (\oftwo {1}). We made these plots with the help of Paul Seeburger's online applet, \href  {https://www.monroecc.edu/faculty/paulseeburger/calcnsf/CalcPlot3D/}{CalcPlot3D}. }}{21}{figure.3.4}\protected@file@percent }
\newlabel{fig:archimedes}{{3.4}{21}{Minima that are flat \emph {with respect to} $C$ attract SGD}{figure.3.4}{}}
\pp@pagectr{footnote}{40}{21}{21}
\newpmemlabel{^_10}{21}
\citation{di18}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces  \textbf  {Predictions near minima excel for large $\eta T$.} \oftwo {0}: SGD travels \textls  [50]{\textsmallcaps  {Archimedes}}' valley of global minima in the positive $z$ direction. \oftwo {1}: SGD's difference from SDE after $\eta T \approx 10^{-1}$ with maximal coarseness on \textls  [50]{\textsmallcaps  {Gauss}}. }}{22}{figure.3.5}\protected@file@percent }
\newlabel{fig:vssde}{{3.5}{22}{Minima that are flat \emph {with respect to} $C$ attract SGD}{figure.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Sharp and flat minima both overfit less than medium minima}{22}{section.3.3}\protected@file@percent }
\newlabel{subsect:overfit}{{3.3}{22}{Sharp and flat minima both overfit less than medium minima}{section.3.3}{}}
\newpmemlabel{^_11}{22}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces  \textbf  {Both sharp and flat minima overfit less.} \oftwo {0}: For \textls  [50]{\textsmallcaps  {Mean Estimation}}\tmspace  +\thinmuskip {.1667em} with fixed $C$ and a range of $H$s, initialized at the truth, the test losses after fixed-$T$ GD are smallest for very sharp and very flat $H$. \oftwo {1}: Blue intervals regularization using Corollary \ref  {cor:overfit}. When the blue intervals fall below the black bar, this proposed method outperforms plain GD. }}{22}{figure.3.6}\protected@file@percent }
\newlabel{fig:tic}{{3.6}{22}{Sharp and flat minima both overfit less than medium minima}{figure.3.6}{}}
\citation{am98}
\citation{vi00}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Artificial Landscapes}{23}{section.3.4}\protected@file@percent }
\newlabel{appendix:artificial-landscapes}{{3.4}{23}{Artificial Landscapes}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\textls  [50]{\textsmallcaps  {Gauss}}}{23}{subsection*.5}\protected@file@percent }
\pp@pagectr{footnote}{41}{23}{23}
\@writefile{toc}{\contentsline {subsection}{\textls  [50]{\textsmallcaps  {Archimedes}}}{23}{subsection*.6}\protected@file@percent }
\pp@pagectr{footnote}{42}{23}{23}
\citation{kr09}
\citation{xi17}
\@writefile{toc}{\contentsline {subsection}{\textls  [50]{\textsmallcaps  {Mean Estimation}}}{24}{subsection*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Image-classification landscapes}{24}{section.3.5}\protected@file@percent }
\newlabel{appendix:natural}{{3.5}{24}{Image-classification landscapes}{section.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{Architectures}{24}{subsection*.8}\protected@file@percent }
\pp@pagectr{footnote}{43}{24}{24}
\pp@pagectr{footnote}{44}{24}{24}
\@writefile{toc}{\contentsline {subsection}{Datasets}{24}{subsection*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Measurement process}{25}{section.3.6}\protected@file@percent }
\newlabel{appendix:measure}{{3.6}{25}{Measurement process}{section.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{Diagram evaluation on real landscapes}{25}{subsection*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Descent simulations}{25}{subsection*.11}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces  \textbf  {Trials per condition, by landscape}. We list the conditions tested for each landscape in the prose to the left. }}{25}{table.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Implementing optimizers}{25}{subsection*.12}\protected@file@percent }
\newlabel{appendix:optimizers}{{3.6}{25}{Implementing optimizers}{subsection*.12}{}}
\citation{we19b}
\citation{ro18}
\citation{bo91}
\citation{go18}
\citation{st19}
\citation{ni11}
\citation{fi17}
\pp@pagectr{footnote}{45}{26}{26}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Conclusion}{26}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sect:concl}{{4}{26}{Conclusion}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Contributions}{26}{section.4.1}\protected@file@percent }
\pp@pagectr{footnote}{46}{26}{26}
\pp@pagectr{footnote}{47}{26}{26}
\pp@pagectr{footnote}{48}{26}{26}
\pp@pagectr{footnote}{49}{26}{26}
\citation{la60,la51}
\citation{ab07}
\citation{zh16}
\citation{am98}
\citation{ni17}
\pp@pagectr{footnote}{50}{27}{27}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Future topics}{27}{section.4.2}\protected@file@percent }
\newlabel{appendix:future}{{4.2}{27}{Future topics}{section.4.2}{}}
\pp@pagectr{footnote}{51}{27}{27}
\pp@pagectr{footnote}{52}{27}{27}
\pp@pagectr{footnote}{53}{27}{27}
\bibstyle{plainnat}
\bibdata{perturb}
\bibcite{ab07}{{1}{2007}{{Absil et~al.}}{{Absil, Mahony, and Sepulchre}}}
\bibcite{ab12}{{2}{2012}{{Abu-Mostafa et~al.}}{{Abu-Mostafa, Magdon-Ismail, and Lin}}}
\bibcite{am98}{{3}{1998}{{Amari}}{{}}}
\bibcite{ba17}{{4}{2017}{{Bartlett et~al.}}{{Bartlett, Foster, and Telgarsky}}}
\bibcite{be64}{{5}{1964}{{Bernstein}}{{}}}
\bibcite{bo13}{{6}{2013}{{Bonnabel}}{{}}}
\bibcite{bo91}{{7}{1991}{{Bottou}}{{}}}
\bibcite{ca47}{{8}{1847}{{Cauchy}}{{}}}
\bibcite{ch18}{{9}{2018}{{Chaudhari and Soatto}}{{}}}
\bibcite{ch17}{{10}{2017}{{Chaudhari et~al.}}{{Chaudhari, Choromanska, Soatto, LeCun, Baldassi, Borgs, Chayes, Sagun, and Zecchina}}}
\bibcite{ch87}{{11}{1787}{{Chladni}}{{}}}
\bibcite{ch79}{{12}{1979}{{Chv\'atal}}{{}}}
\bibcite{di17}{{13}{2017}{{Dinh et~al.}}{{Dinh, Pascanu, Bengio, and Bengio}}}
\bibcite{di18}{{14}{2018}{{Dixon and Ward}}{{}}}
\bibcite{dy19}{{15}{2019}{{Dyer and Gur-Ari}}{{}}}
\bibcite{dy49a}{{16}{1949}{{Dyson}}{{}}}
\bibcite{fe49}{{17}{1949}{{Feynman}}{{}}}
\bibcite{fi17}{{18}{2017}{{Finn et~al.}}{{Finn, Abbeel, and Levine}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{28}{chapter*.13}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{ga23}{{19}{1823}{{Gauss}}{{}}}
\bibcite{go18}{{20}{2018}{{Goyal et~al.}}{{Goyal, Doll\'{a}r, Girshick, Noordhuis, Wesolowski, Kyrola, Tulloch, Jia, and He}}}
\bibcite{gu19}{{21}{2019}{{Gur-Ari et~al.}}{{Gur-Ari, Roberts, and Dyer}}}
\bibcite{ho17}{{22}{2017}{{Hoffer et~al.}}{{Hoffer, Hubara, and Soudry}}}
\bibcite{im10}{{23}{2010}{{Impagliazzo and Kabanets}}{{}}}
\bibcite{ke17}{{24}{2017}{{Keskar et~al.}}{{Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang}}}
\bibcite{ki52}{{25}{1952}{{Kiefer and Wolfowitz}}{{}}}
\bibcite{ko93}{{26}{1993}{{Kol\'{a}\u {r} et~al.}}{{Kol\'{a}\u {r}, Michor, and Slov\'{a}k}}}
\bibcite{kr09}{{27}{2009}{{Krizhevsky}}{{}}}
\bibcite{ku19}{{28}{2019}{{Kunstner et~al.}}{{Kunstner, Hennig, and Balles}}}
\bibcite{la51}{{29}{1951}{{Landau and Lifshitz}}{{}}}
\bibcite{la60}{{30}{1960}{{Landau and Lifshitz}}{{}}}
\bibcite{le15}{{31}{2015}{{LeCun et~al.}}{{LeCun, Bengio, and Hinton}}}
\bibcite{li17}{{32}{2017}{{Li et~al.}}{{Li, Tai, and E}}}
\bibcite{li18}{{33}{2018}{{Liao et~al.}}{{Liao, Miranda, Banburski, Hidary, and Poggio}}}
\bibcite{mu18}{{34}{2018}{{Mulzer}}{{}}}
\bibcite{ne17a}{{35}{2017{a}}{{Neyshabur et~al.}}{{Neyshabur, Bhojanapalli, McAllester, and Srebro}}}
\bibcite{ne17b}{{36}{2017{b}}{{Neyshabur et~al.}}{{Neyshabur, Tomioka, Salakhutdinov, and Srebro}}}
\bibcite{ni17}{{37}{2017}{{Nickel and Kiela}}{{}}}
\bibcite{ni11}{{38}{2011}{{Niu et~al.}}{{Niu, Recht, R\'e, and Wright}}}
\bibcite{pe71}{{39}{1971}{{Penrose}}{{}}}
\bibcite{ro51}{{40}{1951}{{Robbins and Monro}}{{}}}
\bibcite{ro18}{{41}{2018}{{Roberts}}{{}}}
\bibcite{ro64}{{42}{1964}{{Rota}}{{}}}
\bibcite{ro12}{{43}{2012}{{Roux et~al.}}{{Roux, Bengio, and Fitzgibbon}}}
\bibcite{st56}{{44}{1956}{{Stein}}{{}}}
\bibcite{st17}{{45}{2017}{{Steinke and Ullman}}{{}}}
\bibcite{st19}{{46}{2019}{{Strubell et~al.}}{{Strubell, Ganesh, and McCallum}}}
\bibcite{vi00}{{47}{circa $10^{1/2}$ b.c.e.}{{Vitruvius}}{{}}}
\bibcite{wa18}{{48}{2018}{{Wang et~al.}}{{Wang, Keskar, Xiong, and Socher}}}
\bibcite{we19b}{{49}{2019}{{Wei and Schwab}}{{}}}
\bibcite{we74}{{50}{1974}{{Werbos}}{{}}}
\bibcite{wu18}{{51}{2018}{{Wu et~al.}}{{Wu, Ma, and E}}}
\bibcite{xi17}{{52}{2017}{{Xiao et~al.}}{{Xiao, Rasul, and Vollgraf}}}
\bibcite{ya19a}{{53}{2019{a}}{{Yaida}}{{}}}
\bibcite{ya19b}{{54}{2019{b}}{{Yaida}}{{}}}
\bibcite{zh17}{{55}{2017}{{Zhang et~al.}}{{Zhang, Bengio, Hardt, Recht, and Vinyals}}}
\bibcite{zh16}{{56}{2016}{{Zhang et~al.}}{{Zhang, Reddi, and Sra}}}
\pp@pagectr{footnote}{54}{33}{33}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}How to calculate test losses}{33}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{appendix:tutorial}{{A}{33}{How to calculate test losses}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}An example calculation: the effect of epochs}{33}{section.1.1}\protected@file@percent }
\newlabel{appendix:example}{{A.1}{33}{An example calculation: the effect of epochs}{section.1.1}{}}
\newlabel{qst:multi}{{2}{33}{}{quest.2}{}}
\@writefile{toc}{\contentsline {subsection}{Space-time grids}{34}{subsection*.15}\protected@file@percent }
\newpmemlabel{^_12}{34}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces  \textbf  {The space-time grids of single-epoch and of multi-epoch SGD.} A cell at row $n$ and column $t$ is shaded provided that the $n$th training sample inhabits the $t$th batch. Both grids depict $N=7$ training points and batch size $B=1$; neither depicts training-set permutation between epochs. \newline  \textbf  {Left}: SGD with $M=2$ update per training sample for a total of $T = MN = 2N$ many updates. \newline  \textbf  {Right}: SGD with $M=1$ update per training sample for a total of $T = MN = N$ many updates. }}{34}{figure.1.1}\protected@file@percent }
\newlabel{fig:spacetimes-epoch}{{A.1}{34}{Space-time grids}{figure.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{Embeddings of diagrams into space-time}{34}{subsection*.16}\protected@file@percent }
\newpmemlabel{^_13}{34}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces  \textbf  {The diagram $\sdia {c(01-2)(01-12)}$ embeds into multi-epoch but not single-epoch space-time.} Drawn on each of the two grids are examples of embeddings. The black nodes external to the grids are positioned arbitrarily. From top to bottom in each grid, the five diagrams embedded are $\sdia {c(01-2)(01-12)}$ (or $\sdia {c(0-1-2)(01-12)}$), $\sdia {c(0-1)(01)}$, $\sdia {c(0-1-2)(01-12)}$, $\sdia {c(0-1-2)(02-12)}$, and $\sdia {c(01-2)(02-12)}$ (or $\sdia {c(0-1-2)(02-12)}$). The diagram $\sdia {c(0-1-2)(01-12)}$ may be embedded wherever the diagram $\sdia {c(01-2)(01-12)}$ may be embedded, but not vice versa. Likewise for $\sdia {c(0-1-2)(02-12)}$ and $\sdia {c(01-2)(02-12)}$. \textbf  {Left}: $\sdia {c(01-2)(01-12)}$ embeds into multi-epoch space-time. \textbf  {Right}: $\sdia {c(01-2)(01-12)}$ cannot embed into single-epoch space-time. Indeed, the correlation condition forces both red nodes into the same row and thus the same cell, while the time-ordering condition forces the red nodes into distinct columns and thus distinct cells. }}{35}{figure.1.2}\protected@file@percent }
\newlabel{fig:multi-embeddings}{{A.2}{35}{Embeddings of diagrams into space-time}{figure.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{Values of the embeddings}{35}{subsection*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Sum of the values}{35}{subsection*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.2}How to identify the relevant space-time}{36}{section.1.2}\protected@file@percent }
\newlabel{appendix:draw-spacetime}{{A.2}{36}{How to identify the relevant space-time}{section.1.2}{}}
\newpmemlabel{^_14}{36}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces  \textbf  {The space-time grids of two SGD variants.} Shaded cells show $(n,t)$ pairs (see text). \newline  \textbf  {Left}: Two epoch SGD with batch size one. The training set is permuted between epochs. \newline  \textbf  {Right}: Four epoch SGD with batch size two. The training set is not permuted between epochs. }}{36}{figure.1.3}\protected@file@percent }
\newlabel{fig:spacetimes}{{A.3}{36}{How to identify the relevant space-time}{figure.1.3}{}}
\pp@pagectr{footnote}{55}{36}{36}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}How to identify the relevant diagram embeddings}{37}{section.1.3}\protected@file@percent }
\newlabel{appendix:draw-embeddings}{{A.3}{37}{How to identify the relevant diagram embeddings}{section.1.3}{}}
\newpmemlabel{^_15}{37}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces  Embeddings, legal and illegal. \textbf  {Left}: illegal embedding of $\sizeddia {c(0-1-2)(01-12)}{0.10}$, since the time-ordering condition is not obeyed. For the same reason, not a legal embedding of $\sizeddia {c(01-2)(01-12)}{0.10}$. \textbf  {Middle}: an embedding of $\sizeddia {c(0-1-2)(01-12)}{0.10}$. Also an embedding of $\sizeddia {c(01-2)(01-12)}{0.10}$, since the correlation condition is obeyed. \textbf  {Right}: a legal embedding of $\sizeddia {c(0-1-2)(01-12)}{0.10}$. Not an embedding of $\sizeddia {c(01-2)(01-12)}{0.10}$, since the correlation condition is not obeyed. }}{37}{figure.1.4}\protected@file@percent }
\newlabel{fig:embeddings}{{A.4}{37}{How to identify the relevant diagram embeddings}{figure.1.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces  \textbf  {Multiple ways to draw the $6$ distinct degree-$3$ diagrams for $B=E=1$ SGD's test loss.} Because the space-time of $B=E=1$ SGD has only one cell per row and one cell per column, the only diagrams that have a non-zero number of embeddings are the diagrams that obey \S  \ref  {sect:calculus}'s path condition. We show $(4+2)+(2+2+3)+(1)$ ways to draw the $6$ diagrams. In fact, these drawings show all of the time-orderings of the diagrams' nodes that are consistent with the time-ordering condition. }}{38}{table.1.1}\protected@file@percent }
\newlabel{tab:scatthree}{{A.1}{38}{How to identify the relevant diagram embeddings}{table.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}How to evaluate each embedding}{38}{section.1.4}\protected@file@percent }
\newlabel{appendix:evaluate-embeddings}{{A.4}{38}{How to evaluate each embedding}{section.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{Un-resummed values: $\text  {\textnormal  {uvalue}}(D)$}{38}{subsection*.19}\protected@file@percent }
\citation{ro64}
\newlabel{exm:unresum}{{5}{39}{Un-resummed value}{exm.5}{}}
\@writefile{toc}{\contentsline {subsection}{Resummed values: $\text  {\textnormal  {rvalue}}_f(D)$}{39}{subsection*.20}\protected@file@percent }
\newlabel{exm:resum}{{6}{39}{Re-summed value}{exm.6}{}}
\@writefile{toc}{\contentsline {subsection}{Overall}{40}{subsection*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.5}How to sum the embeddings' values}{40}{section.1.5}\protected@file@percent }
\newlabel{appendix:sum-embeddings}{{A.5}{40}{How to sum the embeddings' values}{section.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.6}Interpreting diagrams intuitively}{41}{section.1.6}\protected@file@percent }
\newlabel{appendix:interpret-diagrams}{{A.6}{41}{Interpreting diagrams intuitively}{section.1.6}{}}
\newpmemlabel{^_16}{41}
\@writefile{lof}{\contentsline {figure}{\numberline {A.5}{\ignorespaces  \textbf  {Edges carry information}. Embedding of a $4$-edged diagram. }}{41}{figure.1.5}\protected@file@percent }
\newlabel{fig:intuition}{{A.5}{41}{Interpreting diagrams intuitively}{figure.1.5}{}}
\newpmemlabel{^_17}{41}
\@writefile{lof}{\contentsline {figure}{\numberline {A.6}{\ignorespaces  \textbf  {Resummation propagates information, damped by curvature}. \textbf  {Left}: Here is one of many un-resummed terms captured by a single resummed embedding for $\sizeddia {c(0-1)(01)}{0.10}$. \textbf  {Right}: each resummed value represents many un-resummed values. Here is one of many un-resummed terms captured by a single resummed embedding for $\sizeddia {c(01-2)(02-12)}{0.10}$. }}{42}{figure.1.6}\protected@file@percent }
\newlabel{fig:more-intuition}{{A.6}{42}{Interpreting diagrams intuitively}{figure.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.7}How to solve variant problems}{42}{section.1.7}\protected@file@percent }
\newlabel{appendix:solve-variants}{{A.7}{42}{How to solve variant problems}{section.1.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.8}Do diagrams streamline computation?}{43}{section.1.8}\protected@file@percent }
\newlabel{appendix:diagrams-streamline}{{A.8}{43}{Do diagrams streamline computation?}{section.1.8}{}}
\pp@pagectr{footnote}{56}{43}{43}
\@writefile{toc}{\contentsline {subsection}{Effect of batch size}{43}{subsection*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Effect of non-Gaussian noise at a minimum.}{45}{subsection*.23}\protected@file@percent }
\pp@pagectr{footnote}{57}{48}{48}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Mathematics of the theory}{48}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{appendix:math}{{B}{48}{Mathematics of the theory}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Assumptions and Definitions}{48}{section.2.1}\protected@file@percent }
\newlabel{appendix:assumptions}{{B.1}{48}{Assumptions and Definitions}{section.2.1}{}}
\pp@pagectr{footnote}{58}{48}{48}
\newlabel{dfn:diagrams}{{2}{48}{Diagrams}{defn.2}{}}
\citation{dy49a}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}A key lemma \`a la Dyson}{49}{section.2.2}\protected@file@percent }
\newlabel{appendix:key-lemma}{{B.2}{49}{A key lemma \`a la Dyson}{section.2.2}{}}
\newlabel{lem:dyson}{{B.2}{49}{}{section.2.2}{}}
\newlabel{eq:dyson}{{B.1}{49}{}{equation.2.2.1}{}}
\newlabel{eq:smalleta}{{B.2}{49}{A key lemma \`a la Dyson}{equation.2.2.2}{}}
\pp@pagectr{footnote}{59}{49}{49}
\newlabel{eq:expansion}{{B.3}{50}{A key lemma \`a la Dyson}{equation.2.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}From Dyson to diagrams}{50}{section.2.3}\protected@file@percent }
\newlabel{appendix:toward-diagrams}{{B.3}{50}{From Dyson to diagrams}{section.2.3}{}}
\newlabel{thm:pathint}{{3}{51}{Test Loss as a Path Integral}{thm.3}{}}
\newlabel{eq:sgdcoef}{{3}{51}{Test Loss as a Path Integral}{thm.3}{}}
\newlabel{eq:toprove}{{B.4}{51}{From Dyson to diagrams}{equation.2.3.4}{}}
\newlabel{rmk:leibniz}{{4}{51}{Differentiating Products}{rmk.4}{}}
\newlabel{eqn:countclaim}{{B.5}{52}{From Dyson to diagrams}{equation.2.3.5}{}}
\citation{ro64}
\@writefile{toc}{\contentsline {section}{\numberline {B.4}Theorems \ref  {thm:resum} and \ref  {thm:converge}}{53}{section.2.4}\protected@file@percent }
\newlabel{appendix:resum}{{B.4}{53}{Theorems \ref {thm:resum} and \ref {thm:converge}}{section.2.4}{}}
\newlabel{subsubsect:resum}{{B.4}{53}{Theorems \ref {thm:resum} and \ref {thm:converge}}{section.2.4}{}}
\pp@pagectr{footnote}{60}{53}{53}
\newlabel{eq:hard}{{B.6}{53}{Theorems \ref {thm:resum} and \ref {thm:converge}}{equation.2.4.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.5}Proofs of corollaries}{55}{section.2.5}\protected@file@percent }
\newlabel{appendix:corollaries}{{B.5}{55}{Proofs of corollaries}{section.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{Corollary \ref  {cor:entropic}}{55}{subsection*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Corollary \ref  {cor:overfit}'s first part}{55}{subsection*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Corollary \ref  {cor:overfit}'s second part}{55}{subsection*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Corollaries \ref  {cor:epochs} and \ref  {cor:batch}}{56}{subsection*.27}\protected@file@percent }
\newlabel{prop:ordtwo}{{2}{56}{}{prop.2}{}}
\newlabel{tbl:ordtwo}{{B.5}{56}{Corollaries \ref {cor:epochs} and \ref {cor:batch}}{prop.2}{}}
\@writefile{toc}{\contentsline {subsection}{Corollary \ref  {cor:vsode}}{56}{subsection*.28}\protected@file@percent }
\citation{ga23}
\@writefile{toc}{\contentsline {section}{\numberline {B.6}Unbiased estimators of landscape statistics}{57}{section.2.6}\protected@file@percent }
\newlabel{appendix:bessel}{{B.6}{57}{Unbiased estimators of landscape statistics}{section.2.6}{}}
\newlabel{eq:bessel}{{B.7}{57}{Unbiased estimators of landscape statistics}{equation.2.6.7}{}}
\newlabel{eq:newbessel}{{B.8}{57}{Unbiased estimators of landscape statistics}{equation.2.6.8}{}}
\pp@pagectr{footnote}{61}{59}{59}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Bonus tracks}{59}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{appendix:bonus}{{C}{59}{Bonus tracks}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C.1}Long-term prediction of SGD dynamics is intractable}{59}{section.3.1}\protected@file@percent }
\newlabel{appendix:pspace}{{C.1}{59}{Long-term prediction of SGD dynamics is intractable}{section.3.1}{}}
\newlabel{prop:pspace}{{3}{59}{}{prop.3}{}}
\pp@pagectr{footnote}{62}{59}{59}
\citation{mu18}
\citation{be64}
\citation{ch79}
\citation{im10}
\citation{mu18}
\citation{st17}
\@writefile{toc}{\contentsline {section}{\numberline {C.2}A new proof of a Chernoff bound}{61}{section.3.2}\protected@file@percent }
\newlabel{appendix:chernoff}{{C.2}{61}{A new proof of a Chernoff bound}{section.3.2}{}}
\pp@pagectr{footnote}{63}{61}{61}
\pp@pagectr{footnote}{64}{61}{61}
\pp@pagectr{footnote}{65}{61}{61}
\pp@pagectr{footnote}{66}{61}{61}
\pp@pagectr{footnote}{67}{61}{61}
\newpmemlabel{^_18}{61}
\@writefile{lof}{\contentsline {figure}{\numberline {C.1}{\ignorespaces \emph  { We randomly select points on $N$ vertical sticks. Each stick has three parts: \textbf  {green} with length $1-p$, \textbf  {red} with length $p$, and \textbf  {blue} with length $g$. We call non-blue points \textbf  {boxed} and non-green points \textbf  {hollow}. Shown are $9$ boxed points and $7$ hollow ones. }}}{61}{figure.3.1}\protected@file@percent }
\ttl@finishall
