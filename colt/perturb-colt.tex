\documentclass[anon,12pt]{colt2021} % Anonymized submission
%\documentclass[final,12pt]{colt2021} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

\usepackage{amsfonts, makerobust}
\usepackage{mathtools, nicefrac, xstring, enumitem}

\newcommand{\subthreesect}[1]{\par\noindent\textsc{#1} --- }

%   The following reconciles COLT's style with \includegraphics:
%       (see tex.stackexchange.com/questions/520891)
\makeatletter
\let\Ginclude@graphics\@org@Ginclude@graphics
\makeatother

\usepackage[export]{adjustbox}

%---------------------  graphics and figures  ---------------------------------
 \usepackage{wrapfig, caption}
\usepackage{hanging, txfonts, ifthen}

\newcommand{\ofsix}[1]{
    {\tiny \raisebox{0.04cm}{$\substack{
        \ifthenelse{\equal{#1}{0}}{{\color{moor}\blacksquare}}{\square}
        \ifthenelse{\equal{#1}{2}}{{\color{moor}\blacksquare}}{\square}    
        \ifthenelse{\equal{#1}{4}}{{\color{moor}\blacksquare}}{\square} \\
        \ifthenelse{\equal{#1}{1}}{{\color{moor}\blacksquare}}{\square}    
        \ifthenelse{\equal{#1}{3}}{{\color{moor}\blacksquare}}{\square}
        \ifthenelse{\equal{#1}{5}}{{\color{moor}\blacksquare}}{\square}
    }$}}%
}

\newcommand{\offive}[1]{
    {\tiny
        \raisebox{-0.04cm}{\color{gray}\scalebox{2.5}{$\substack{
            \ifthenelse{\equal{#1}{0}}{{\color{moor}\blacksquare}}{\square} 
        }$}}%
        \raisebox{0.04cm}{$\substack{
            \IfSubStr{#1}{1}{{\color{moor}\blacksquare}}{\square}   
            \IfSubStr{#1}{1}{{\color{moor}\blacksquare}}{\square} \\
            \IfSubStr{#1}{2}{{\color{moor}\blacksquare}}{\square}    
            \IfSubStr{#1}{2}{{\color{moor}\blacksquare}}{\square}    
        }$}%
    }%
}

\newcommand{\ofthree}[1]{
    {\tiny \raisebox{0.04cm}{$
        \ifthenelse{\equal{#1}{0}}{{\color{moor}\blacksquare}}{\square}
        \ifthenelse{\equal{#1}{1}}{{\color{moor}\blacksquare}}{\square}    
        \ifthenelse{\equal{#1}{2}}{{\color{moor}\blacksquare}}{\square}
    $}}%
}

\newcommand{\offour}[1]{
    {\tiny \raisebox{0.04cm}{$
        \ifthenelse{\equal{#1}{0}}{{\color{moor}\blacksquare}}{\square}
        \ifthenelse{\equal{#1}{1}}{{\color{moor}\blacksquare}}{\square}    
        \ifthenelse{\equal{#1}{2}}{{\color{moor}\blacksquare}}{\square}
        \ifthenelse{\equal{#1}{3}}{{\color{moor}\blacksquare}}{\square}
    $}}%
}




%---------------------  colors  -----------------------------------------------

\usepackage{xcolor, framed}
\definecolor{moolime}{rgb}{0.90,1.00,0.90}
\definecolor{moosky}{rgb}{0.90,0.90,1.00}
\definecolor{moopink}{rgb}{1.00,0.90,0.90}

\definecolor{moogold}{rgb}{1.00,1.00,0.80}

\definecolor{moor}{rgb}{0.8,0.2,0.2}
\definecolor{moog}{rgb}{0.2,0.8,0.2}
\definecolor{moob}{rgb}{0.2,0.2,0.8}
\definecolor{mooteal}{rgb}{0.1,0.6,0.4}

\definecolor{spacetimered}{rgb}{0.8, 0.2, 0.0}
\definecolor{spacetimeorange}{rgb}{0.6, 0.4, 0.1}
\definecolor{spacetimelemon}{rgb}{0.4, 0.6, 0.1}
\definecolor{spacetimegreen}{rgb}{0.0, 0.8, 0.2}
\definecolor{spacetimeteal}{rgb}{0.1, 0.6, 0.4}
\definecolor{spacetimesky}{rgb}{0.1, 0.4, 0.6}
\definecolor{spacetimeblue}{rgb}{0.2, 0.0, 0.8}
\definecolor{spacetimeindigo}{rgb}{0.4, 0.1, 0.6}
\definecolor{spacetimepurple}{rgb}{0.6, 0.1, 0.4}



%---------------------  intertext: footnotes and hyperlinks  ------------------ 

\usepackage[perpage]{footmisc}
\renewcommand*{\thefootnote}{%
    \color{red}%
    \arabic{footnote}%
    %\fnsymbol{footnote}%
} 

\usepackage{hyperref}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Theorem Environments  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  mathematical results  ---------------------------------

%\theoremstyle{plain}
    \newtheorem*{klem*}{Key Lemma}
    \newtheorem{thm}{Theorem}
    \newtheorem*{thm*}{Theorem}
    \newtheorem{cor}{Corollary}
    \newtheorem{prop}{Proposition}
    \newtheorem*{prop*}{Proposition}
    \setcounter{prop}{-1}

%---------------------  mathematical questions  -------------------------------

    \newtheorem{conj}{Conjecture}
    \newtheorem{quest}{Question}
    \newtheorem*{quest*}{Question}
    \newtheorem*{quests*}{Questions}

%---------------------  definitions, answers, remarks  ------------------------

%\theoremstyle{definition}
    \newtheorem{dfn}{Definition}
    \newtheorem*{answ*}{Answer}
    \newtheorem{rmk}{Remark}
    \newtheorem*{midea*}{Main Idea}
    \newtheorem*{rmk*}{Remark}
    \newtheorem{exm}{Example}


%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Custom Math Commands  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\newcommand{\pr}{\prime}

%---------------------  expanding containers  ---------------------------------


\newcommand{\wrap}[1]{\left(#1\right)}
\newcommand{\wasq}[1]{\left[#1\right]}
\newcommand{\wang}[1]{\left\langle#1\right\rangle}
\newcommand{\wive}[1]{\left\llbracket#1\right\rrbracket}
\newcommand{\worm}[1]{\left\|#1\right\|}
\newcommand{\wabs}[1]{\left|#1\right|}
\newcommand{\wurl}[1]{\left\{#1\right\}}

\newcommand{\partitionbox}[1]{
    \text{
        \fboxsep=0.5pt
        \tiny
        \fbox{#1}
    }
}

%---------------------  special named objects  --------------------------------


        \newcommand{\nb} { \nabla }
        \newcommand{\lx} { l_x(\theta) }
        \newcommand{\teq} { \triangleq }
        \newcommand{\ex}[1] { \expc_x \wasq{#1} }


\newcommand{\Free}{\mathcal{F}}
\newcommand{\Forg}{\mathcal{G}}
\newcommand{\Mod}{\mathcal{M}}
\newcommand{\Hom}{\text{\textnormal{Hom}}}
\newcommand{\Aut}{\text{\textnormal{Aut}}}
\newcommand{\image}{\text{\textnormal{im}}}
\newcommand{\uvalue}{\text{\textnormal{uvalue}}}
\newcommand{\rvalue}{\text{\textnormal{rvalue}}}
\newcommand{\edges}{\text{\textnormal{edges}}}
\newcommand{\ords}{\text{\textnormal{ords}}}
\newcommand{\parts}{\text{\textnormal{parts}}}
\newcommand{\SGD}{\text{\textnormal{SGD}}}
\DeclareMathOperator*{\Avg}{\text{\sffamily A}}
\newcommand{\expc}{\mathbb{E}}
\newcommand{\expct}[1]{\mathbb{E}\left[#1\right]}

%---------------------  fancy letters  ----------------------------------------

\newcommand{\Aa}{\mathcal{A}}
\newcommand{\Bb}{\mathcal{B}}
\newcommand{\Cc}{\mathcal{C}}   \newcommand{\CC}{\mathbb{C}}
\newcommand{\Dd}{\mathcal{D}}
\newcommand{\Ee}{\mathcal{E}}
\newcommand{\Ff}{\mathcal{F}}
\newcommand{\Gg}{\mathcal{G}}
\newcommand{\Hh}{\mathcal{H}}
\newcommand{\Ll}{\mathcal{L}}
\newcommand{\Mm}{\mathcal{M}}
\newcommand{\Nn}{\mathcal{N}}   \newcommand{\NN}{\mathbb{N}}
\newcommand{\Oo}{\mathcal{O}}
\newcommand{\Pp}{\mathcal{P}}
\newcommand{\Qq}{\mathcal{Q}}   \newcommand{\QQ}{\mathbb{Q}}
\newcommand{\Rr}{\mathcal{R}}   \newcommand{\RR}{\mathbb{R}}
\newcommand{\Ss}{\mathcal{S}}
\newcommand{\Tt}{\mathcal{T}}
\newcommand{\Uu}{\mathcal{U}}
\newcommand{\Vv}{\mathcal{V}}
\newcommand{\Ww}{\mathcal{W}}
\newcommand{\Xx}{\mathcal{X}}
\newcommand{\Yy}{\mathcal{Y}}
\newcommand{\Zz}{\mathcal{Z}}   \newcommand{\ZZ}{\mathbb{Z}}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Pictures  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  pictures with specified width or height  --------------

\newcommand{\plotmoow}[3]{\includegraphics[width=#2          ]{../#1}}
\newcommand{\plotmooh}[3]{\includegraphics[         height=#3]{../#1}}
\newcommand{\pmoo}[2]{\includegraphics[height=#1]{../plots/#2}}
\newcommand{\dmoo}[2]{\includegraphics[height=#1]{../diagrams/#2}}

%---------------------  inline diagrams of various sizes  ---------------------

\newcommand{\sizeddia}[2]{%
    \begin{gathered}%
        \includegraphics[scale=#2]{../diagrams/#1.png}%
    \end{gathered}%
}
\newcommand{\bdia}[1]{\protect \sizeddia{#1}{0.22}}
\newcommand{\dia} [1]{\protect \sizeddia{#1}{0.18}}
\newcommand{\mdia}[1]{\protect \sizeddia{#1}{0.14}}
\newcommand{\sdia}[1]{\protect \sizeddia{#1}{0.10}}

\newcommand{\mend}{\hfill $\Diamond$}

\newcommand{\Gauss}{\textsc{Gauss}}
\newcommand{\Helix}{\textsc{Helix}}
\newcommand{\MeanEstimation}{\textsc{Mean Estimation}}




\newcommand{\sS}{\hspace{0.43em}}
\newcommand{\sammail}{%
    C{\sS}%
    O{\sS}%
    L{\sS}%
    I{\tiny{@}}%
    M{\sS}%
    I{\sS}%
    T{\sS}%
    .{\sS}%
    e{\sS}%
    d{\sS}%
    u%
}

\title[A Perturbative Analysis of SGD]{
    A Perturbative Analysis of Stochastic Gradient Descent
}
\usepackage{times}
\coltauthor{%
    \Name{Samuel C.\ Tenka} \Email{\sammail}    \\
    \addr MIT, CSAIL
}

\begin{document}

    \maketitle
    
    \begin{abstract}%
        We quantify how gradient noise shapes the dynamics of stochastic
        gradient descent (SGD) by taking Taylor series in the learning rate.
        %
        We present in particular a new diagram-based notation that permits
        resummation to convergent results.
        %
        We employ our theory to contrast SGD against two popular
        approximations: deterministic descent and stochastic differential
        equations.  We find that SGD's trajectory avoids regions of weight
        space with high gradient noise and avoids minima that are sharp with
        respect to gradient noise.
    \end{abstract}
    
    \begin{keywords}%
        SGD, learning rates, generalization, gradient noise, perturbation. 
    \end{keywords}

    \section{Introduction}

        %\subsection{SGD}

            %--  object of study  ---------------------------------------------
            %
            One benefits from the intuition that stochastic gradient descent
            (SGD) approximates deterministic gradient descent (GD)
            \citep{bo91}.  This paper refines that intuition by showing how
            gradient noise biases learning toward certain (high curvature, high gradient-noise) areas of weight
            space.

            %--  vs ode and sde  ----------------------------------------------
            %
            Departing from prior work, we model discrete time and correlated
            gradient noise.  Indeed, we derive corrections to continuous-time,
            independent noise approximations such as ordinary and stochastic
            differential equations (ODE, SDE: \cite{li18}).
            %--
            Our corrections lead to qualitative differences in dynamics: for
            example, we construct a non-pathological\footnote{%
                All higher derivatives exist and are quadratically bounded; the
                gradient noise at each weight vector is $1$-subgaussian.%
            } loss landscape on which SGD's trajectory perpetually
            \emph{circulates} clockwise; slightly modifying this
            landscape leads SGD to perpetually \emph{ascend}.
            %--
            These examples, though artificial, demonstrate how drastically
            standard intuitions about SGD may fail.  We argue that our theory's
            quantitative results enhance practical intuitions, and we verify
            our theory on convolutional CIFAR-10 and Fashion-MNIST loss
            landscapes.

            %--  soft benefits: retrospective  --------------------------------
            %
            \begin{wrapfigure}{r}{0.40\textwidth}
                \centering  
                \vspace{-0.30cm}
                \plotmoow{diagrams/paradigm}{0.99\linewidth}{}\vspace{-0.10cm}
                \caption{
                    \textbf{An information-flow process}.  The $5$th datum 
                    participates in the $2$nd SGD update.  This
                    {\color{spacetimepurple}$(n=5,t=2)$ event} affects the
                    testing loss both directly and via the
                    {\color{spacetimeteal}$(1,12)$ event}, which is also
                    modulated by the {\color{spacetimeindigo}$(2,5)$
                    event}. 
                }\vspace{+0.60cm}
                \label{fig:paradigm}
            \end{wrapfigure}
            At a high level, we Taylor-expand the testing loss as a function of
            the learning rate $\eta$.  We tame a combinatorial explosion of
            terms and some large-time convergence difficulties by
            interpreting the terms in terms of
            information-flow processes.  For example, an
            instance of the process\footnote{
                Throughout, colors help us refer to parts of diagrams; they
                lack mathematical meaning.
            }
                \vspace{-0.35cm}
            $$
                \mdia{MOOc(01-2-3-4)(04-13-23-34)}
                \vspace{-0.50cm}
            $$
            is shown on the right.
            %We depict these processes with diagrams such as
            %%$\sdia{c(01-2-3)(02-12-23)}$
            %$\mdia{MOOc(01-2-3-4)(04-13-23-34)}$
            %analogous to those of \cite{fe49,pe71}.%\footnote{%
            %%    E.g.\ the diagram here shows a red update doubly
            %%    affecting a green update that in turn affects the final testing
            %%    loss.  The diagrams with few edges suffice for leading
            %%    order results.  Colors help us refer to diagram parts; they
            %%    lack meaning.
            %%}
            %%
            %%We may thus compute expected testing losses by
            %%summing small sets of diagrams.
            We develop a diagram notation for such processes, 
            explain how to compute the effect of each process, and show
            that summing the finitely many processes with fewer than $d$ edges
            suffices to answer dynamical questions to error $o(\eta^d)$.
            %
            Our analysis thus offers a new physics-inspired perspective of SGD
            as a superposition of many concurrent information-flow processes.

        \newpage
        \subsection{Background, notation, assumptions}

            %--  the landscape  -----------------------------------------------
    
            We fix a \emph{testing loss} function $l:\Mm\to\RR$ on a space
            $\Mm$ of weights $\theta$.  We fix a distribution $\Dd$ from which
            unbiased estimates $l_x$ of $l$ are drawn.  We write $(l_n:
            0\leq n<N)$ for a training sequence drawn i.i.d.\ from $\Dd$.  We
            refer to $n$ and to $l_n$ as \emph{training points}.  We assume
            \S\ref{appendix:assumptions}'s hypotheses, e.g.\ that $l, l_x$ are
            analytic and that all moments exist.
            %
            %--  specialization to a common case  -----------------------------
            %
            For instance, our theory accounts for $\tanh$ networks with cross
            entropy loss on bounded data --- and with weight sharing, skip
            connections, soft attention, dropout, and weight decay.  But it
            does not model $\text{ReLU}$ networks.

            %--  names of sgd parameters  -------------------------------------

            SGD performs $\eta$-steepest descent on the estimates $l_n$.  Our
            theory describes SGD with any number
                $\mathbf{N}$ of training points,
                $\mathbf{T}$ of updates, and 
                $\mathbf{B}$ of points per batch.
            Specifically, SGD runs $T$ many updates (hence
                $\mathbf{E}=TB/N$ epochs or
                $\mathbf{M}=T/N$ updates per training point) of the form
            %    \colorbox{moolime}{$\mathbf{N}$ of training points},
            %    \colorbox{moolime}{$\mathbf{T}$ of updates}, and 
            %    \colorbox{moolime}{$\mathbf{B}$ of points per batch}.
            %Specifically, SGD runs $T$ many updates (hence
            %    \colorbox{moolime}{$\mathbf{E}=TB/N$ epochs} or
            %    \colorbox{moolime}{$\mathbf{M}=T/N$ updates per training point}) of the form
            $$
                \textstyle
                \theta^\mu
                \coloneqq
                \theta^\mu -
                \eta^{\mu\nu} \nabla_\nu
                    \sum_{n\in \Bb_t} l_n(\theta) / B
            $$
            where in each epoch, we sample the $t$th batch $\Bb_t$ without
            replacement from the training sequence.  So each initialization
            $\theta_0 \in \Mm$ induces a distribution over
            trajectories $(\theta_t: 0\leq t \leq T)$, with randomness due both
            to training data and batch selection.  We shall especially study
            the \emph{final testing loss} $\expc[l(\theta_T)]$.

            %--  tensor conventions  ------------------------------------------

            \begin{wrapfigure}{r}{0.4\textwidth}
                \vspace{-0.3cm}
                \begin{tabular}{lclcl}
                    $G$ &$=$& $\ex{\nb\lx}        $ &$\leftrightsquigarrow$& $\mdia{MOO(0)(0)}       $\\
                    $H$ &$=$& $\ex{\nb\nb\lx}     $ &$\leftrightsquigarrow$& $\mdia{MOO(0)(0-0)}     $\\ 
                    $J$ &$=$& $\ex{\nb\nb\nb\lx}  $ &$\leftrightsquigarrow$& $\mdia{MOO(0)(0-0-0)}   $\\
                    $C$ &$=$& $\ex{(\nb\lx - G)^2}$ &$\leftrightsquigarrow$& $\mdia{MOOc(01)(0-1)}   $\\
                    $S$ &$=$& $\ex{(\nb\lx - G)^3}$ &$\leftrightsquigarrow$& $\mdia{MOOc(012)(0-1-2)}$
                \end{tabular}
                \vspace{-0.5cm}
                \caption*{
                    \textbf{Above}: Named tensors, typically evaluated at
                    initialization ($\theta=\theta_0$).
                    \S\ref{sect:diagrams} explains how diagrams depict tensors.
                }
                \vspace{-0.2cm}
            \end{wrapfigure}
            Our analysis makes heavy use of the tensors defined to the right:
            $G, H, J; C, S$ have $1, 2, 3; 2, 3$ indices, respectively.  We
            shall implicitly sum repeated Greek indices: if a covector $U$ and
            a vector $V$\footnote{
                Vectors/covectors, a.k.a.\ column/row vectors,
                represent distinct geometric concepts \citep{ko93}. 
            } have coefficients $U_\mu, V^\mu$, then 
            $
                U_\mu V^\mu
                \triangleq
                \sum_\mu U_\mu \cdot V^\mu
            $.
            We regard the learning rate as an inverse metric $\eta^{\mu\nu}$
            that converts gradient covectors to displacement vectors
            \citep{bo13}.  We use the learning rate $\eta$ to raise indices;
            thus,
            $
                H^{\mu}_{\lambda}
                \triangleq
                \sum_{\nu} 
                \eta^{\mu\nu} H_{\nu\lambda}
            $ and
            $
                C^{\mu}_{\mu}
                \triangleq
                \sum_{\mu \nu} \eta^{\mu\nu} \cdot C_{\nu\mu}
            $.
            A quantity $q$ \emph{vanishes to order $\eta^d$} when
            $\lim_{\eta\to 0} q/p(\eta) = 0$ for some homogeneous degree-$d$
            polynomial $p$; we then say $q\in o(\eta^d)$.

            %--  example  -----------------------------------------------------

            To illustrate our notation, we quote a well-known proposition
            (\cite{ne04}, \S 2.1):
            \begin{prop}\label{prop:nest}
                $G$ controls the leading order loss decrease:
                $
                    \expc[l(\theta_T) - l(\theta_0)] \in
                    - 
                    T G_\mu G^\mu
                    + o(\eta^1)
                $.
            \end{prop}
            One proves this estimate by induction on $T$.  When the loss
            landscape is noiseless and linear (that is, when $\nabla
            l_x(\theta)$ depends on neither $x$ nor $\theta$), this estimate is
            exact.

            This paper's contributions are two-fold: first, to identify how
            gradient noise and curvature correct Proposition \ref{prop:nest};
            and second, to replace induction by less opaque and more convergent
            large-$T$ techniques.
            %
            For example, our framework allows us to assess how gradient noise's
            non-Gaussianity affects the final testing loss.
            \S\ref{sect:diagrams} details how evaluation of a
            \emph{single diagram}
                \vspace{-0.15cm}
            $$
                \sdia{c(012-3)(03-13-23)}
                \vspace{-0.45cm}
            $$
            gives the leading order result (Corollary \ref{cor:vsode}), quoted
            here assuming isotropic curvature ($\eta H \propto I$): 
            \begin{prop}\label{prop:splash}
                If we initialize near an non-degenerate minimum of $l$, then in
                the large-$T$ limit, the skewness $S$ of gradient noise
                contributes 
                $
                    - S_{\alpha\beta\gamma}
                    J^{\alpha\beta\gamma} / 18 \|\eta H\|_2 + o(\eta^2)
                $
                to the final testing loss.  
            \end{prop}
            So skewness affects loss in proportion to the logarithmic
            derivative $J/H$ of curvature.  The dependence on $\eta$
            is second order\footnote{
                Three $\eta$s raise $J$'s indices; one $\eta$ appears in the
                denominator $18 \|\eta H\|$.
            } and is hence a leading correction
            to Proposition \ref{prop:nest}.  Gaussian approximations (e.g.\
            SDE) miss this effect. 

    \section{Perturbative theory of SGD}

        \S\ref{sect:exegesis} illustrates by example a Taylor series approach
        to studying SGD dynamics.  \S\ref{sect:diagrams} introduces a diagram
        notation for the formulae that thus appear.  \S\ref{sect:using} shows
        how to evaluate diagrams to obtain numbers.  \S\ref{sect:main} states
        our main result: that diagram-based computation is correct.

        \subsection{Challenges when using Taylor series to study SGD}\label{sect:exegesis}
            We analyze Proposition \ref{prop:nest} to highlight our points of
            departure.
            %
            Intuitively, each step of SGD displaces $\theta$ by $-\eta G$;
            $T$ steps displace $\theta$ by $-T\eta G$.  Since $l$ rises by $Gv$
            for each displacement $v$, the overall change in $l$ is $-GT\eta
            G$.  We follow \cite{ne04,ro18} to rigorize this intuition:
            \begin{proof} (of Proposition \ref{prop:nest}).
                By the smoothness assumptions of \S\ref{appendix:assumptions},
                we have $\theta_T^\mu - \theta_0^\mu \in O(\eta^1)$ for each
                $T$.  We claim that $\theta_T^\mu - \theta_0^\mu \in
                -T\eta^{\mu\nu}G_\nu + o(\eta^1)$.
                %
                The claim holds when SGD is run for $T=0$ timesteps.  Moreover,
                if $T = T^\pr+1$ and the claim holds when SGD is run for
                $T^\pr$ timesteps, then: 
                \begin{align*}
                    \theta_{T} - \theta_{T^\pr}
                    &= - \eta \nabla l_{T^\pr}(\theta_{T^\pr}) \\
                    &\in - \eta \nabla (l_{T^\pr}(\theta_0) + \text{\colorbox{moosky}{$\nabla l_{T^\pr}(\theta_0) \cdot (\theta_{T^\pr} - \theta_0)$}} + o(\theta_{T^\pr} - \theta_0)) \\ 
                    &\subseteq - \eta \nabla (l_{T^\pr}(\theta_0) + \nabla l_{T^\pr}(\theta_0) \cdot O(\eta^1) + o(O(\eta^1))) \\
                    &\subseteq \text{\colorbox{moolime}{$- \eta \nabla l_{T^\pr}(\theta_0)$}} + o(\eta^1)
                \end{align*}
                Here, we write $l_{T^\pr}$ as shorthand for the batch average
                $\sum_{n\in \Bb_{T^\pr}} l_n(\theta) / B$.  Applying the induction
                hypothesis proves the claim.
                %
                Finally, we plug the claim into a Taylor expansion of $l$: 
                \begin{align*}
                    \expc[l(\theta_T) - l(\theta_0)]
                    &= \nabla l(\theta_0) \cdot \text{\colorbox{moopink}{$\expc[\theta_T - \theta_0]$}} + \expc[o(\theta_T - \theta_0)] \\
                    &\in \nabla l(\theta_0) \cdot (-T\eta G + o(\eta^1)) + o(O(\eta^1)) \\
                    &= \text{\colorbox{moogold}{$-T^1G\eta G$}}+ o(\eta^1) \hfill \square
                \end{align*}
                By the moment assumptions of \S\ref{appendix:assumptions}, the
                above expectations of $o(\eta^1)$ terms are still $o(\eta^1)$.
            \end{proof}
            This paper extends the above by keeping higher order terms.
            We emphasize three features that the above shares with its na\"ive
            generalizations: the ``\emph{transparency}'' of future updates to
            old effects, the combinatorial \emph{explosion} of intermediate
            terms, and the results' \emph{polynomial} dependence on $T$.

            By \textbf{transparency}, we mean that to first order the $t$th
            update contributes the same amount (namely, $-\eta \expc[\nabla
            l_{t}(\theta_0)]$) in expectation to
            $\theta_T$ no matter by how much $T$ exceeds $t$ (indeed, in the
            above, \colorbox{moolime}{$- \eta \nabla l_{T^\pr}(\theta_0)$} is
            constant with respect to $\theta_t$ for $t>0$). 
            %
            Consequently, we may imagine each update as independently affecting
            the final test loss.  
            %
            This independence may be surprising, for it fails when we expand to
            higher order and hence account for curvature: SGD on a bounded
            landscape does not descend indefinitely at a constant rate. 
            %
            Still, for higher orders $d$ of expansion there is an
            analogue of independence that finds precise expression in Theorem
            \ref{thm:resum}.  The intuition that arises is that of many
            independent processes occuring, each blind to all but $d$ updates.

            Expanding to higher order, we encounter an \textbf{explosion} of
            terms.  To order $\eta^2$, for instance, we may not
            discard the hessian term \colorbox{moosky}{$\eta\nabla\nabla
            l_{T^\pr}(\theta_0) \cdot (\theta_{T^\pr} - \theta_0)$}.  The order
            $\eta^1$ contributions of all updates prior to $T^\pr$ thus each
            contribute via $\theta_{T^\pr}$ to this undiscarded term.  So,
            while the proof above sums $O(T)$ terms, an order $\eta^2$ analysis
            sums $O(T^2)$ terms, each (e.g.\ $\nabla\nabla l_{t=5} \nabla
            l_{t=2}$) involving a \emph{pair} of times.  The terms vary in
            form, too: some (perhaps $\nabla\nabla l_{t=5} \nabla l_{t=2}$) of
            the quadratic terms that replace \colorbox{moopink}{$\expc[\theta_T
            - \theta_0]$} have statistically independent factors that permit
            expectations to factor, while others (perhaps $\nabla\nabla l_{t=5}
            \nabla l_{t=5}$) do not.  This is how covariances of noise appear
            in our analysis.

            A routine induction on $(d,T)$ characterizes this explosion: for
            each $d$, there are $O(T^d)$ many terms when we expand to order
            $\eta^d$.  Thus the result most analogous to Proposition
            \ref{prop:nest}'s result estimates the expected final testing loss
            as a degree-$d$ \textbf{polynomial} in $T$.  For instance,
            \colorbox{moogold}{$-T^1G\eta G$} is degree $1$ in $T$.  Though
            polynomial dependence is correct for fixed $T$ and $\eta$
            sufficiently small relative to $T$, it is absurd when we fix $\eta$
            and vary
            $T$:\footnote{%
                The $\eta$ expansion's domain of convergence
                depends on $T$, so we do not expect $\lim_{\eta\to 0}$
                and $\lim_{T\to\infty}$ to commute.%
            }
            for example, SGD on a bounded loss function should not lead to 
            unbounded loss as $T$ grows.  By contrast, our
            Theorems \ref{thm:resum}, \ref{thm:converge} produce expressions
            that for each $T$ have the correct Taylor data with respect to
            $\eta$ and that for each $\eta$ are bounded as $T$ grows.  We
            achieve this result by cancelling the aforementioned polynomial
            divergences at each order with higher order terms.  We perform this
            \textbf{resummation} without omission or redundancy so
            that absolute convergence ensures consistency with the un-resummed
            result.  What we find is that 
            %resummation replaces ``\emph{transparency}'' by
            %``\emph{translucence}'':
            a term that involves updates
            separated in time decays exponentially in the separation.

        \subsection{Diagrams arise from Taylor series and depict information-flow processes}\label{sect:diagrams}

            We introduce diagrams to exploit transparency, tame the
            combinatorial explosion of terms, and temper the aforementioned
            polynomial divergence.
            %
            First, we characterize the higher order terms.  Suppose
            $s$ is an analytic function on $\Mm$.  For example, $s$ might be
            the testing loss $l$.  The following Lemma, reminiscent of
            \cite{dy49a}'s, tracks $s(\theta)$ as SGD updates $\theta$:
            \begin{klem*} \label{lem:dyson}
                For all $T$: for $\eta$ sufficiently small, $s(\theta_T)$ is a
                sum over tuples of natural numbers:
                \begin{equation}\label{eq:dyson}
                    \sum_{(d_t: 0\leq t<T) \in \NN^T}
                    (-\eta)^{\sum_t d_t}
                    \wrap{
                        \prod_{0 \leq t < T}
                            \wrap{\left.
                                \frac{(g \nabla)^{d_t}}{d_t!}
                            \right|_{g = \sum_{n\in \Bb_t} \nabla l_n(\theta) / B}}
                    }(s) (\theta_0)
                \end{equation}
                Moreover, an expectation symbol (over training sets) commutes
                with the sum over $d$s.
            \end{klem*}
            Here, we consider each $(g \nabla)^{d_t}$ as a higher order
            function that takes in a function $f$ defined on weight space and
            outputs a function equal to the $d_t$th derivative of $f$, times
            $g^{d_t}$.  The above product then indicates composition of $(g
            \nabla)^{d_t}$'s across the different $t$'s.  In total, that
            product takes the function $s$ as input and outputs a function
            equal to some polynomial of $s$'s derivatives.

            For example, the $\eta^3$ terms that appear in the above
            (for $s=l$) include:
            $$
                -\nabla_{\mu} l_{t=2} \nabla_{\nu} l_{t=2}
                 \nabla^{\mu}\nabla^{\nu}\nabla_{\lambda} l_{t=5}
                 \nabla^{\lambda} l
                %
                \hspace{2cm}
                %
                -\nabla_{\mu} l_{t=2} \nabla_{\lambda} l_{t=2}
                 \nabla^{\mu}\nabla_{\nu} l_{t=5}
                 \nabla^{\nu}\nabla^{\lambda} l
            $$
            Let us take expectations over training sets.  Suppose $B=1$ and
            $N>5$ so that the batches at $t=2,5$ are statistically independent.
            Then the expectations factor as: 
            \begin{align*}
               -\expc[\nabla_{\mu} l_{t=2} \nabla_{\nu} l_{t=2}]
                \expc[\nabla^{\mu}\nabla^{\nu}\nabla_{\lambda} l_{t=5}]
                \expc[\nabla^{\lambda} l]
                &&
               -\expc[\nabla_{\mu} l_{t=2} \nabla_{\nu} l_{t=2}]
                \expc[\nabla^{\mu}\nabla_{\lambda} l_{t=5}]
                \expc[\nabla^{\nu}\nabla^{\lambda} l] \\
               = -(GG+C)_{\mu\nu}J^{\mu\nu}_{\lambda}G^{\lambda} 
                && 
               = -(GG+C)_{\mu\nu}H^{\mu}_{\lambda} H^{\nu\lambda} \\
                \leftrightsquigarrow -\uvalue(\sdia{c(01-2-3)(02-12-23)})
                && 
                \leftrightsquigarrow -\uvalue(\sdia{c(01-2-3)(02-13-23)})
            \end{align*}
            We associate tensors with diagrams.\footnote{
                We write $\leftrightsquigarrow$ instead of $=$ because diagrams
                evaluate to products of cumulants $C$ rather than of moments
                $GG+C$.
            }
            \textbf{uvalue} stands for ``un-resummed value'', defined as a product
            containing a $d$th derivative of $l_x$ for each degree-$d$ node,
            grouped under expectation symbols per the diagram's gray
            outlined ties, and tensor-contracted per the diagram's
            black edges.
            %
            In fact, (\ref{eq:dyson}) is a weighted sum of the $\uvalue$s of
            all diagrams formally defined below.  The following examples of
            valid and invalid diagrams supply an informal sense sufficient to
            read this paper's body.
            \begin{dfn}[\S\ref{appendix:toward-diagrams}]
                A \textbf{diagram} is a rooted tree equipped with a partition
                of its non-root nodes.  We draw the tree structure using black
                edges, oriented left-to-right so that children precede parents.
                We depict the partition structure using minimally many gray
                outlined ties.  
            \end{dfn}
            \begin{table}[h!]
                \centering
                \vspace{-0.5cm}
                \adjustbox{width=\textwidth}{
                \begin{tabular}{ccc}
                    valid diagrams &\hspace{0.2cm}& invalid diagrams \\ \hline
                    $\mdia{c(0-1)(01)},
                    \mdia{c(0-1-2)(02-12)},
                    \mdia{c(012-3)(02-12-23)},
                    \mdia{c(01-2-3)(02-12-23)},
                    \mdia{c(0-1-2-3)(01-12-23)}, 
                    \dia{MOOc(03-12-4)(03-13-23-34)}$
                        &\hspace{0.2cm}&
                    $\mdia{MOOc(01)(0-1)},  
                     \mdia{MOOc(01)(01)}, 
                     \dia{MOOc(0-1-2)(01-02-12)},
                     \dia{MOOc(0-1-2)(01-02)}, 
                     \dia{MOOc(0-1-2)(02)},$
                     \adjustbox{trim=0 0 {.5\width} 0}{$\dia{MOOc(02-012-3)(02-12-23)}$}
                     ~~
                \end{tabular}
                }
                \vspace{-0.5cm}
            \end{table}
            Since a diagram is just a rooted tree and partition,
            $
                \sdia{c(01-2-3)(01-13-23)} = 
                \sdia{c(02-1-3)(02-13-23)} = 
                \sdia{c(0-12-3)(03-12-23)} 
            $ are the same diagrams.

            There are dozens of small diagrams.  In many analyses, only
            a few diagrams are relevant.
            Indeed, for fixed $T$, \textbf{to order $\eta^d$ we may
            neglect all diagrams with more than $d$ edges}.
            %
            If $E=B=1$, then a diagram with an ancestor-descendant pair in the
            same part contributes zero.  For SGD initialized at a minimum of
            $l$, all diagrams vanish that contain a leaf participating in no
            gray tie.

        \subsection{Diagrams overcome the challenges of using Taylor series to study SGD}\label{sect:using}

            %-----  embeddings and patterns of influence  ---------------------
            %\subsubsection{Embeddings}
            Having re-expressed (\ref{eq:dyson}) as a weighted sum of uvalues
            of diagrams, we just need to compute the coefficient for each
            uvalue.  We find that a diagram's coefficient counts certain
            assignments of a diagram's non-root nodes to update events at
            concrete times $t$.
            %
            For example, $\sdia{(0-1)(01)}$ has just one non-root node and
            hence $T$ many assignments.  Since $\sdia{(0-1)(01)}$ is the only
            diagram with one edge, it gives the full $\eta^1$ contribution to
            final testing loss.  We immediately obtain Proposition
            \ref{prop:nest}:
            $$
                -(\text{\# of assigments for~}\sdia{(0-1)(01)}) \cdot \uvalue(\sdia{(0-1)(01)}) 
                =
                -T \cdot G_\mu G^\mu 
            $$

            Specifically, we count \emph{embeddings}, defined below with
            respect to given SGD hyperparameters $N,E,B$.\footnote{
                and w.r.t.\ a deterministic algorithm for sampling the
                $t$th batch.  One may later take expectations over such
                algorithms.
            }
            This enables us to restate (\ref{eq:dyson}) into what intuitively
            is a ``sum over histories'', where each history is a concrete
            instantiation of the abstract process represented by a diagram: 
            \begin{dfn}
                An \textbf{embedding} of a diagram is an assignment of non-root
                nodes to $\emph{(n,t)}$ pairs such that: the $\emph{n}$th
                training point participates in the $t$th batch; parents'
                $\emph{t}$s strictly exceed their children's $\emph{t}$s; and
                any two nodes' $\emph{n}$s are equal if and only if the nodes
                are in the same part of the partition. 
            \end{dfn}
            \begin{klem*}\emph{(\textbf{restated})} %\label{thm:pathint}
                For all $T$: for $\eta$ sufficiently small, the final testing
                loss is:
                \vspace{-0.20cm}
                \begin{equation*}\label{eq:sgdcoef}
                    \sum_{\substack{D~\text{a} \\ \text{diagram}}}
                    ~
                    \sum_{\substack{f~\text{an embed-} \\ \text{-ding of}~D}}
                    ~
                        \frac{1}{\wabs{\Aut_f(D)}}
                    \frac{\uvalue(D)}{(-B)^{|\edges(D)|}}
                \end{equation*}

                \vspace{-0.40cm}
                \noindent
                Here, $\wabs{\Aut_f(D)}$ counts the graph automorphisms of $D$
                that preserve $f$.
            \end{klem*}

            %-----  isolate effect of tensors; crossing symmetries  -----------
            This restatement streamlines analysis of SGD because it is in practice
            straightforward to count a diagram's embeddings.
            %
            Moreover, we may
            compute the effect of the skewness of gradient noise in isolation
            by evaluating only those diagrams containing
            $\sdia{MOOc(012)(0-1-2)}$.    
            These considerations show that the $t^d T^{-p}$-th order
            correction\footnote{We compare ODE integrated to time $t$ to $T$
            steps of SGD with $\eta = \eta_\star t/T$ and $E=B=1$, and we
            assume $p\neq 0$.} to the ordinary differential equation
            approximation of SGD is given by diagrams with $d$ edges and $p$
            many gray ties.

            %We interpret edges as carrying influence from the training set
            %toward the test measurement (figure \ref{fig:paradigm}). 

            %\begin{wrapfigure}{r}{0.4\textwidth}
            %    \centering  
            %    \plotmoow{diagrams/spacetime-f}{0.99\linewidth}{}
            %    \caption{
            %        %\textbf{Edges carry information}.
            %        Embedding of $\mdia{MOOc(01-2-3-4)(04-13-23-34)}$.
            %    }
            %    \vspace{-0.20cm}
            %    \label{fig:intuition1}
            %\end{wrapfigure}

            %    Moreover, a diagram's uvalue depends only on its
            %    graph and partition structures (not on its root), so, e.g.:\footnote{The physics-oriented reader
            %    will recognize this as a \emph{crossing symmetry}.}
            %    $$\uvalue(\sdia{c(0-1-2)(01-12)})=
            %    \uvalue(\sdia{c(0-1-2)(02-12)})$$ 
            %    %These relations reduce the effort of evaluating (\ref{eq:dyson}).
            %    These relate the contribution of a process (e.g.\
            %    $\sdia{c(0-1-2)(01-12)}$, whose degree-two node temporally
            %    separates its neighbors) to a time-distorted version of the process
            %    (e.g.\ $\sdia{c(0-1-2)(02-12)}$, whose
            %    degree-two node succeeds its neighbors). 


            \newpage
            %-----  resummation  ----------------------------------------------
            \subsubsection{Resummation}
            So far, diagrams have been a convenient but dispensible
            book-keeping tool.  For instance, \S\ref{sect:exegesis}'s
            polynomial divergence remains.  We now show that the topology of
            diagrams enables correction of this divergence, thus establishing
            diagrams as a necessary concept.

            Observe that each of the following lists contains diagrams of ``the
            same'' shape:
            \vspace{-0.30cm}
            \begin{align*}
                \sdia{c(0-1)(01)},
                \sdia{c(0-1-2)(01-12)},
                \sdia{c(0-1-2-3)(01-12-23)},
                \mdia{MOOc(0-1-2-3-4)(01-12-23-34)},\cdots
                &&
                \sdia{c(01-2)(02-12)},
                \sdia{c(02-1-3)(01-13-23)},
                \mdia{MOOc(03-1-2-4)(01-12-24-34)},
                \mdia{MOOc(02-1-3-4)(01-14-23-34)},\cdots
            \end{align*}
            \vspace{-0.60cm}

            \noindent
            Intuitively, our notion of ``the same'' permits chains to grow
            or shrink.  We will express in closed form the total
            contribution to (\ref{eq:dyson}) of all diagrams in such a
            list.  This is the \emph{resummed value} of the list's smallest
            member.  The idea is that the uvalues of chains are powers of
            hessians --- for example,
            $\uvalue(\sdia{MOOc(0-1-2-3-4)(01-12-23-34)}) = GH^3G$ --- so
            we may sum over all possible chain lengths via geometric
            series.

            \begin{dfn}
                A \textbf{link} is a degree-$2$ node that participates in
                no gray ties.  To \textbf{reduce} a diagram at a link, we
                replace the link by a black edge connecting the link's two
                neighbors.  Reduction generates an equivalence relation on
                diagrams. Each equivalence class has exactly one
                \textbf{linkless} diagram.
            \end{dfn}

            \noindent
            More specifically, the \textbf{rvalue} of an embedded diagram is
            defined as the $\uvalue$ except that pairs of tensor indices are
            not contracted using $\eta^{\mu\nu}$.  Instead, whenever an
            embedded edge spans $\Delta t$ many time steps, we contract the
            two corresponding tensor indices using $(I-\eta H)^{\Delta t-1}\eta$. 

            %\begin{figure}[h!] 
            \begin{wrapfigure}{r}{0.35\textwidth}
                \centering  
                \vspace{-0.5cm}
                \dmoo{2.5cm}{spacetime-g}
                %\hfill
                %\dmoo{3cm}{spacetime-h}
                \caption{
                    \textbf{Resummation propagates information damped by
                    curvature}.
                    %\textbf{Left}:
                    Each resummed value (here, for $\sdia{c(0-1)(01)}$)
                    represents many un-resummed values, four shown here, each
                    modulated by the Hessian ($\sdia{MOOc(0)(0-0)}$) in a
                    different way.
                    %
                    %\textbf{Right}: One of many un-resummed terms
                    %captured by a single resummed term for
                    %$\sdia{c(01-2)(02-12)}$.
                    %Because two nodes appear at $(n=2,t=2)$, the process shown
                    %is an effect of the 2nd cumulant of the gradient noise
                    %distribution.
                }
                \vspace{-1.5cm}
                \label{fig:resumintuition}
            \end{wrapfigure}

        \subsection{Main result}\label{sect:main}
    
            %-----  recipe for test loss  -------------------------------------

            Theorem \ref{thm:resum} expresses SGD's testing loss as a sum over
            diagrams.  A diagram with $d$ edges scales as $O(\eta^d)$, so the
            following is a series in $\eta$.  In practice, we truncate the
            series to small $d$ (invoking Theorem \ref{thm:converge} when
            possible), thus focusing on few-edged diagrams.  Here we state a special case:
            \begin{thm} \label{thm:resum}
                For any $T$: for $\eta$ small enough, the final testing loss is
                a sum over \emph{linkless} diagrams: 
                \begin{equation*} \label{eq:resum}
                    \sum_{\substack{D~\text{a linkless} \\ \text{diagram}}}
                    ~
                    \sum_{\substack{f~\text{an embed-} \\ \text{-ding of}~D}}
                    ~
                    \frac{1}{\wabs{\Aut_f(D)}}
                    \,
                    \frac{{\rvalue_f}(D)}{(-B)^{|\edges(D)|}}
                \end{equation*}
            \end{thm}
    
            %-----  simplifications  ------------------------------------------
     
            \begin{rmk} \label{rmk:integrate}
                In practice, we approximate sums over embeddings by integrals
                over times and $(I-\eta H)^t$ by $\exp(- \eta H t)$, reducing
                to a routine integration of exponentials at the cost of an
                error factor $1 + o(\eta)$.
            \end{rmk}
    
            %-----  convergence  ----------------------------------------------
     
            \begin{thm} \label{thm:converge}
                If $\theta_\star$is a non-degenerate local minimum of $l$
                (i.e.\ $G(\theta_\star)=0$ and $H(\theta_\star) > 0$), then for
                SGD initialized sufficiently close to $\theta_\star$, the
                $d$th-order truncation of Theorem \ref{thm:resum} converges as
                $T\to \infty$.
            \end{thm}
            \par\noindent
            Caution: the $T\to \infty$ limit in Theorem \ref{thm:converge}
            might not measure any well-defined limit of SGD, since the limit
            might not commute with the infinite sum.  We see no such
            pathologies in practice, so we will freely speak of ``SGD in the
            large-$T$ limit'' as informal shorthand when referencing this
            Theorem.

    \section{Consequences of the theory}

        \begin{samepage}
        These few corollaries do not exhaust the scope of our theory.  For
        instance, \S\ref{appendix:future} briefly discusses bridges to physics
        and relations to Hessian methods and natural GD.
        \subsection{Gradient noise repels SGD}\label{subsect:epochs-batch}
        \end{samepage}
            Physical intuition suggests that noise repels SGD: if two
            neighboring regions of weight space have high and low levels of
            gradient noise, respectively, then we expect the rate at which
            $\theta$ jumps from the former to the latter to exceed the opposite
            rate.
            %
            There is thus a net movement toward regions of small $C$.\footnote{
                This is the same mechanism by which sand on a vibrating
                plate accumulates in quiet regions \citep{ch87}.  We thus dub
                the SGD phenomenon the
                \href{http://dataphys.org/list/chladni-plates/}{Chladni
                effect}.
            }
            %
            Our theory makes this intuition precise; the drift is in the
            direction of $-\nabla C$, and the effect is strongest when gradient
            noise is not averaged out by large batch sizes:
            \begin{cor}[$\sdia{c(01-2)(01-12)}$] \label{cor:batch}
                SGD with $E=B=1$ avoids high-$C$ regions more than GD:
                $
                    l_{C}
                        \triangleq
                    \frac{N-1}{4 N}
                    \nabla^\mu C^{\nu}_{\nu}
                        =
                    \expct{\theta_{GD} - \theta_{SGD}}^\mu - o(\eta^2)
                $.
                If $\hat{l_c}$ is a smooth unbiased estimator of $l_c$, then GD
                on $l + \hat{l_c}$ has an expected testing loss that agrees with
                SGD's to order $\eta^2$.  We call this method GDC.
            \end{cor}
            \noindent
            \cite{ro19} obtained a version of this Corollary with a nearly
            equal error of $O(\eta^2/N)\vee o(\eta^2)$

            An analogous form of averaging occurs over multiple epochs.  For a
            tight comparison, we scale the learning rates appropriately so
            that, to leading order, few-epoch and many-epoch SGD agree.  Then
            few-epoch and many-epoch SGD differ, to leading order, in their
            sensitivity to $\nabla C$:
            \begin{cor}[$\sdia{c(01-2)(01-12)}$] \label{cor:epochs}
                SGD with $M=1$ and $\eta=\eta_0$ avoids high-$C$ regions more
                than SGD with $M=M_0$ and $\eta=\eta_0/M_0$.  Precisely:
                $
                    \expct{\theta_{M=M_0} - \theta_{M=1}}^\mu
                        =
                    \wrap{\frac{M_0-1}{4 M_0}} N
                    \wrap{\nabla^\mu C^{\nu}_{\nu}}
                    + o(\eta^2)
                $.
            \end{cor}

            In sum, high-$C$ regions repel small-$(E,B)$ SGD more than
            large-$(E,B)$ SGD.  We thus extend the $T=2$ result of \cite{ro18}
            and resolve some questions posed therein.    

        \subsection{Non-Gaussian noise affects SGD but not SDE}
    
            Stochastic differential equations (SDE: see \cite{li18}) are a
            popular theoretical approximation of SGD, but SDE and SGD differ in
            several ways.  For instance, the inter-epoch noise correlations in
            multi-epoch SGD measurably affect SGD's final testing loss (Corollary
            \ref{cor:epochs}), but SDE assumes uncorrelated gradient updates.
            Even if we restrict to single-epoch SGD, time discretization and non-Gaussian noise lead
            SGD and SDE to respond differently to changes in curvature.
            The following
            treats SGD with $E=B=1$.   
                        %
            \begin{cor}[$\sdia{c(01-2)(02-12)}$, $\sdia{c(012-3)(03-13-23)}$] \label{cor:vsode}
                For fixed $T$, SGD's final testing loss exceeds both ODE's and
                SDE's by
                $
                    \frac{T}{2} C_{\mu\nu} H^{\mu\nu} + o(\eta^2)
                $.  The skewness of gradient
                noise contributes (we work in an eigenbasis of $\eta H$): 
                \begin{align*}
                    -\frac{\eta^3}{3!}
                    \sum_{\mu\nu\lambda}
                        S_{\mu\nu\lambda}
                        \frac{
                            1 - \exp(-T\eta (H_{\mu\mu} + H_{\nu\nu} + H_{\lambda\lambda}))
                        }{
                            \eta (H_{\mu\mu} + H_{\nu\nu} + H_{\lambda\lambda})
                        }
                        J_{\mu\nu\lambda}
                        + o(\eta^3)
                \end{align*}
                to the excess final testing loss over SDE.  This unwieldy
                expression specializes to Proposition \ref{prop:splash}.
            \end{cor}

        \subsection{SGD descends on a landscape smoothed by the current $C$}
    
            \begin{cor}[Computed from $\sdia{c(01-2-3)(02-12-23)}$]
                \label{cor:entropic}
                Run SGD for $T \gg 1/\eta H$ from a non-degenerate test
                minimum.  Written in an eigenbasis of $\eta H$, $\theta$ has an
                expected displacement of
                $$
                    - \frac{\eta^3}{2}
                    \sum_{\mu\nu}
                        C_{\mu\nu}
                        \frac{1}{\eta (H_{\mu\mu} + H_{\nu\nu})}
                        J_{\mu\nu\lambda}
                        \frac{1}{H_{\lambda\lambda}}
                    + o(\eta^2)
                $$
            \end{cor}

            %Intuitively, $D = \sdia{c(01-2-3)(02-12-23)}$ connects the
            %subdiagram $\sdia{c(01-2)(02-12)} \propto CH$, via an extra edge on
            %the green node (an extra $\nabla$ on $H$), to $D$'s degree-$1$
            %root, $G$.  By l'H\^opital,\footnote{
            %    %
            %    Roughly:
            %    if a displacement $\Delta\theta$ grows loss by $G C\nabla H$
            %    nats, and by $G$ nats per foot, then $\Delta \theta$ is
            %    $C\nabla H$ feet.
            %    %
            %} the displacement is $\propto -C\nabla H$.  
            We see that the displacement scales as $-C\nabla H$.
            That is, SGD moves
            toward minima that are flat \emph{with respect to} $C$ (Figure
            \ref{fig:cubicandspring}\offive{0}).
            %
            Taking limits to drop the non-degeneracy hypothesis, we expect
            \emph{sustained} motion toward flat regions in a valley of minima.
            In avoiding \cite{we19b}'s assumptions of constant $C$, we find
            that SGD's velocity field is typically non-conservative, i.e.\ has
            curl (\S\ref{subsect:entropic}).  Indeed, $\nabla(CH)$ is a total
            derivative but $C\nabla H$ is not.  Since, by low-pass filter
            theory, $CH/2+o(C)$ is the loss increase upon convolving $l$ with a
            $C$-shaped Gaussian, we say that SGD descends on a $C$-smoothed
            landscape that changes as $C$ does.
            %
            Our $T\gg 1$ result is $\Theta(\eta^2)$, while \cite{ya19b}'s
            similar $T=2$ result is $\Theta(\eta^3)$.  Indeed, our analysis
            integrates the noise over many updates, hence amplifying $C$'s
            effect.  Experiments verify our law.
      
            %So SGD descends on a $C$-smoothed landscape and prefers 
            %minima flat with respect to $C$.

            \begin{figure}[h!]
                \centering
                \plotmooh{colt/cubic}{}{0.35\columnwidth}
                \hfill  
                \plotmooh{colt/spring}{}{0.35\columnwidth}
                \caption{%
                    \textbf{Geometric intuition for curvature-noise interactions.}
                    \textbf{Left}:
                        Gradient noise pushes SGD toward flat minima
                        (Corollary
                        \ref{cor:entropic}).
                        Gray lines sketch a two-dimensional loss landscape near
                        a valley of minima.
                        %
                        Red densities show the 
                        typical $\theta$s, perturbed by noise $C$ from the
                        minimum due, in two cross sections of the
                        loss valley.  $J = \nabla H$ measures
                        curvature's change across the valley.  Our theory
                        does not assume separation between ``fast'' and
                        ``slow'' modes, but we label them here to
                        ease comparison with \cite{we19b}.
                        Compare with Figure \ref{fig:archimedes}.
                    \textbf{\bf Right}:
                        Both curvature and noise structure affect overfitting.
                        In each subplot, the  $\leftrightarrow$
                        axis represents weight space and the $\updownarrow$
                        axis represents loss.  Noise (blue) transforms
                        the testing loss (thin curve) into the observed loss
                        (thick curve).  Red dots mark the testing loss at the
                        arg-min of the observed loss.  \protect\offive{1}:
                        \emph{covector}-perturbed landscapes favor large $H$s.
                        \protect\offive{2}: \emph{vector}-perturbed landscapes
                        favor small $H$s.  SGD's implicit regularization
                        interpolates between these rows (Corollary
                        \ref{cor:overfit}).
                }
                \label{fig:cubicandspring}
            \end{figure}



        \subsection{Both flat and sharp minima overfit less}
            \label{subsect:curvature-and-overfitting}%

            Intuitively, sharp minima are robust to slight changes in the
            average \emph{gradient} and flat minima are robust to slight
            \emph{displacements} in weight space (Figure
            \ref{fig:cubicandspring}\protect\offive{12}).  However, as SGD by
            definition equates displacements with gradients, it may be unclear
            how to reason about overfitting in the presence of curvature.
            %
            Our theory, by (automatically) accounting for the implicit
            regularization of fixed-$T$ descent, shows that both effects play
            a role.  In fact, by routine calculus on the left hand side of
            Corollary \ref{cor:overfit}, overfitting is maximized for medium
            minima with curvature $H \sim (\eta T)^{-1}$.
            %
            \begin{cor}[from $\sdia{c(01-2)(02-12)}$, $\sdia{c(01)(01)}$]\label{cor:overfit}
                Initialize GD at a non-degenerate test minimum $\theta_\star$.
                The overfitting (testing loss minus $l(\theta_\star)$) and generalization
                gap (testing minus training loss) due to training are:
                $$
                    \wrap{\frac{C/N}{2H}}_{\mu\nu}^{\rho\lambda} ~
                        \wrap{(I - \exp(-\eta T H))^{\otimes 2}}^{\mu\nu}_{\rho\lambda}
                        + o(\eta^2)
                    ~~~~~ ; ~~~~~
                    \wrap{\frac{C/N}{H}}_{\mu\nu}^{\mu\lambda} ~
                        \wrap{I - \exp(-\eta T H)}^{\nu}_{\lambda}
                        + o(\eta)
                $$
            \end{cor}
            The generalization gap tends  
            to $C_{\mu\nu}(H^{-1})^{\mu\nu}/N$ as $T\to\infty$.  For maximum
            likelihood (ML) estimation in well-specified models near the ``true''
            minimum, $C=H$ is the Fisher metric, so we recover the AIC:
            $(\textnormal{model dimension})/N$.  Unlike AIC, our more general
            expression is descendably smooth, may be used with MAP or ELBO tasks
            instead of just ML, and does not assume a well-specified model.
    
   
    \section{Experiments}
        Despite the convergence results in Theorems \ref{thm:resum} and
        \ref{thm:converge}, we have no theoretical bounds for the domain and
        rate of convergence.  Instead, we test our predictions by experiment.
        We perceive support for our theory in drastic rejections of the null
        hypothesis.  For instance, in Figure \ref{fig:vanilla}\ofsix{4},
        \cite{ch18} predict a velocity of $0$ while we predict a velocity of
        $\eta^2/6$.
        %
        Here, \texttt{I} bars, \texttt{+} signs, and shaded regions all mark
        $95\%$ confidence intervals based on the standard error of the mean.
        \S\ref{appendix:experiments} describes neural architectures, of
        artificial landscapes, sample sizes, and further plots.

        \subsection{Training time, epochs, and batch size; $C$ repels SGD more
        than GD}
            %-----  vanilla sgd  ---------------------------------------------- 
            We test Theorem \ref{thm:resum}'s order $\eta^3$ truncation on
            smooth convnets for CIFAR-10 and Fashion-MNIST.  Theory agrees with
            experiment through timescales long enough for accuracy to increase
            by $0.5\%$ (Figure \ref{fig:vanilla}\ofsix{0},\ofsix{1}).
            %-----  epochs and overfitting  -----------------------------------
            \S\ref{appendix:figures} supports Corollary \ref{cor:epochs}'s
            predictions about epoch number.
            %-----  emulating small batches with large ones  ------------------
            Figure \ref{fig:vanilla}\ofsix{2} tests Corollary \ref{cor:batch}'s
            claim that, relative to GD, high-$C$ regions \emph{repel} SGD.
            This is significant because $C$ controls the rate at which the
            gap (testing minus training loss) grows (Corollary
            \ref{cor:overfit}, Figure \ref{fig:vanilla}\ofsix{3}).
            \begin{figure}[h!] 
                \centering
                \pmoo{3.85cm}{neurips-test-small} \hfill \pmoo{3.85cm}{new-big-bm-new}          \hfill \pmoo{3.85cm}{neurips-thermo-linear-screw}
                \pmoo{3.85cm}{neurips-test-large} \hfill \pmoo{3.85cm}{neurips-gen-cifar-lenet} \hfill \pmoo{3.85cm}{neurips-tak}
                \caption{
                    {\bf Experiments on natural and artificial landscapes.}
                    \texttt{rvalue} refers to Theorem \ref{thm:resum}'s
                    predictions, approximated as in Remark \ref{rmk:integrate}.
                    %\texttt{uvalue} indicates polynomial approximations
                    %to Theorem \ref{thm:resum}'s result (see
                    %\S\ref{appendix:sum-embeddings}).
                    \texttt{uvalue}s are
                    simpler but (see\protect\ofsix{4})
                    less accurate.
                    %%%
                    %%%
                    \newline
                    {\bf Left: Perturbation models SGD for small $\eta
                    T$.} Fashion-MNIST convnet's testing loss vs learning rate.
                    In this small $T$ setting, we choose to use our theory's
                    simpler un-resummed values
                    (\ref{appendix:evaluate-embeddings}) instead of the more
                    precise $\rvalue$s.
                    %
                    \protect\ofsix{0}: For all initializations tested ($1$
                    shown, $11$ unshown), the order $3$ prediction agrees with
                    experiment through $\eta T \approx 10^0$, corresponding to
                    a decrease in $0\mbox{-}1$ error of $\approx 10^{-3}$.
                    %
                    \protect\ofsix{1}: For large $\eta T$, our predictions
                    break down.  Here, the order $3$ prediction holds until the
                    $0\mbox{-}1$ error improves by $5\cdot 10^{-3}$.  Beyond
                    this, $2$nd order agreement with experiment is
                    coincidental.  
                    %%%
                    %%%
                    \newline
                    {\bf Center: $C$ controls generalization gap.}%.and distinguishes GD from SGD.}
                    %
                    With equal-scaled axes, \protect\ofsix{2} shows that GDC
                    matches SGD (small vertical variance) better than GD
                    matches SGD (large horizontal variance) in testing loss for
                    a range of $\eta$ ($\approx 10^{-3}-10^{-1}$) and initializations
                    (zero and several Xavier-Glorot trials) for logistic
                    regression and convnets.  Here, $T=10$. 
                    %
                    \protect\ofsix{3}: CIFAR-10 generalization gaps.  For all
                    initializations tested ($1$ shown, $11$ unshown), the degree-$2$
                    prediction agrees with experiment through $\eta T \approx
                    5\cdot 10^{-1}$.
                    %%%
                    %%%
                    \newline
                    {\bf Right: Predictions near minima excel for
                    large $\eta T$.} \protect\ofsix{4}: SGD traverses \Helix'
                    valley of global minima. %in the positive $z$ direction.
                    Note: $H$ and $C$ are bounded across the valley, we see
                    drift for all small $\eta$, and we see displacement
                    exceeding the landscape's period of $2\pi$.  So: the drift
                    is not a pathology of well-chosen $\eta$, of divergent
                    noise, or of ephemeral initial conditions.
                    %
                    \protect\ofsix{5}: For \MeanEstimation\, with fixed $C$ and
                    a range of $H$s, initialized at the truth, the testing
                    losses after fixed-$T$ GD are smallest for very sharp and
                    very flat $H$.  Near $H=0$, our predictions improve on AIC,TIC
                    \citep{di18}.
                    %%%
                    %%%
                }
                \label{fig:vanilla}
            \end{figure}
        %----------------------------------------------------------------------
        %           Thermodynamic Engine                        
        %----------------------------------------------------------------------
        \subsection{Minima that are flat \emph{with respect to} $C$ attract SGD}
            %
            \label{subsect:entropic}
            To test Corollary \ref{cor:entropic}'s $C$-dependence,
            \S\ref{appendix:artificial} constructs a landscape, \Helix, on
            whose valley of global minima $C$ varies.  Figure
            \ref{fig:archimedes} depicts \Helix.\footnote{
                Thanks to Paul Seeburger's online applet,
                \href{https://www.monroecc.edu/faculty/paulseeburger/calcnsf/CalcPlot3D/}{CalcPlot3D}.
            }  As in Rock-Paper-Scissors, each point $\theta$ has a neighbor
            that is more attractive (flatter) with respect to $C(\theta)$.
            This permits eternal motion into the page despite the landscape's
            discrete translation symmetry in that direction.  In
            \S\ref{appendix:artificial}, we wrap \Helix\ in a loop to make SGD
            perpetually circulate and thus to witness a
            non-conservative velocity field.

            More precisely, Corollary \ref{cor:entropic} predicts 
            a velocity of $+\eta^2/6$ per timestep, while \cite{ch18}'s
            SDE-based analysis predicts a constant velocity of $0$.\footnote{
                Indeed, \Helix' velocity is $\eta$-perpendicular to the image
                of $(\eta C)^\mu_\nu$ in tangent space.
            }
            Our prediction agrees with experiment (Figure
            \ref{fig:vanilla}\ofsix{4}).
            %
            One hopes for an ``effective loss'' $\tilde{l}$ such that, up
            to $\sqrt{T}$ diffusion terms, SGD on $l$ mimics ODE on
            $\tilde{l}$.  The non-conservativity of SGD's velocity shows that
            no such $\tilde{l}$ exists.

            \begin{figure}[h!]
                \centering
                \rotatebox[origin=c]{-90}{\plotmooh{plots/from-above}{}{0.19\textwidth}}
                \plotmoow{colt/screw-trajectory}{0.79\textwidth}{} 
                \caption{%
                    \textbf{Leftmost}: The \Helix\ landscape is defined on
                    a three-dimensional weight space that extends
                    indefinitely into and out of the page.  A helical level
                    surface (orange-green) of $l$ winds around its axis, a
                    one-dimensional valley of minima perpendicular to the page.
                    $l$ is large outside this surface.  Gradient noise is
                    parallel to the page and tends to point from the valley
                    toward the outer two tubes.  
                    %
                    \textbf{Rightmost four}: \Helix\ induces SGD to move
                    into the page.  Green dots trace a trajectory over four
                    cross sections of weight space that descend progressively
                    into the page.  In blue are partial contour maps of $l$;
                    the valley of minima intersects each pane's dark blue
                    center.  Dotted blue curves help to compare adjacent panes.
                    Red arrows show the major axis of gradient noise in each
                    pane.  
                    %
                    \textbf{Green trajectory, explained}: 
                    Pane\protect\offour{0} shows $\theta$, initialized at the
                    dark green dot, following $l$'s gradient toward point A
                    in the next pane (deeper into the page).
                    %
                    Next (\hspace{-0.08cm}\protect\offour{1}), gradient noise
                    kicks $\theta$ from point A, leading $\theta$ to fall
                    toward point B.
                    %
                    Such falling continues in\protect\offour{2}, even
                    though the gradient noise happens to oppose the previous
                    pane's.
                    %
                    \protect\offour{3} shows $\theta$ at point C
                    kicked uphill by gradient noise; $\theta$ never settles and
                    the phenomena depicted here continue for all time.
                }
                \label{fig:archimedes}
            \end{figure}
                
        %----------------------------------------------------------------------
        %           Sharp vs Flat Minima                        
        %----------------------------------------------------------------------
    
        \subsection{Sharp and flat minima both overfit less than medium minima} \label{subsect:overfit}
    
            Prior work (\S\ref{sect:related}) finds both that SGD leads to
            overfits less near \emph{sharp} minima (for, $l^2$ regularization
            sharpens minima) or that SGD overfits less near \emph{flat} minima
            (for, flat minima are robust to small displacements).  In fact,
            both phenomena occur, and noise structure determines which
            dominates (Corollary \ref{cor:overfit}).  This effect appears even
            in \MeanEstimation\, (\S\ref{appendix:artificial}): Figure
            \ref{fig:vanilla}\ofsix{5}.
            %
            To combat overfitting, we may add Corollary \ref{cor:overfit}'s
            generalization gap estimate to $l$.  By descending on this
            regularized loss, we may tune smooth hyperparameters such as $l_2$
            regularization coefficients for small datasets ($H \ll C/N$)
            (\S\ref{appendix:figures}).  Since matrix exponentiation takes time
            cubic in dimension, this regularizer is most useful for small
            models.

    \section{Conclusion}

        %\subsection{Contributions}

            %-----  summarize contributions  ----------------------------------
    
            This paper presents a new perspective on SGD. 
            %
            We used diagrams to study stochastic optimization on short timescales
            or near minima.  Corollaries \ref{cor:entropic} and \ref{cor:overfit}
            together potentially illuminate SGD's success in training deep networks:
            SGD avoids curvature and noise, which control generalization.

            Analyzing $\sdia{c(01-2)(02-12)}$, we proved that \textbf{flat and
            sharp minima both overfit less} than medium minima.  Intuitively, flat
            minima are robust to vector noise, sharp minima are robust to covector
            noise, and medium minima robust to neither.  We thus proposed a
            regularizer enabling gradient-based hyperparameter tuning.
            %
            Inspecting $\sdia{c(01-2-3)(02-12-23)}$, we extended \cite{we19b} to
            nonconstant, nonisotropic covariance to reveal that \textbf{SGD
            descends on a landscape smoothed by the current covariance $C$}.  As
            $C$ evolves, the smoothed landscape evolves, resulting in
            non-conservative dynamics.
            %
            Examining $\sdia{c(01-2)(01-12)}$, we showed that \textbf{GD may
            emulate SGD}, as suggested by \cite{ro18}.  This is significant
            because, while small batch sizes can lead to better generalization
            \citep{bo91}, modern infrastructure increasingly rewards large batch
            sizes \citep{go18}.  

            %-----  anticipate criticism of limitations  ----------------------
    
            Since our predictions depend only on loss data near initialization,
            they break down after the weight moves far from initialization.  Our
            theory thus best applies to small-movement contexts, whether for long
            times (large $\eta T$) near an isolated minimum or for short times
            (small $\eta T$) in general.
            %
            Thus, the theory might aid future analysis of fine-tuners such as 
            \cite{fi17}'s MAML.
    
            Much as meteorologists understand how warm and cold fronts interact
            despite long-term forecasting's intractability, we quantify how
            curvature and noise contribute to counter-intuitive dynamics governing
            each short-term interval of SGD's trajectory.  Equipped with our
            theory, users of deep learning may refine intuitions --- e.g.\ that
            SGD descends on the training loss --- to account for noise.

        \subsection{Related work}\label{sect:related}
    
            %--  history of sgd  ----------------------------------------------

            \cite{ki52} united gradient descent \citep{ca47} with stochastic
            approximation \citep{ro51} to invent SGD.  Since the development of
            back-propagation for efficient differentiation \citep{we74}, SGD
            has been used to train connectionist models, e.g.\ neural networks
            \citep{bo91}, recently to remarkable success \citep{le15}.
        
            %--  analyzing overfitting; relevance of optimization; sde errs  --
        
            Several lines of work treat the overfitting of SGD-trained networks
            \citep{ne17a}.  For example, \cite{ba17} controls the Rademacher
            complexity of deep hypothesis classes, leading to
            optimizer-agnostic generalization bounds.  Yet SGD-trained networks
            generalize despite their ability to shatter large sets
            \citep{zh17}, so generalization must arise from not only
            architecture but also optimization \citep{ne17b}.  

            Some analyses of optimization's implicit regularization use a Langevin dynamics or
            SDE approximation (e.g.\ \cite{ch18}), but, per \cite{ya19a}, such
            continuous-time or uncorrelated-noise analyses treat SGD noise
            incorrectly.
            %
            %--  we extend dan's approach  ------------------------------------
            %
            We avoid these pitfalls by Taylor expanding around $\eta=0$ as in
            \cite{ro18}.  Unlike that work, we generalize beyond order $\eta^1$
            and $T=2$.  To do so, we develop new summation techniques with
            improved large-$T$ convergence.  Our interpretion of the resulting
            terms offers a new qualitative picture of SGD as a superposition of
            simpler information-flow processes.
            
            %--  double descent  ----------------------------------------------

            Other studies focusing on \emph{double descent} suggest that some
            highly overparameterized models share implicit regularization
            properties with linear least-squares models \citep{be19}, for
            example by bounding log-determinants (and hence the effective
            dimensions) of feature matrices and weight spaces
            \citep{me20}.\footnote{
                \cite{me20}'s eq.\ 75 bounds a log-determinant defined in eq.\
                61 of a transformed feature matrix.  Compare to linear
                Representer Theorems \citep{mo18b}.
            }
            %
            Our work reveals new dynamics toward and within valleys of minima
            that may also reduce the effective dimension of model space.
            However, our focus on the structure of gradient noise may be
            overspecific, since recent work finds that GD and SGD may both
            converge to the same set of global minima \citep{zo20}. 

            %--  phenomenology of rademacher correlates such as hessians  -----
        
            Our predictions are vacuous for large $\eta$.  Other analyses treat
            large-$\eta$ learning phenomenologically, whether by finding
            empirical correlates of the generalization gap \citep{li18}, by
            showing that \emph{flat} minima generalize (\cite{ho17},
            \cite{ke17}, \cite{wa18}), or by showing that \emph{sharp} minima
            generalize (\cite{st56}, \cite{di17}, \cite{wu18}).  We find that
            SGD's implicit regularization mediates between these seemingly
            clashing intuitions (\S \ref{subsect:overfit}).
            
            %--  our work vs other perturbative approaches  -------------------
        
            Prior work analyzes SGD perturbatively: \cite{dy19} perturb in
            inverse network width, using 't Hooft diagrams to correct the
            Gaussian Process approximation for specific nets.  Perturbing
            to order $\eta^2$, \cite{ch18} and \cite{li17} assume uncorrelated
            Gaussian noise.  By contrast, we use Penrose diagrams to compute
            testing losses to arbitrary order in $\eta$.  We allow correlated,
            non-Gaussian noise and thus \emph{any} smooth architecture.  E.g.\
            we assume no information-geometric relationships between $C$ and
            $H$,\footnote{
                Disagreement of $C$ and
                $H$ is typical in modern learning \citep{ro12, ku19}
            } so we may model VAEs. 

    \newpage
    \acks{
        We are deeply grateful to Sho Yaida and Dan A.\ Roberts for their
        generous mentorship and to Joshua B.\ Tenenbaum for patiently
        granted autonomy.  Dan introduced us to the SGD literature, taught us
        Taylor series street smarts, and inspired this project.  Sho stoked and
        usefully channeled our interest in physics, galvanized our search for a
        resummation technique, and made time for wide-ranging chats.  We also
        appreciate technical discussions with Greg Wornell, David Schwab, and
        Wenli Zhao as well as writerly advice from Ben R.\ Bray, Chloe
        Kleinfeldt, and Karl Winsor.
        %
        We thank our anonymous reviewers for incisive feedback toward our
        writing's clarity.
    }
    
    \bibliography{perturb}
    
    %APPENDIX
    \appendix

    \newpage
    \section*{Organization of the appendices}
        The following three appendices serve three respective functions:
        \setlist{nolistsep}
        \begin{itemize}[noitemsep]
            \item to explain how to calculate using diagrams;
            \item to prove our results (and pose a conjecture);
            \item to specify our experimental methods and results.
        \end{itemize}
        In more detail, we organize the appendices as follows.\\
    
        {\bf
        \par\noindent A ~ Tutorial: how to use diagrams}                        \hfill {\bf page \pageref{appendix:tutorial}}
        \par\indent     A.1 ~~ An example calculation: the effect of epochs     \hfill \pageref{appendix:example}
        \par\indent     A.2 ~~ How to identify the relevant grid                \hfill \pageref{appendix:draw-spacetime} 
        \par\indent     A.3 ~~ How to identify the relevant diagram embeddings  \hfill \pageref{appendix:draw-embeddings}
        \par\indent     A.4 ~~ How to evaluate each embedding                   \hfill \pageref{appendix:evaluate-embeddings}
        \par\indent     A.5 ~~ How to sum the embeddings' values                \hfill \pageref{appendix:sum-embeddings}
        %\par\indent     A.6 ~~ Interpreting diagrams intuitively                \hfill \pageref{appendix:interpret-diagrams}
        \par\indent     A.6 ~~ How to solve variant problems                    \hfill \pageref{appendix:solve-variants}
        \par\indent     A.7 ~~ Do diagrams streamline computation?              \hfill \pageref{appendix:diagrams-streamline}
    
        {\bf
        \par\noindent B ~ Mathematics of the theory}                            \hfill {\bf page \pageref{appendix:math}}
        \par\indent     B.1 ~~ Setting and assumptions                          \hfill \pageref{appendix:assumptions}
        \par\indent     B.2 ~~ A key lemma \`a la Dyson                         \hfill \pageref{appendix:key-lemma}
        \par\indent     B.3 ~~ From Dyson to diagrams                           \hfill \pageref{appendix:toward-diagrams}
        \par\indent     B.4 ~~ Interlude: a review of M\"obius inversion        \hfill \pageref{appendix:mobius}
        \par\indent     B.5 ~~ Theorems \ref{thm:resum} and \ref{thm:converge}  \hfill \pageref{appendix:resum}
        \par\indent     B.6 ~~ Proofs of corollaries                            \hfill \pageref{appendix:corollaries}
        \par\indent     B.7 ~~ Future topics                                    \hfill \pageref{appendix:future}
    
        {\bf
        \par\noindent C ~ Experimental methods}                                 \hfill {\bf page \pageref{appendix:experiments}}
        \par\indent     C.1 ~~ What artificial landscapes did we use?           \hfill \pageref{appendix:artificial}  
        \par\indent     C.2 ~~ What image-classification landscapes did we use? \hfill \pageref{appendix:natural}
        \par\indent     C.3 ~~ Measurement process                              \hfill \pageref{appendix:measure}
        \par\indent     C.4 ~~ Implementing optimizers                          \hfill \pageref{appendix:optimizers}
        \par\indent     C.5 ~~ Software frameworks and hardware                 \hfill \pageref{appendix:frameworks}
        \par\indent     C.6 ~~ Unbiased estimators of landscape statistics      \hfill \pageref{appendix:bessel}
        \par\indent     C.7 ~~ Additional figures                               \hfill \pageref{appendix:figures}
 
\newpage
\section{Tutorial: how to use diagrams}                \label{appendix:tutorial}
    This paper presents a new technique for calculating the expected learning
    curves of SGD in terms of statistics of the loss landscape near
    initialization.  Here, we explain this technique.
    %New combinatorial objects --- \emph{grids} --- arise as we
    %relax the paper body's assumption that $E=B=1$.  This, too, we will
    %explain.
    There are are {\bf four steps} to computing the expected testing loss, or
    other quantities of interest, after a specific number of gradient updates: 
    \begin{itemize}
        \item {\bf Specify, as a grid}, the batch size, training set
            size, and number of epochs. 
        \item {\bf Draw embeddings}, of diagrams into the
            grid, as needed for the desired precision.
        \item {\bf Evaluate each diagram embedding}, whether exactly
            (via $\rvalue$s)
            or roughly
            (via $\uvalue$s).
        \item {\bf Sum the embeddings' values} to obtain the quantity of
              interest as a function of $\eta$.
    \end{itemize}
    \noindent
    After presenting an example calculation that follows these four steps, we
    detail each step individually.  Though we focus on the computation of
    expected testing losses, we describe how the four steps may give us other
    quantities of interest: variances instead of expectations, training
    statistics instead of testing statistics, or weight displacements instead
    of losses.  

    \subsection{An example calculation: the effect of epochs}       \label{appendix:example}

        \begin{quest}\label{qst:multi}
            How does multi-epoch SGD differ from single-epoch SGD?
            Specifically, what is the difference between the expected
            testing losses of the following two versions of SGD?
            \begin{itemize}
                \item SGD over $T=M_0 \times N$ time steps, learning rate $\eta_0/M$, and
                    batch size $B=1$
                \item SGD over $T=N$ time steps, learning rate $\eta_0$, and batch size $B=1$
            \end{itemize}
            We seek an answer expressed in terms of the landscape statistics
            at initialization: $G,H,C, \cdots$.
        \end{quest}
        To make our discussion concrete, we will set $M_0=2$; our analysis 
        generalizes directly to larger $M_0$.

        We scaled the above two versions of SGD deliberately, to create an
        interesting comparison.
        Specifically, on a noiseless
        linear landscape $l_x=l \in (\RR^n)^*$, the versions
        attain equal testing losses, namely $l(\theta_0) - T l_\mu \eta^{\mu\nu}$.
        %
        So Question \ref{qst:multi}'s answer will be second-order (or
        higher-order) in $\eta$.

        \subsubsection{Grids}
            We take an $N\times T$ grid and shade its cells, shading the
            $(n,t)$th cell when the $t$th update involves the $n$th data point.
            Thus, each column contains $B$ (batch size) many shaded cells and
            each row contains $E$ (epoch number) many shaded cells.
            This is SGD's \textbf{grid}.
            Two grids are relevant to Question \ref{qst:multi}: one for
            multi-epoch SGD and another for single-epoch SGD --- see Figure
            \ref{fig:spacetimes-epoch}.
            \begin{figure}[h!] 
                \centering
                \dmoo{3.55cm}{spacetime-b1-e2-nosh}
                ~~~~~
                \dmoo{3.55cm}{spacetime-b1-e1-nosh}
                \caption{
                    \textbf{The grids of single-epoch and of
                    multi-epoch SGD.}  A cell at row $n$ and column $t$ is
                    shaded provided that the $n$th training sample inhabits the
                    $t$th batch.  Both grids depict $N=7$
                    training points and batch size $B=1$; neither
                    depicts training-set permutation between epochs.
                    \newline
                    \textbf{Left}:
                        SGD with $M=2$ update per training sample for a total
                        of $T = MN = 2N$ many updates.
                    \newline
                    \textbf{Right}:
                        SGD with $M=1$ update per training sample for a total
                        of $T = MN = N$ many updates.
                }
                \label{fig:spacetimes-epoch}
            \end{figure}

        \newpage
        \subsubsection{Embeddings of diagrams into a grid}
            There are four two-edged diagrams: 
            $\sdia{c(0-1-2)(02-12)}$,
            $\sdia{c(01-2)(02-12)}$,
            $\sdia{c(0-1-2)(01-12)}$, and
            $\sdia{c(01-2)(01-12)}$.
            %We permit the diagram $\sdia{c(01-2)(02-12)}$, which violates the
            %path condition mentioned in \S\ref{sect:calculus}, because we are
            %no longer restricting to the special case $E=B=1$.
            %
            An \emph{embedding} of a diagram $D$ into a grid is an
            assignment of $D$'s non-root nodes to shaded cells $(n,t)$ obeying
            the following criteria:
            \begin{itemize}
                \item \textbf{time-ordering condition}: the times $t$ strictly increase 
                    along each path from leaf to root; and
                \item \textbf{correlation condition}: if two nodes are in the same
                    part of $D$'s partition, then they are assigned to the same
                    datapoint $n$.
            \end{itemize}

            We may conveniently draw embeddings by placing nodes in the shaded
            cells to which they are assigned.  Figure
            \ref{fig:multi-embeddings} shows some embeddings of order-$1$ and
            order-$2$ diagrams (i.e. one-edged and two-edged diagrams) into the
            grid relevant to Question \ref{qst:multi}.
            \begin{figure}[h!] 
                \centering
                \dmoo{3.55cm}{spacetime-d}
                ~~~~~
                \dmoo{3.55cm}{spacetime-c}
                \caption{
                    \textbf{The diagram $\protect\sdia{c(01-2)(01-12)}$ embeds
                        into multi-epoch but not single-epoch grid.}
                    Drawn on each of the two grids are examples of embeddings.
                    The black nodes external to the grids are positioned
                    arbitrarily. 
                    From top to bottom in each grid, the five 
                        diagrams embedded are
                        $\protect\sdia{c(01-2)(01-12)}$ (or $\protect\sdia{c(0-1-2)(01-12)}$), 
                        $\protect\sdia{c(0-1)(01)}$,
                        $\protect\sdia{c(0-1-2)(01-12)}$, 
                        $\protect\sdia{c(0-1-2)(02-12)}$, and 
                        $\protect\sdia{c(01-2)(02-12)}$ (or $\protect\sdia{c(0-1-2)(02-12)}$).
                    The diagram $\protect\sdia{c(0-1-2)(01-12)}$ may be embedded
                    wherever the diagram $\protect\sdia{c(01-2)(01-12)}$ may
                    be embedded, but not vice versa.  Likewise for
                    $\protect\sdia{c(0-1-2)(02-12)}$
                    and
                    $\protect\sdia{c(01-2)(02-12)}$.
                    \textbf{Left}: $\protect\sdia{c(01-2)(01-12)}$
                        embeds into multi-epoch grid.
                    \textbf{Right}: $\protect\sdia{c(01-2)(01-12)}$ cannot
                        embed into single-epoch grid.  Indeed, 
                        the correlation condition forces both red nodes into 
                        the same row and thus the same cell, while the
                        time-ordering condition forces the red nodes into
                        distinct columns and thus distinct cells.
                }
                \label{fig:multi-embeddings}
            \end{figure}

        \subsubsection{Values of the embeddings}

            We choose to compute $\uvalue$s instead of $\rvalue$s.  The former
            are an approximation of the latter, appropriate when $T$ is fixed
            instead of taken to infinity.  $\uvalue$s have the same asymptotic
            error as $\rvalue$s with respect to $\eta$.  Moreover, $\uvalue$s
            are simpler to calculate, since their numeric values depend only on
            diagrams, not on embeddings.  So to compute a testing loss, we
            multiply each diagram's $\uvalue$ by the number of ways that
            diagram embeds.

            Figure \ref{fig:multi-embeddings} shows us that the diagram
            $\sdia{c(0-1-2)(02-12)}$ embeds similarly into multi-epoch
            and single-epoch spacetimes.  More precisely,
            its multi-epoch embeddings correspond by a $M_0^2:1$ map to
            its single-epoch embeddings.  Since we scaled the learning rate of
            the two SGD versions by a factor of $M_0$, and since $2$-edged
            diagrams such as $\sdia{c(0-1-2)(02-12)}$ scale as $\eta^2$, the
            total $\uvalue$ of the diagram's multi-epoch embeddings will match
            the total $\uvalue$ of the diagram's single-epoch embeddings. 
            %
            In fact, Figure \ref{fig:multi-embeddings} shows that this
            cancellation happens for all of the order-$2$ diagrams
            \emph{except} for $\sdia{c(01-2)(01-12)}$.
            %
            Therefore, to second order, the answer to Question \ref{qst:multi}
            will be (some multiple of) $\uvalue(\sdia{c(01-2)(01-12)})$.

            To compute $\sdia{c(01-2)(01-12)}$'s value, we follow the rules
            in Section \ref{sect:calculus}; the edge rule for $\uvalue$s is
            that each edge becomes an $\eta$.
            So
            $$
                \uvalue(\sdia{c(01-2)(01-12)}) =
                \expct{\nabla_\mu l_x \nabla_\nu \nabla_\lambda l_x}
                \expct{\nabla_\rho l_x}
                \eta^{\mu\lambda}
                \eta^{\nu\rho}
                =
                (\nabla_\nu C_{\mu\lambda} / 2)
                G^\rho
                \eta^{\mu\lambda}
                \eta^{\nu\rho}
            $$

        \subsubsection{Sum of the values}

            Referring again to Figure \ref{fig:multi-embeddings}, we see that
            $\sdia{c(01-2)(01-12)}$ has ${M_0 \choose 2} \, N$ many embeddings
            into the multi-epoch grid (one embedding per pair
            of distinct epochs, per row) --- and no embeddings into the 
            single-epoch grid.  Moreover, each embedding of $\sdia{c(01-2)(01-12)}$
            has $\wabs{\Aut_f(D)}= 1$.  Now we plug into the overall formula
            for testing loss: 
            \begin{equation*}
                \sum_{\substack{D~\text{a} \\ \text{diagram}}}
                ~
                \sum_{\substack{f~\text{an embed-} \\ \text{-ding of}~D}}
                ~
                \frac{(-B)^{-|\edges(D)|}}{\wabs{\Aut_f(D)}}
                \,
                {\uvalue}(D)
            \end{equation*}
            We conclude that the testing loss of $M=M_0$ SGD exceeds the testing loss
            of $M=1$ SGD by this much:
            $$
                {M_0 \choose 2} \, N \cdot
                \frac{(-1)^2}{1} \cdot
                (\nabla_\nu C_{\mu\lambda} / 2)
                G^\rho
                \eta^{\mu\lambda}
                \eta^{\nu\rho}
                + o(\eta^2)
            $$
            Since Question \ref{qst:multi} defines $\eta^2 = \eta_0^2/M_0^2$,
            we can rewrite our answer as:
            $$
                l(\theta_{M=M_0,\eta=\eta_0/M_0}) - l(\theta_{M=1,\eta=\eta_0})
                =
                \frac{M_0-1}{4 M_0} N \cdot
                G^\nu (\nabla_\nu C_\mu^\mu)
                + o(\eta_0^2)
            $$
            where we use $\eta_0$ to raise indices.
            This completes the example problem.

            \begin{rmk*}
                An essentially similar
                argument proves Corollary \ref{cor:epochs}.
                \mend
            \end{rmk*}

    \vfill
    \subsection{How to identify the relevant grid}            \label{appendix:draw-spacetime}

        Diagrams tell us about the loss landscape but not about
        SGD's batch size, number of epochs, and training set size
        We encode this SGD data as a set of pairs $(n,t)$, where we have
        one pair for each participation of the $n$th datapoint in the $t$th
        update.  For instance, full-batch GD has $NT$ many pairs, and
        singeleton-batch SGD has $T$ many pairs.  We will draw these
        $(n,t)$ pairs as shaded cells in an $N\times T$ grid; we will call
        the shaded grid the SGD's \textbf{grid}.  See Figure
        \ref{fig:spacetimes}.  

        \begin{figure}[h!] 
            \centering
            \dmoo{3.55cm}{spacetime-b1-e2-shuf}
            ~~~~~
            \dmoo{3.55cm}{spacetime-b2-e4-nosh}
            \caption{
                \textbf{The grids of two SGD variants.}
                Shaded cells show $(n,t)$ pairs (see text).
                \newline
                \textbf{Left}: Two epoch SGD with batch size one.
                    The training set is permuted between epochs.
                \newline
                \textbf{Right}: Four epoch SGD with batch size
                    two.  The training set is not permuted between epochs.
            }
            \label{fig:spacetimes}
        \end{figure}

        In sum, when using the diagram method to solve a problem relating
        to SGD with batch size $B$ and $E$ many epochs (over $T$ many time
        steps and on $N$ many training samples), one shades the 
        cells of an $N\times T$ grid with $B$ shaded cells per column and
        $E$ shaded cells per row.
        
        \begin{rmk*}
            A grid also depicts the shuffling of training sets
            between epochs.  Since each grid commits to a concrete sequence
            of training set permutations, we may analyze SGD with
            randomized permutations by taking expectations over multiple
            grids.  However, all of the corollaries in this text
            are invariant to inter-epoch training set permutations, so we
            will not focus on this point.\footnote{A routine check shows
            that for fixed $T$, inter-epoch shuffling yields only an
            $o(\eta^3)$ effect on testing losses.}
            \mend
        \end{rmk*}

    \newpage
    \subsection{How to identify the relevant diagram embeddings}    \label{appendix:draw-embeddings}
        A \emph{diagram} is a finite rooted tree equipped with a partition of
        its nodes, such that the root node occupies a part of size $1$.  Note
        that this definition generalizes the special case reported in the paper
        body; in particular, we no longer require the paper body's ``path
        condition'' to hold.  For example, there are four diagrams with two
        edges:
        $\protect\sdia{c(0-1-2)(02-12)}$,
        $\protect\sdia{c(01-2)(02-12)}$,
        $\protect\sdia{c(0-1-2)(01-12)}$, and
        $\protect\sdia{c(01-2)(01-12)}$.
        As always, we specify a diagram's root by drawing it rightmost.

        A diagram is \emph{linkless} when each of its degree-$2$ nodes is in
        a part of size one.  Intuitively, this rules out multi-edge chains
        unadorned by fuzzy ties.
        Thus, only the first diagram in the list 
        $\sdia{c(0-1)(01)}, \sdia{c(0-1-2)(01-12)},
        \sdia{c(0-1-2-3)(01-12-23)}, \cdots$
        is linkless.  Only the first diagram in the list
        $\sdia{c(01-2)(01-12)}, \sdia{c(01-2-3)(01-12-23)}, \cdots$
        is linkless.
        Only the first diagram in the list
        $\sdia{c(0-1-2)(02-12)}, \sdia{c(0-1-2-3)(01-13-23)}, \cdots$
        is linkless.

        An \emph{embedding} of a diagram $D$ into a grid is an
        assignment of $D$'s non-root nodes to shaded cells $(n,t)$ that
        obeys the following two criteria:
        \begin{itemize}
            \item \textbf{time-ordering condition}: the times $t$ strictly increase 
                along each path from leaf to root; and
            \item \textbf{correlation condition}: if two nodes are in the same
                part of $D$'s partition, then they are assigned to the same
                datapoint $n$.
        \end{itemize}
        We may conveniently draw embeddings by placing nodes in the shaded
        cells to which they are assigned.  Then, the time-ordering condition 
        forbids (among other things) intra-cell edges, and the correlation
        condition demands that fuzzily tied nodes are in the same row.  See
        Figure \ref{fig:embeddings}.
        \begin{figure}[h] 
            \centering  
            \plotmooh{diagrams/spacetime-e}{}{0.26\columnwidth}
            \caption{
                Embeddings, legal and illegal.
                \textbf{Left}: illegal embedding of $\sdia{c(0-1-2)(01-12)}$,
                    since the time-ordering condition is not obeyed. 
                    For the same reason, not a legal embedding of $\sdia{c(01-2)(01-12)}$.
                \textbf{Middle}: an embedding of $\sdia{c(0-1-2)(01-12)}$.
                Also an embedding of $\sdia{c(01-2)(01-12)}$,
                since the correlation condition is obeyed.
                \textbf{Right}: a legal embedding of $\sdia{c(0-1-2)(01-12)}$.
                    Not an embedding of
                    $\sdia{c(01-2)(01-12)}$, since the correlation condition is
                    not obeyed.
            }
            \label{fig:embeddings}
        \end{figure}

        In principle, the relevant diagrams for a calculation with error
        $o(\eta^d)$ are the diagrams with at most $d$ edges.  For $d$ greater
        than $2$, there will be many such diagrams.  However, in practice
        we gain insight even from considering one diagram at a time:
        \begin{rmk*}
            In this paper's corollaries, we seek to extract the specific effect
            of a specific landscape or optimization feature such as skewed
            noise (Example \ref{exm:first}) or multiple epochs
            (\S\ref{appendix:example}).  In these cases, it is usually the case
            that most diagrams are irrelevant.  For example, because a diagram
            evaluates to a product of its components, the only way the skewness
            of gradient noise can appear in our calculations is through 
            diagrams such as $\sdia{c(012-3)(03-13-23)}$ that have a part of
            size $3$.  Thus, the analysis in Example \ref{exm:first} was able
            to ignore diagrams such as $\sdia{c(01-2)(02-12)}$. 
            Likewise, in \S\ref{appendix:example} we argued by considering 
            which embeddings that the only diagram relevant to Question
            \ref{qst:multi} is $\sdia{c(01-2)(01-12)}$.  
            \mend
        \end{rmk*}

        In sum, when using the diagram method to analyze how a quantity affects
        SGD to order $o(\eta^d)$, we must consider all diagrams with $d$ or
        fewer edges that include that quantity as a component and that have a
        non-zero number of embeddings into the relevant grid.  If we
        are using $\rvalue$s (see next section for discussion of $\rvalue$s and
        $\uvalue$s), then we consider only the linkless diagrams.  For each
        diagram, we must enumerate the embeddings, i.e. the assignments of the
        diagram's nodes to grid cells that obey both the time-ordering
        condition and correlation condition.

        Here are some further examples.  Table \ref{tab:scatthree} shows the
        $6$ diagrams that may embed into the grid of $E=B=1$.  It
        shows each diagram in multiple ways to underscore that diagrams are
        purely topological and to suggest the ways in which these diagrams may
        embed into a grid.
        \begin{table}[h!]
            \centering 
            %\resizebox{\columnwidth}{!}{%
            \begin{tabular}{ccccc}
                {\large $\Theta\left((\eta N)^3 N^{-0}\right)$} &&
                {\large $\Theta\left((\eta N)^3 N^{-1}\right)$} &&
                {\large $\Theta\left((\eta N)^3 N^{-2}\right)$} \\ \hline
                \begin{tabular}{c}
                    \begin{tabular}{ll}
                        $\mdia{c(0-1-2-3)(01-12-23)}$ & $\mdia{c(0-1-2-3)(01-13-23)}$
                    \end{tabular} \\
                    \begin{tabular}{ll}
                        $\mdia{c(0-1-2-3)(02-13-23)}$ & $\mdia{c(0-1-2-3)(03-12-23)}$
                    \end{tabular} \\ \hline
                    \begin{tabular}{ll}
                        $\mdia{c(0-1-2-3)(03-13-23)}$ & $\mdia{c(0-1-2-3)(02-12-23)}$
                    \end{tabular}
                \end{tabular}
                &&
                \begin{tabular}{c}
                    \begin{tabular}{ll}
                        $\mdia{c(01-2-3)(02-13-23)}$ & $\mdia{c(01-2-3)(03-12-23)}$
                    \end{tabular} \\ \hline
                    \begin{tabular}{ll}
                        $\mdia{c(0-12-3)(01-13-23)}$ & $\mdia{c(0-12-3)(02-13-23)}$
                    \end{tabular} \\ \hline
                    \begin{tabular}{lll}
                        $\mdia{c(01-2-3)(03-13-23)}$ & $\mdia{c(0-12-3)(03-13-23)}$ & $\mdia{c(01-2-3)(02-12-23)}$ 
                    \end{tabular}
                \end{tabular}
                &&
                \begin{tabular}{c}
                    \begin{tabular}{l}
                        $\mdia{c(012-3)(03-13-23)}$
                    \end{tabular}
                \end{tabular}
            \end{tabular}
            %}
            \caption{
                \textbf{Multiple ways to draw the $6$ distinct degree-$3$
                diagrams for $B=E=1$ SGD's testing loss.}
                Because the grid of $B=E=1$ SGD has only one cell per row
                and one cell per column, the only diagrams that have a non-zero
                number of embeddings are the diagrams that obey
                \S\ref{sect:calculus}'s path condition. 
                We show $(4+2)+(2+2+3)+(1)$ ways to draw the $6$ diagrams.
                In fact, these drawings show all of the time-orderings of the
                diagrams' nodes that are consistent with the time-ordering
                condition.
                %
                \textbf{Organization}:
                We organize the diagrams into columns by the number
                of parts in their partitions.  Because partitions (fuzzy
                outlines) indicate correlations between nodes (i.e. noise), 
                diagrams with fuzzy outlines show deviations of SGD away from
                deterministic ODE.  The big-$\Theta$ notation that heads the
                columns gives the asymptotics of the sum-over-embeddings of 
                each diagram's $\uvalue$s 
                (for $N$ large and $\eta$ small even relative to $1/N$).
                {\bf Left}: Diagrams for ODE behavior.
                {\bf Center}: $1$st order deviation of SGD
                away from ODE.
                {\bf Right}: $2$nd order deviation of SGD
                from ODE with appearance of non-Gaussian statistics.
            }
            \label{tab:scatthree}
        \end{table}
   
    \subsection{How to evaluate each embedding}                     \label{appendix:evaluate-embeddings}
        We will discuss how to compute both $\rvalue$s and $\uvalue$s.  Both
        are ways of turning a diagram embedding into a number.  The paper body
        mainly mentions $\rvalue$s.  $\uvalue$s are simpler to calculate, since
        they depend only on a diagram's topology, not on the way it is
        embedded.  $\rvalue$s are more accurate; in particular, when we
        initialize near a local minimum, $\rvalue$s do not diverge to $\pm
        \infty$ as $T\to\infty$.
        
        \subsubsection{Un-resummed values: $\uvalue(D)$}
            Each part in a diagram's partition looks like one of the following
            fragments (or one of the infinitely many analogous fragments):
            \begin{center}
            \begin{tabular}{p{6cm}p{6cm}}
                {\begin{align*}
                    G\triangleq\ex{\nb\lx}              &\triangleq \mdia{MOOc(0)(0)}            \\
                    H\triangleq\ex{\nb\nb\lx}           &\triangleq \mdia{MOOc(0)(0-0)}          \\
                    J\triangleq\ex{\nb\nb\nb\lx}        &\triangleq \mdia{MOOc(0)(0-0-0)}        \\
                    \ex{(\nb\lx - G)(\nb\nb\lx - H)}    &\triangleq \mdia{MOOc(01)(0-1-1)}       \\
                    \ex{(\nb\nb\lx - H)(\nb\nb\lx - H)} &\triangleq \mdia{MOOc(01)(0-0-1-1)}     \\
                    \ex{(\nb\lx - G)(\nb\nb\nb\lx - J)} &\triangleq \mdia{MOOc(01)(0-1-1-1)}
                \end{align*}}
                &
                {\begin{align*}
                    C\triangleq\ex{(\nb\lx - G)^2}      &\triangleq \mdia{MOOc(01)(0-1)}         \\
                    S\triangleq\ex{(\nb\lx - G)^3}      &\triangleq \mdia{MOOc(012)(0-1-2)}      \\ 
                    \ex{(\nb\lx - G)^4} - 3C^2          &\triangleq \mdia{MOOc(0123)(0-1-2-3)}   \\ 
                    \ex{(\nb\lx - G)^5} - 10CS          &\triangleq \mdia{MOOc(01234)(0-1-2-3-4)}
                \end{align*}}
            \end{tabular}
            \end{center}
            The above examples illustrate the
            %
            \textbf{Node rule}: each degree $d$ node evaluates to 
            $\nabla^d l_x$.

            Fuzzy outlines dictate how to collect the $\nabla^d l_x$s into
            expectation brackets.  For example, we could collect the nodes
            within each part (of the partition) into a pair of expectation
            brackets $\expc_x\wasq{\cdot}$ --- call the result the
            \textbf{moment value}.
            However, this would yield (un-centered)
            moments such as $\ex{(\nb\lx)^2}$ instead of cumulants such as
            $C=\ex{(\nb\lx - G)^2}$.
            For technical reasons explained in \S\ref{appendix:mobius} and
            \S\ref{appendix:resum}, cumulants will be easier to work with than
            moments, so we will choose to define the values of diagrams
            slightly differently as follows.
            %
            \textbf{Outline rule}: a partition on nodes evaluates to the
            difference $X-Y$, where $X$ is the moment-value of the partition
            and $Y$ is the sum of all strictly finer partitions.

            This is just the standard M\"obius recursion for defining cumulants
            (see \cite{ro64}).

            \begin{exm}
                For example, if we denote moment values by solid
                gray fuzzy ties (instead of fuzzy outlines), then: 
                \begin{align*}
                    \mdia{c(012-3)(01-13-23)}
                        &\triangleq
                    \mdia{(012-3)(01-13-23)}
                        -
                    \mdia{c(01-2-3)(01-13-23)}
                        -
                    \mdia{c(02-1-3)(01-13-23)}
                        -
                    \mdia{c(0-12-3)(01-13-23)}
                        -
                    \mdia{(0-1-2-3)(01-13-23)} \\
                        &\triangleq
                    \mdia{(012-3)(01-13-23)}
                        -
                    \mdia{(01-2-3)(01-13-23)}
                        -
                    \mdia{(02-1-3)(01-13-23)}
                        -
                    \mdia{(0-12-3)(01-13-23)}
                        +
                    2 \mdia{(0-1-2-3)(01-13-23)}
                \end{align*}
                We will use the concept of ``moment values'' again in \S\ref{appendix:mobius}.
                \mend
            \end{exm}

            Finally, we come to edges. 
            \textbf{Edge rule}: insert a factor of $\eta^{\mu\nu}$ for each
            edge.  The indices $\mu, \nu$ should match the corresponding
            indices of the two nodes incident to the edge.

            \begin{exm}[Un-resummed value] \label{exm:unresum}
                Remember that
                $
                    \mdia{MOOc(01)(0-1)} = C_{\mu\nu}
                $ and
                $
                    \mdia{MOOc(0)(0-0)} = H_{\lambda\rho}
                $, so that
                $
                    \mdia{MOOc(01)(0-1)}
                    \mdia{MOOc(0)(0-0)}
                    = C_{\mu\nu} H_{\lambda\rho}
                $.
                Then 
                $$
                    \uvalue(\mdia{c(01-2)(02-12)})
                    = C_{\mu\nu} H_{\lambda\rho}
                    \eta^{\mu\lambda}
                    \eta^{\nu\rho}
                $$
                Here, $\mdia{c(01-2)(02-12)}$ has two edges, which correspond
                in this example to the tensor contractions via
                $\eta^{\mu\lambda}$ and via $\eta^{\nu\rho}$, respectively.
                \mend
            \end{exm}

        \subsubsection{Resummed values: $\rvalue_f(D)$}
            The only difference between $\rvalue$s and $\uvalue$s is in their
            rule for evaluating edges.

            \textbf{Edge rule}: if an edge's endpoints are embedded to times
            $t, t^\prime$, insert a factor of $K^{\wabs{t^\prime-t}-1} \eta$,
            where $K \triangleq (I-\eta H)$.  Here, we consider the root node
            as embedded to the time $T$.

            \begin{exm}[Re-summed value] \label{exm:resum}
                Recall as in Example \ref{exm:unresum} that 
                $
                    \mdia{MOOc(01)(0-1)} = C_{\mu\nu}
                $ and
                $
                    \mdia{MOOc(0)(0-0)} = H_{\lambda\rho}
                $, so that
                $
                    \mdia{MOOc(01)(0-1)}
                    \mdia{MOOc(0)(0-0)}
                    = C_{\mu\nu} H_{\lambda\rho}
                $.
                Then if $f$ is an embedding of $\mdia{c(01-2)(02-12)}$ that
                sends the diagram's red part to a time $t$ (and its green root
                to $T$), we have:
                $$
                    \rvalue_f(\mdia{c(01-2)(02-12)})
                    = C_{\mu\nu} H_{\lambda\rho}
                    \wrap{K^{T-t-1} \eta}^{\mu\lambda}
                    \wrap{K^{T-t-1} \eta}^{\nu\rho}
                $$
                Here, $\mdia{c(01-2)(02-12)}$ has two edges, which correspond
                in this example to the tensor contractions via
                $\wrap{K^{\cdots}\eta}^{\mu\lambda}$ and via
                $\wrap{K^{\cdots}\eta}^{\nu\rho}$, respectively.
                \mend
            \end{exm}

        \subsubsection{Overall}
            In sum, we evaluate an embedding of a diagram by using the 
            \textbf{node}, 
            \textbf{outline}, and
            \textbf{edge}
            rules to build an expression of $\nabla^d l_x$s, $\expc_x$s and
            $\eta$s.  The difference between $\uvalue$s and $\rvalue$s lies
            only in their edge rule.

    \subsection{How to sum the embeddings' values}                  \label{appendix:sum-embeddings}
        Theorem \ref{thm:resum} in the paper body generalizes to
        \begin{thm*}
            For any $T$: for $\eta$ small enough, SGD has expected testing loss
            \begin{equation*}
                \sum_{\substack{D~\text{a linkless} \\ \text{diagram}}}
                ~
                \sum_{\substack{f~\text{an embed-} \\ \text{-ding of}~D}}
                ~
                \frac{(-B)^{-|\edges(D)|}}{\wabs{\Aut_f(D)}}
                \,
                {\rvalue_f}(D)
            \end{equation*}
            which is the same as
            \begin{equation*}
                \sum_{\substack{D~\text{a} \\ \text{diagram}}}
                ~
                \sum_{\substack{f~\text{an embed-} \\ \text{-ding of}~D}}
                ~
                \frac{(-B)^{-|\edges(D)|}}{\wabs{\Aut_f(D)}}
                \,
                {\uvalue}(D)
            \end{equation*}
            Here, $B$ is the batch size.
            %We say an embedding is \emph{strict} if it assigns to each part
            %a different datapoint $n$.
        \end{thm*}

        How do we evaluate the above sum?
        Summing $\uvalue$s reduces to counting embeddings, which in all the
        applications reported in this text is a routine combinatorial exercise. 
        However, when summing $\rvalue$s, it is often convenient to replace
        a sum over embeddings by an integral over times, and
        the power $\wrap{I-\eta H}^{\Delta t-1}$ by
        the exponential $\exp{-\Delta t \eta H}$.  This incurs a term-by-term
        $1+o(\eta)$ error factor, meaning that it preserves leading order
        results. 

        \begin{exm}
            Let us return to $D=\mdia{c(01-2)(02-12)}$, embedded, say, in the
            grid of  one-epoch one-sample-per-batch SGD.
            From Example \ref{exm:resum}, we know that we want to sum the 
            following value over all embeddings $f$, i.e. over all $0\leq t<T$
            to which the red part of the diagram's partition may be assigned:
            $$
                \rvalue_f(\mdia{c(01-2)(02-12)})
                = C_{\mu\nu} 
                \wrap{K^{T-t-1} \eta}^{\mu\lambda}
                \wrap{K^{T-t-1} \eta}^{\nu\rho}
                H_{\lambda\rho}
            $$
            Each embedding has a factor 
                $(-B)^{-|\edges(D)|}/\wabs{\Aut_f(D)} = (-B)^{-2}/2$;
            we will multiply in this factor at the end so we now we focus on
            the $\sum_f$.
            So, using the aforementioned approximation, we seek to evaluate
            \begin{align*}
                \int_{0\leq t<T} \, dt \, 
                    C_{\mu\nu} 
                    \wrap{\exp\wrap{-(T-t)\eta H} \eta}^{\mu\lambda}
                    \wrap{\exp\wrap{-(T-t)\eta H} \eta}^{\nu\rho}
                    H_{\lambda\rho}
                = \\
                C_{\mu\nu} 
                \wrap{
                \int_{0\leq t<T} \, dt \, 
                    \exp\wrap{-(T-t)((\eta H)\otimes I + I \otimes (\eta H))}^{\mu\nu}_{\pi\sigma}
                }
                \eta^{\pi\lambda}
                \eta^{\sigma\rho}
                H_{\lambda\rho}
            \end{align*}
            We know from linear algebra and calculus that
            $\int_{0\leq u<T} \, du \, \exp(-u A) = (I - \exp(-T A))/A$ 
            (when $A$ is a non-singular linear endomorphism).
            Applying this rule for $u=T-t$ and $A=(\eta H)\otimes I + I \otimes
            (\eta H)$, we evaluate the integral as:
            \begin{align*}
                \cdots =
                C_{\mu\nu} 
                \wrap{\frac{I - \exp\wrap{-T ((\eta H)\otimes I + I \otimes (\eta H))}}
                           {(\eta H)\otimes I + I \otimes (\eta H)}
                     }^{\mu\nu}_{\pi\sigma}
                \eta^{\pi\lambda}
                \eta^{\sigma\rho}
                H_{\lambda\rho}
            \end{align*}
            This is perhaps easier to write in an eigenbasis of $\eta H$:
            \begin{align*}
                \cdots = 
                \sum_{\mu\nu}
                C_{\mu\nu} 
                \,
                \frac{1 - \exp\wrap{-T ((\eta H)^\mu_\mu + (\eta H)^\nu_\nu)}}{(\eta H)^\mu_\mu + (\eta H)^\nu_\nu}
                \,
                (\eta H \eta)^{\mu\nu}
            \end{align*}
            Multiplying this expression by the aforementioned $(-B)^{-2}/2$
            gives the contribution of $\mdia{c(01-2)(02-12)}$ to SGD's test
            loss.
            \mend
        \end{exm}

        In short, we sum embeddings of $\uvalue$s directly.
        We sum embeddings of $\rvalue$s using an integral-of-exponentials
        approximation along with the rule 
            $\int_{0\leq u<T} \, du \, \exp(-u A) = (I - \exp(-T A))/A$. 
        When written in an eigenbasis of $\eta H$, this $A$'s coefficients are
        sums of one or more eigenvalues of $\eta H$ (one eigenvalue for each
        edge involved in the relevant degrees of freedom over which we
        integrate).  As another example, see Example \ref{exm:first}.

    %\subsection{Interpreting diagrams intuitively}                  \label{appendix:interpret-diagrams}

    %    We may intuitively interpret edges as carrying influence from the
    %    training set toward the test measurement.  See Figure
    %    \ref{fig:intuition}.  From this perspective, we may intuitively
    %    interpret edges in an $\rvalue$ calculation as carrying influence from
    %    the training set toward the test measurement.  See Figure
    %    \ref{fig:intuition}.

    %    \begin{figure}[h!] 
    %        \centering  
    %        \plotmooh{diagrams/spacetime-f}{}{0.26\columnwidth}
    %        \caption{
    %            \textbf{Edges carry information}.
    %            Embedding of a $4$-edged diagram.
    %        }
    %        \label{fig:intuition}
    %    \end{figure}


    %    \begin{figure}[h!] 
    %        \centering  
    %        \dmoo{3cm}{spacetime-g}
    %        \dmoo{3cm}{spacetime-h}
    %        \caption{
    %            \textbf{Resummation propagates information, damped by
    %            curvature}.  Each resummed valuerepresents many un-resummed
    %            values, each modulated by the Hessian ($\sdia{MOOc(0)(0-0)}$)
    %            in a different way.
    %            \textbf{Left}: Here is one of many un-resummed terms captured by
    %            a single resummed embedding for $\sdia{c(0-1)(01)}$.
    %            \textbf{Left}: each resummed value represents many un-resummed
    %            values.  Here is one of many un-resummed terms captured by
    %            a single resummed embedding for $\sdia{c(01-2)(02-12)}$.
    %        }
    %        \label{fig:intuition}
    %    \end{figure}

    %%        \subsubsection{Chladni effect}
    %%(\S\ref{appendix:interpret-diagrams})
    %%            {\color{red} Make sure to also discuss Chladni effect
    %%            \cite{ch87}!!!}

    \subsection{How to solve variant problems}                      \label{appendix:solve-variants}
        In \S\ref{appendix:future}, we briefly discuss second-order methods
        and natural gradient descent.  Here, we briefly discuss modifications.
        We omit proofs, which would closely follow \S\ref{appendix:math}'s
        proof of the expectation-of-test-loss case.

        \subsubsection*{Variance (instead of expectation)}
            To compute variances instead of expectations (with respect to the
            noise in the training set), one considers generalized diagrams   
            that have ``two roots'' instead of one.  More precisely, to
            compute, say, the un-centered second moment of testing loss, one uses
            diagrams whose edge structures are not rooted trees but instead
            forests consisting of two rooted trees.  As in the case of test
            loss expectations, we require that the set of roots (now a set
            of size two instead of size one) is a part of the diagram's
            partition.  We draw the two roots rightmost. 
            %
            For example, the generalized diagrams $\mdia{MOOc(01)(01)}$ or
            $\mdia{MOOc(01-23)(02-13)}$ may appear in this computation.

        \subsubsection*{Measuring on the training (instead of test) set}

            To compute the training loss, we compute with all the same
            diagrams as the testing loss, and we also allow all the additional
            generalized diagrams that violate the constraint that a diagram's
            root should be in a part of size one.
            %
            Therefore, to compute the generalization gap (i.e.\ testing loss minus
            training loss), we sum over all the diagrams that expressly 
            violate this constraint (and then, since gen.\ gp is test minus
            train instead of train minus test, we multiply the whole answer
            by $-1$).
            %
            For example, the generalized diagrams $\mdia{MOOc(01)(01)}$ or
            $\mdia{MOOc(0-123)(02-12-23)}$ may appear in this computation.

        \subsubsection*{Weight displacement (instead of loss)}
            To compute displacements instead of losses, one considers
            generalized diagrams that have a ``loose end'' instead of a root.
            %
            For example, the generalized diagrams $\mdia{MOOc(0)(0)}$ or
            $\mdia{MOOc(01)(01-1)}$ may appear in this computation.

    \subsection{Do diagrams streamline computation?}                \label{appendix:diagrams-streamline}

        Diagram methods from Stueckelberg to Peierls have flourished in physics
        because they enable swift computations and offer immediate intuition
        that would otherwise require laborious algebraic manipulation.  We
        demonstrate how our diagram formalism likewise streamlines analysis of
        descent by comparing direct perturbation\footnote{
            By ``direct perturbation'', we mean direct application of our Key
            Lemma (\S\ref{appendix:key-lemma}).
        }
        to the new formalism on two sample problems.

        Aiming for a conservative comparison of derivation ergonomics, we lean
        toward explicit routine when using diagrams and allow ourselves to use
        clever and lucky simplifications when doing direct perturbation.  For
        example, while solving the first sample problem by direct perturbation,
        we structure the SGD and GD computations so that the coefficients (that
        in both the SGD and GD cases are) called $a(T)$ manifestly agree in
        their first and second moments.  This allows us to save some lines.

        Despite these efforts, the diagram method yields arguments about
        \emph{four times shorter} --- and strikingly more conceptual --- than
        direct perturbation yields.  These examples specifically suggest that:
        diagrams obviate the need for meticulous index-tracking, from the start
        focus one's attention on non-cancelling terms by making visually
        obvious which terms will eventually cancel, and allow immediate
        exploitation of a setting's special posited structure, for instance
        that we are initialized at a test minimum or that the batch size is
        $1$.  We regard these examples as evidence that diagrams offer a
        practical tool for the theorist.

        We make no attempt to compare the re-summed version of our formalism to
        direct perturbation because the algebraic manipulations involved for
        the latter are too complicated to carry out.  

        We now compare {\colorbox{moolime}{Diagram Rules}} vs
        {\colorbox{moosky}{Direct Perturbation}}.

        \subsubsection{Effect of batch size}
            We compare the testing losses of pure SGD and pure GD.  Because pure
            SGD and pure GD differ in how samples are correlated, their testing loss
            difference involves a covariance and hence occurs at order $\eta^2$.  

            \subthreesect{Diagram Method}
            \colorlet{shadecolor}{moolime}
            \begin{shaded}
                Since SGD and GD agree on noiseless landscapes, we consider only
                diagrams with fuzzy ties.  Since we are working to second order, we
                consider only two-edged diagrams.  There are only two such
                diagrams, $\sdia{(01-2)(02-12)}$ and $\sdia{(01-2)(01-12)}$.  The
                first diagram, $\sdia{(01-2)(02-12)}$, embeds in GD's space time in
                $N^2$ as many ways as it embeds in SGD's spacetime, due to
                horizontal shifts.  Likewise, there are $N^2$ times as many
                embeddings of $\sdia{(01-2)(02-12)}$ in distinct epochs of GD's
                spacetime as there are in distinct epochs of SGD's spacetime.
                However, each same-epoch embedding of $\sdia{(01-2)(01-12)}$ within
                any one epoch of GD's spacetime corresponds by vertical shifts to
                an embedding of $\sdia{(0-1-2)(01-12)}$ in SGD.  There are
                $MN{N\choose 2}$ many such embeddings in GD's spacetime, so GD's
                testing loss exceeds SGD's by 
                $
                    \frac{MN{N\choose 2}}{N^2}~
                    \sdia{c(01-2)(01-12)}
                $.
                Reading the diagram's value from its graph structure, we
                unpack that expression as:
                $$
                    \eta^2 \frac{M(N-1)}{4} G \nabla C 
                $$
            \end{shaded}

            %\newpage
            \subthreesect{Direct Perturbation} 
            \colorlet{shadecolor}{moosky}
            \begin{shaded}
                We compute the displacement $\theta_T-\theta_0$ to order $\eta^2$ 
                for pure SGD and separately for pure GD.  Expanding
                $
                    \theta_t \in \theta_0 + \eta a(t) + \eta^2 b(t) + o(\eta^2)
                $, we find:
                \begin{align*}
                    \theta_{t+1} &=     \theta_t - \eta \nabla l_{n_t} (\theta_t) \\
                                 &\in       \theta_0
                                        +   \eta a(t) + \eta^2 b(t)
                                        -   \eta (
                                                    \nabla l_{n_t}
                                                +   \eta \nabla^2 l_{n_t} a(t) 
                                            )
                                        +   o(\eta^2) \\
                                 &=     \theta_0
                                    +   \eta (a(t) - \nabla l_{n_t})
                                    +   \eta^2 (b(t) - \nabla^2 l_{n_t} a(t)) 
                                    +   o(\eta^2)
                \end{align*}
                To save space, we write $l_{n_t}$ for $l_{n_t}(\theta_0)$.  It's
                enough to solve the recurrence $a(t+1) = a(t) - \nabla l_{n_t}$ and
                $b(t+1) = b(t) - \nabla^2 l_{n_t} a(t)$.  Since $a(0), b(0)$
                vanish, we have $a(t) =-\sum_{0\leq t<T} \nabla l_{n_t}$ and $b(t)
                = \sum_{0\leq t_0 < t_1 < T} \nabla^2 l_{n_{t_1}} \nabla
                l_{n_{t_0}}$.  We now expand $l$:
                \begin{align*}
                    l(\theta_T) \in    l   &+   (\nabla l) (\eta a(T) + \eta^2 b(T)) \\
                                           &+   \frac{1}{2} (\nabla^2 l) (\eta a(T) + \eta^2 b(T))^2
                                            +   o(\eta^2) \\
                                =      l   &+   \eta ((\nabla l) a(T))
                                            +   \eta^2 ((\nabla l) b(T) + \frac{1}{2} (\nabla^2 l) a(T)^2 )
                                            +   o(\eta^2)
                \end{align*}
                Then $\expct{a(T)} = -MN(\nabla l)$ and, since the $N$ many
                singleton batches in each of $M$ many epochs are pairwise
                independent,
                \begin{align*}
                    \expct{(a(T))^2}
                    ~&=
                    \sum_{0\leq t<T} \sum_{0\leq s<T} \nabla l_{n_t} \nabla l_{n_s} \\
                    ~&= 
                    M^2N(N-1)   \expct{\nabla l}^2 +
                    M^2N        \expct{(\nabla l)^2}
                \end{align*}
                Likewise, 
                \begin{align*}
                    \expct{b(T)}
                    = 
                    ~&\sum_{0\leq t_0 < t_1 < T} \nabla^2 l_{n_{t_1}} \nabla l_{n_{t_0}} \\
                    =
                    ~&\frac{M^2N(N-1)}{2} \expct{\nabla^2 l} \expct{\nabla l} + \\
                    ~&\frac{M(M-1)N}{2}  \expct{(\nabla^2 l) (\nabla l)} 
                \end{align*}
                %
                Similarly, for pure GD, we may demand that $a, b$ obey recurrence
                relations $a(t+1) = a(t) - \sum_n \nabla l_n/N$ and
                $b(t+1) = b(t) - \sum_n \nabla^2 l_n a(t)/N$, meaning that
                $a(t) = -t \sum_n \nabla l_n/N$ and
                $b(t) = {t \choose 2} \sum_{n_0} \sum_{n_1} \nabla^2 l_{n_0} \nabla l_{n_1}/N^2$.
                So $\expct{a(T)} = -MN(\nabla l)$ and
                \begin{align*}
                    \expct{(a(T))^2}
                    ~&=
                    M^2 
                    \sum_{n_0} \sum_{n_1} \nabla l_{n_0} \nabla l_{n_1} \\
                    ~&= 
                    M^2 N(N-1)  \expct{\nabla l}^2 + 
                    M^2 N       \expct{(\nabla l)^2}
                \end{align*}
                and
                \begin{align*}
                    \expct{b(T)}
                    = 
                    ~&{MN \choose 2}\frac{1}{N^2}
                    \sum_{n_0} \sum_{n_1} \nabla^2 l_{n_0} \nabla l_{n_1} \\
                    =
                    ~&\frac{M(MN-1)(N-1)}{2} \expct{\nabla^2 l} \expct{\nabla l} + \\
                    ~&\frac{M(MN-1)}{2}      \expct{(\nabla^2 l) (\nabla l)} 
                \end{align*}
            %    $\cdots$
            %\end{shaded}
            %%\newpage
            %\begin{shaded}
            %   $\cdots$
                We see that the expectations for $a$ and $a^2$ agree
                between pure SGD and pure GD.  So only $b$ contributes.  We
                conclude that pure GD's testing loss exceeds pure SGD's by
                \begin{align*}
                       ~&\eta^2
                        \wrap{\frac{M(MN-1)(N-1)}{2}  - \frac{M^2N(N-1)}{2}}
                        \expct{\nabla^2 l} \expct{\nabla l}^2 \\
                    +   ~&\eta^2 
                        \wrap{\frac{M(MN-1)N}{2} - \frac{M(M-1)N}{2}}
                        \expct{(\nabla^2 l) (\nabla l)} \expct{\nabla l} \\
                    = 
                        ~&\eta^2     \frac{M(N-1)}{2}
                    \expct{\nabla l} \wrap{
                          \expct{(\nabla^2 l) (\nabla l)}
                        - \expct{\nabla^2 l} \expct{\nabla l}
                    }
                \end{align*}
                Since $(\nabla^2 l) (\nabla l) = \nabla((\nabla l)^2)/2$, we can 
                summarize this difference as
                $$
                    \eta^2 \frac{M(N-1)}{4}
                    G \nabla C 
                $$
            \end{shaded}

        \subsubsection{Effect of non-Gaussian noise at a minimum.}
            We consider vanilla SGD initialized at a local minimum of the testing loss.
            One expects $\theta$ to diffuse around that minimum according to
            gradient noise.  We compute the effect on testing loss of non-Gaussian
            diffusion.  Specifically, we compare SGD testing loss on the loss
            landscape to SGD testing loss on a different loss landscape defined as a
            Gaussian process whose every covariance agrees with the original
            landscape's.  We work to order $\eta^3$ because at lower orders,
            the Gaussian landscapes will by construction match their non-Gaussian
            counterparts.

            \subthreesect{Diagram Method}
            \colorlet{shadecolor}{moolime}
            \begin{shaded}
                Because $\expct{\nabla l}$ vanishes at initialization, all diagrams
                with a degree-one vertex that is a singleton vanish.  Because we
                work at order $\eta^3$, we consider $3$-edged diagrams.  Finally,
                because all first and second moments match between the two
                landscapes, we consider only diagrams with at least one partition
                of size at least $3$.  The only such test diagram is
                $\sdia{c(012-3)(03-13-23)}$.  This embeds in $T$ ways (one for each
                spacetime cell of vanilla SGD) and has symmetry factor $1/3!$ for a
                total of
                $$
                    \frac{T \eta^3 }{6}
                    \expct{\nabla^3 l}
                    \expct{\nabla l_{n_{t_a}} \nabla l_{n_{t_b}} \nabla l_{n_{t_c}}}
                $$
            \end{shaded}

            %\newpage
            \subthreesect{Direct Perturbation}
            \colorlet{shadecolor}{moosky}
            \begin{shaded}
                We compute the displacement $\theta_T-\theta_0$ to order $\eta^3$ 
                for vanilla SGD.  Expanding
                $
                    \theta_t \in \theta_0 + \eta a_t + \eta^2 b_t + \eta^3 c_t 
                    + o(\eta^3)
                $, we find:
                \begin{align*}
                    \theta_{t+1}
                    =
                    \theta_t    &-  \eta \nabla l_{n_t} (\theta_t) \\
                    \in\theta_0 &+  \eta a_t + \eta^2 b_t + \eta^3 c_t \\
                                &-  \eta \wrap{
                                         \nabla l_{n_t}
                                        +\nabla^2 l_{n_t} (\eta a_t + \eta^2 b_t)
                                        +\frac{1}{2} \nabla^3 l_{n_t} (\eta a_t)^2
                                    }
                                 +  o(\eta^3) \\
                    =
                    \theta_0    &+   \eta   \wrap{a_t - \nabla l_{n_t}} \\
                                &+   \eta^2 \wrap{b_t - \nabla^2 l_{n_t} a_t} \\ 
                                &+   \eta^3 \wrap{
                                         c_t
                                        -\nabla^2 l_{n_t} b_t
                                        -\frac{1}{2} \nabla^3 l_{n_t} a_t^2
                                     }
                                 +   o(\eta^3)
                \end{align*}
                We thus have the recurrences
                $
                    a_{t+1} = a_t - \nabla l_{n_t}
                $,
                $
                    b_{t+1} = b_t - \nabla^2 l_{n_t} a_t
                $, and
                $
                    c_{t+1} = c_t -\nabla^2 l_{n_t} b_t 
                                  -\frac{1}{2} \nabla^3 l_{n_t} a_t^2
                $
                with solutions:
                $a_t = -\sum_{t} \nabla l_{n_t}$ and
                $\eta^2 b_t = +\eta^2 \sum_{t_0 < t_1} \nabla^2 l_{n_{t_1}} \nabla l_{n_{t_0}}$.
                We do not compute $c_t$ because we will soon see that it will be
                multiplied by $0$.
                %
                To third order, the testing loss of SGD is
                \begin{align*}
                    l(\theta_T)
                    \in
                            l(\theta_0)
                    &+     (\nabla   l)   (\eta a_T + \eta^2 b_T + \eta^3 c_T)                              \\
                    &+\frac{\nabla^2 l}{2}(\eta a_T + \eta^2 b_T             )^2                            \\
                    &+\frac{\nabla^3 l}{6}(\eta a_T                          )^3 
                     +o(\eta)^3                                                                             \\
                    =
                        l(\theta_0)
                    &+  \eta       \wrap{(\nabla l) a_T                               }                     \\
                    &+  \eta^2     \wrap{(\nabla l) b_T + \frac{\nabla^2 l}{2} a_T^2  }                     \\
                    &+  \eta^3     \wrap{(\nabla l) c_T + (\nabla^2 l) a_T b_T + \frac{\nabla^3 l}{6} a_T^3}
                     +o(\eta)^3                                                                             
                \end{align*}
                Because $\expct{\nabla l}$ vanishes at initialization, we neglect
                the $(\nabla l)$ terms.  The remaining $\eta^3$ terms involve
                $a_T b_T$, and $a_T^3$.  So let us
                compute their expectations:
                \begin{align*}
                    \expct{a_T b_T}
                        =&- \sum_{t} \sum_{t_0 < t_1}
                            \expct{\nabla l_{n_t} \nabla^2 l_{n_{t_1}} \nabla l_{n_{t_0}}}
                        \\
                        =&- \sum_{t_0 < t_1}  
                            \sum_{t \notin \{t_0, t_1\}} 
                                \expct{\nabla l_{n_t}} \expct{\nabla^2 l_{n_{t_1}}} \expct{\nabla l_{n_{t_0}}}
                        \\&- \sum_{t_0 < t_1}  
                            \sum_{t = t_0}
                                \expct{\nabla l_{n_t} \nabla l_{n_{t_0}}} \expct{\nabla^2 l_{n_{t_1}}}
                        \\&- \sum_{t_0 < t_1}  
                            \sum_{t = t_1}
                                \expct{\nabla l_{n_t} \nabla^2 l_{n_{t_1}}} \expct{\nabla l_{n_{t_0}}}
                \end{align*}
            %\end{shaded}
            %\newpage
            %\begin{shaded}
                Since $\expct{\nabla l}$ divides $\expct{a_T b_T}$, the latter
                vanishes.
                \begin{align*}
                    \expct{a_T^3}
                        =&- \sum_{t_a, t_b, t_c}
                                \expct{\nabla l_{n_{t_a}} \nabla l_{n_{t_b}} \nabla l_{n_{t_c}}}
                        \\
                        =&- \sum_{\substack{t_a, t_b, t_c\\ \text{disjoint}}}  
                                \expct{\nabla l_{n_{t_a}}} \expct{\nabla l_{n_{t_b}}} \expct{\nabla l_{n_{t_c}}}
                        \\&-3 \sum_{t_a=t_b\neq t_c}  
                                \expct{\nabla l_{n_{t_a}} \nabla l_{n_{t_b}}} \expct{\nabla l_{n_{t_c}}}
                        \\&-\sum_{t_a=t_b=t_c}  
                                \expct{\nabla l_{n_{t_a}} \nabla l_{n_{t_b}} \nabla l_{n_{t_c}}}
                \end{align*}
                As we initialize at a test minimum, only the last line remains, at
                it has $T$ identical summands.
                When we plug into the expression for SGD testing loss, we get
                $$
                    \frac{T \eta^3 }{6}
                    \expct{\nabla^3 l}
                    \expct{\nabla l_{n_{t_a}} \nabla l_{n_{t_b}} \nabla l_{n_{t_c}}}
                $$
            \end{shaded}



%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Math  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\newpage
\section{Mathematics of the theory}\label{appendix:math}
    \subsection{Assumptions and Definitions}                        \label{appendix:assumptions}
        We assume throughout this work the following regularity properties of
        the loss landscape.
        
        \textbf{Existence of Taylor Moments} --- we assume
        that each finite collection of polynomials of the $0$th and higher
        derivatives of the $l_x$, all evaluated at any point $\theta$, may be
        considered together as a random variable insofar as they are equipped
        with a probability measure upon of the standard Borel algebra.

        \textbf{Analyticity Uniform in Randomness} --- we assume that
        the functions $\theta \mapsto l_x(\theta)$ --- and the expectations
        of polynomials of their $0$th and higher derivatives --- exist and are
        analytic with radii of convergence bounded from $0$ (by a potentially
        $\theta$-dependent function).  So expectations and derivatives commute. 

        \textbf{Boundedness of Gradients} --- we also assume that the gradients
        $\nabla l_x(\theta)$, considered as random covectors, are bounded by
        some continuous function of $\theta$.\footnote{
            Some of our experiments involve Gaussian noise, which is not
            bounded and so violates the hypothesis.  In practice, Gaussians are
            effectively bounded
            %, on the one hand in that with high
            %probability no standard normal sample encountered on Gigahertz
            %hardware within the age of the universe will much exceed $\sqrt{2
            %\log(10^{30})} \approx 12$, and on the other hand
            in that
            our predictions vary smoothly with the first few moments of this
            distribution, so that a $\pm 12$-clipped Gaussian will yield almost
            the same predictions.
        }
        A metric-independent way of expressing this boundedness constraint
        is that the gradients all lie in some subset $\Ss \subseteq TM$ of
        the tangent bundle of weight space, where, for any compact $\Cc
        \subseteq M$, we have that the topological pullback --- of
        $\Ss \hookrightarrow TM \twoheadrightarrow M$
        and
        $\Cc \hookrightarrow M$ ---
        is compact.
        
        Now we turn to definitions.

        \begin{dfn}[Diagrams] \label{dfn:diagrams}
            A diagram is a finite rooted tree equipped with a partition of
            nodes.  We draw the tree using thin ``edges''.  By
            convention, we draw each node to the right of its children; the
            root is thus always rightmost.  We draw the partition
            by connecting the nodes within each part via fuzzy ``ties''.  For
            example, $\sdia{c(012-3)(03-13-23)}$ has $2$ parts.
            %
            We insist on using as few fuzzy ties as possible so that, if $d$
            counts edges and $c$ counts ties, then $d+1-c$ counts parts. 
            %
            There may
            be multiple ways to draw a single diagram, e.g.
            $\sdia{c(01-23)(03-13-23)}=\sdia{(02-13)(03-13-23)}$. 
        \end{dfn}
        \begin{dfn}[Embedding a Diagram into a Grid]
            An embedding of a diagram into a grid is an assignment of that
            diagram's non-root nodes to pairs $(n,t)$ such that each node
            occurs at a time $t^\prime$ strictly after each of its children and
            such that two nodes occupy the same row $n$ if they
            inhabit the same part of $D$'s partition.
            %
            %We say an embedding is \emph{strict} if it assigns to each part
            %a different datapoint $n$.
        \end{dfn}
        We define $\uvalue(D)$ and $\rvalue_f(D)$ as in
        \S\ref{appendix:evaluate-embeddings}.

    \subsection{A key lemma \`a la Dyson}                           \label{appendix:key-lemma}

        Suppose $s$ is an analytic function defined on the space of weights.
        The following Lemma, reminiscent of \cite{dy49a}, helps us track
        $s(\theta)$ as SGD updates $\theta$:
        \begin{klem*} \label{lem:dyson}
            For all $T$: for $\eta$ sufficiently small, $s(\theta_T)$ is a sum
            over tuples of natural numbers:
            \begin{equation}\label{eq:dyson2}
                \sum_{(d_t: 0\leq t<T) \in \NN^T}
                (-\eta)^{\sum_t d_t}
                \wrap{
                    \prod_{0 \leq t < T}
                        \wrap{\left.
                            \frac{(g \nabla)^{d_t}}{d_t!}
                        \right|_{g = \sum_{n\in \Bb_t} \nabla l_n(\theta) / B}}
                }(s) (\theta_0)
            \end{equation}
            Moreover, the expectation symbol (over training sets) commutes with
            the sum over $d$s.
        \end{klem*}
        Here, we consider each $(g \nabla)^{d_t}$ as a higher order function
        that takes in a function $f$ defined on weight space and outputs a
        function equal to the $d_t$th derivative of $f$, times $g^{d_t}$.
        The above product then indicates composition of $(g \nabla)^{d_t}$'s
        across the different $t$'s.  In total, that product takes the function
        $s$ as input and outputs a function equal to some polynomial of $s$'s
        derivatives.

        \begin{proof}[Proof of the Key Lemma]%
            We work in a neighborhood of the initialization so that the tangent
            space of weight space is a trivial bundle.  For convenience, we fix
            a  coordinate system, and with it the induced flat,
            non-degenerate inverse metric $\tilde\eta$; the benefit is that we
            may compare our varying $\eta$ against one fixed $\tilde\eta$.
            Henceforth, a ``ball'' unless otherwise specified will mean a ball
            with respect to $\tilde\eta$ around the initialization $\theta_0$.
            Since $s$ is analytic, its Taylor series converges to $s$ within
            some positive radius $\rho$ ball.  By assumption, every $l_t$ is
            also analytic with radius of convergence around $\theta_0$ at least
            some $\rho>0$.  Since gradients are $x$-uniformly
            bounded by a continuous function of $\theta$, and since in finite
            dimensions the closed $\rho$-ball is compact, we have a strict
            gradient bound $b$ uniform in both $x$ and $\theta$ on gradient
            norms within that closed ball.  When
            \begin{equation} \label{eq:smalleta}
                2 \eta T b < \rho \tilde\eta
            \end{equation}
            as norms, SGD after $T$ steps on any train set
            will necessarily stay within the $\rho$-ball.\footnote{
                The $2$ ensures that SGD initialized at
                any point within a $\rho/2$ ball will necessarily stay within
                the $\rho$-ball.
            } We note that the above condition on $\eta$ is weak enough to
            permit all $\eta$ within some open neighborhood of $\eta=0$.  

            Condition \ref{eq:smalleta} together with analyticity of $s$ then
            implies that
            $
                \wrap{\exp(-\eta g \nabla) s}(\theta) = s(\theta - \eta g)
            $
            when $\theta$ lies in the $\tilde\eta$ ball (of radius $\rho$) and
            its $\eta$-distance from that $\tilde\eta$ ball's boundary exceeds
            $b$, and that both sides are analytic in $\eta, \theta$ on the same
            domain --- and \emph{a fortiori} when $\theta$ lies in the ball of
            radius $\rho (1 - 1/(2T))$.  Likewise, a routine induction through
            $T$ gives the value of $s$ (after doing $T$ gradient steps from an
            initialization $\theta$) as
            $$
                \wrap{
                    \prod_{0\leq t<T}
                        \left.
                            \exp(-\eta g \nabla)
                        \right|_{g=\nabla l_t(\theta)}
                }
                (s)(\theta)
            $$
            for any $\theta$ in the $\rho (1-T/(2T)$-ball (that is, the
            $\rho/2$-ball), and that both sides are analytic in $\eta, \theta$
            on that same domain.  Note that in each exponential, the
            $\nabla_\nu$ does not act on the $\nabla_\mu l(\theta)$ with which
            it pairs.  

            Now we use the standard expansion of $\exp$.  Because (by
            analyticity) the order $d$ coeffients of $l_t, s$ are bounded by
            some exponential decay in $d$ that has by assumption an $x$-uniform
            rate, we have absolute convergence and may rearrange sums.  We
            choose to group by total degree:
            \begin{equation} \label{eq:expansion}
                \cdots 
                =
                \sum_{0\leq d < \infty} (-\eta)^d
                \sum_{\substack{(d_t: 0\leq t<T) \\ \sum_t d_t = d}}
                \wrap{
                    \prod_{0 \leq t < T} \left.
                        \frac{(g \nabla)^{d_t}}{d_t!}
                    \right|_{g=\nabla l_t(\theta)}
                } s (\theta)
            \end{equation}
            The first part of the Key Lemma is proved.  It remains to show that
            expectations over train sets commute with the above summation.

            We will apply Fubini's Theorem.  To do so, it suffices to show that   
            $$
                \wabs{c_d((l_t: 0\leq t<T))} 
                \triangleq
                \wabs{
                    \sum_{\substack{(d_t: 0\leq t<T) \\ \sum_t d_t = d}}
                    \wrap{
                        \prod_{0 \leq t < T} \left.
                            \frac{(g \nabla)^{d_t}}{d_t!}
                        \right|_{g=\nabla l_t(\theta)}
                    } s (\theta)
                }
            $$
            has an expectation that decays exponentially with $d$.  The symbol
            $c_d$ we introduce purely for convenience; that its value depends
            on the train set we emphasize using function application
            notation.  Crucially, no matter the train set, we have shown
            that the expansion \ref{eq:expansion} (that features $c_d$ appear
            as coefficients) converges to an analytic function for all $\eta$
            bounded as in condition \ref{eq:smalleta}.  The uniformity of this
            demanded bound on $\eta$ implies by the standard relation between
            radii of convergence and decay of coefficients that $\wabs{c_d}$
            decays exponentially in $d$ at a rate uniform over train sets.
            If the expectation of $\wabs{c_d}$ exists at all, then, it will
            likewise decay at that same shared rate.
            
            Finally, $\wabs{c_d}$ indeed has a well-defined expected value, for
            $\wabs{c_d}$ is a bounded continuous function of a
            (finite-dimensional) space of $T$-tuples (each of whose entries can
            specify the first $d$ derivatives of an $l_t$) and because the
            latter space enjoys a joint distribution.  So Fubini's Theorem
            applies.  The Key Lemma follows.   
        \end{proof}

    \subsection{From Dyson to diagrams}                             \label{appendix:toward-diagrams}

        {\color{red} TODO: define diagrams! FILL IN }

        We now describe the terms that appear in the Key Lemma.  The following
        result looks like Theorem \ref{thm:resum}, except it has $\uvalue(D)$
        instead of $\uvalue_f(D)$, and the sum is over all diagrams, not just
        linkless ones.  In fact, we will use Theorem \ref{thm:pathint} to
        prove Theorem \ref{thm:resum}.

        \begin{thm}[Test Loss as a Path Integral] \label{thm:pathint}
            For all $T$: for $\eta$ sufficiently small, SGD's expected test
            loss is
            \begin{equation*}\label{eq:sgdcoef}
                \sum_{D}
                %\wrap{
                    \sum_{\text{embeddings}~f}
                    \frac{1}{\wabs{\Aut_f(D)}}
                %}
                \frac{\uvalue(D)}{(-B)^{|\edges(D)|}}
            \end{equation*}
            Here, $D$ is a diagram whose root $r$ does not participate in
            any fuzzy edge, $f$ is an embedding of $D$ into a grid, and
            $\wabs{\Aut_f(D)}$ counts the graph-automorphisms of $D$ that
            preserve $f$'s assignment of nodes to cells.
            %
            If we replace $D$ by 
            $
                \wrap{-\sum_{p \in \parts(D)} (D_{rp} - D)/N}
            $, where $r$ is $D$'s root,
            we obtain the expected generalization gap (testing minus training loss).
        \end{thm}

        Theorem \ref{thm:pathint} describe the terms that appear in the Key
        Lemma by matching each term to an embedding of a diagram in a grid,
        so that the infinite sum becomes a sum over all diagram grid 
        configurations.  The main idea is that the combinatorics of diagrams
        parallels the combinatorics of repeated applications of the product
        rule for derivatives applied to the expression in the Key Lemma.
        Balancing against this combinatorial explosion are factorial-style
        denominators, again from the Key Lemma, that we summarize in terms of
        the sizes of automorphism groups.

        \begin{proof}[Proof of Theorem \ref{thm:pathint}]
            We first prove the statement about testing losses.
            Due to the analyticity property established in our proof of the
            Key Lemma, it suffices to show agreement at each degree $d$ and
            train set individually.  That is, it suffices to show --- for
            each train set $(l_n: 0\leq n<N)$, grid $S$, function $\pi:
            S\to [N]$ that induces $\sim$, and natural $d$ --- that
            \begin{align} \label{eq:toprove}
                (-\eta)^d
                \sum_{\substack{
                    (d_t: 0\leq t<T) \\
                    \sum_t d_t = d
                }}
                \wrap{
                    \prod_{0 \leq t < T} \left.
                        \frac{(g \nabla)^{d_t}}{d_t!}
                    \right|_{g=\nabla l_t(\theta)}
                } l (\theta)
                = \nonumber \\
                \sum_{\substack{
                    D \in \image(\Free) \\
                    \textnormal{with $d$ edges}
                }}
                \wrap{
                    \sum_{f: D\to\Free(S)}
                    \frac{1}{\wabs{\Aut_f(D)}}
                }
                \frac{\uvalue_\pi(D, f)}{B^{d}}
            \end{align}
            Here, $\uvalue_\pi$ is the value of a diagram embedding before
            taking expectations over train sets.  We have for all $f$ that
            $\expct{\uvalue_\pi(D, f)} = \uvalue(D)$.
            Observe that both sides of \ref{eq:toprove} are finitary sums.

            \begin{rmk}[Differentiating Products] \label{rmk:leibniz}
                The product rule of Leibniz easily generalizes to higher
                derivatives of finitary products:
                $$
                    \nabla^{\wabs{M}} \prod_{k \in K} p_k
                    = 
                    \sum_{\nu:M\to K} \prod_{k\in K} \wrap{
                        \nabla^{\wabs{\nu^{-1}(k)}} p_k
                    }
                $$
                The above has $\wabs{K}^{\wabs{M}}$ many term indexed by
                functions to $K$ from $M$.
            \end{rmk}

            We proceed by joint induction on $d$ and $S$.  The base cases
            wherein $S$ is empty or $d=0$ both follow immediately from the Key
            Lemma, for then the only embedding is the unique embedding of the
            one-node diagram $\sdia{(0)()}$.  For the induction step, suppose
            $S$ is a sequence of $\Mm = \min S \subseteq S$ followed by a
            strictly smaller $S$ and that the result is proven for $(\tilde d,
            \tilde S)$ for every $\tilde d \leq d$.  Let us group by $d_0$ the
            terms on the left hand side of desideratum \ref{eq:toprove}.
            Applying the induction hypothesis with $\tilde d = d - d_0$, we
            find that that left hand side is:
            \begin{align*}
                \sum_{\substack{
                    0 \leq d_0 \leq d
                }}
                \sum_{\substack{
                    \tilde D \in \image(\Free) \\
                    \textnormal{with $d-d_0$ edges}
                }}
                \frac{1}{d_0!}
                \sum_{\tilde f: \tilde D\to\Free(\tilde S)} \wrap{
                    \frac{1}{\wabs{\Aut_{\tilde f}(\tilde D)}}
                }
                ~\cdot~
                \\ %---------------------------------------------
                (-\eta)^{d_0}
                \left.
                    (g \nabla)^{d_0}
                \right|_{g=\nabla l_0(\theta)}
                \frac{\uvalue_\pi(\tilde D, \tilde f)}{B^{d-d_0}}
            \end{align*}
            Since $\uvalue_\pi(\tilde D, \tilde f)$ is a multilinear product of
            $d-d_0+1$ many tensors, the product rule for derivatives tells us
            that $(g \nabla)^{d_0}$ acts on $\uvalue_\pi(\tilde D, \tilde f)$
            to produce $(d-d_0+1)^{d_0}$ terms.  In fact,
            $
                g = \sum_{m\in \Mm} \nabla l_m(\theta) / B
            $ 
            expands to
            $B^{d_0}(d-d_0+1)^{d_0}$ terms, each conveniently indexed
            by a pair of functions $\beta:[d_0]\to \Mm$ and $\nu:[d_0]\to
            \tilde D$.  The $(\beta, \nu)$-term corresponds to an embedding
            $f$ of a larger diagram $D$ in the sense that it contributes
            $\uvalue_\pi(D, f)/B^{d_0}$ to the sum.  Here, $(f, D)$ is $(\tilde
            f, \tilde D)$ with $\wabs{\wrap{\beta \times \nu}^{-1}(n, v)}$ many
            additional edges from the cell of datapoint $n$ at time $0$ to the
            $v$th node of $\tilde D$ as embedded by $\tilde f$.

            By the Leibniz rule of Remark \label{rmk:leibniz}, this $(\beta,
            \nu)$-indexed sum by corresponds to a sum over embeddings $f$ that
            restrict to $\tilde f$, whose terms are multiples of the value of
            the corresponding embedding of $D$.  Together with the sum over
            $\tilde f$, this gives a sum over all embeddings $f$.  So we now
            only need to check that the coefficients for each $f:D\to S$ are as
            claimed.

            We note that the $(\beta, \nu)$ diagram (and its value) agrees with
            the $(\beta \circ \sigma, \nu \circ \sigma)$ diagram (and its
            value) for any permutation $\sigma$ of $[d_0]$.  The corresponding
            orbit has size
            \begin{align*}
                \frac{d_0!}{
                    \prod_{(m, i) \in \Mm \times \tilde D}
                        \wabs{(\beta \times \nu)^{-1}(m, i)}!
                }
            \end{align*}
            by the Orbit Stabilizer Theorem of elementary group theory.   

            It is thus enough to show that
            \begin{align*} \label{eqn:countclaim}
                \wabs{\Aut_f(D)} = 
                \wabs{\Aut_{\tilde f}(D)}
                \prod_{(m, i) \in \Mm \times \tilde D}
                    \wabs{(\beta \times \nu)^{-1}(m, i)}!
            \end{align*}
            We will show this by a direct bijection.  First, observe that
            $
                f = \beta \sqcup \tilde f:
                    [d_0] \sqcup \tilde D \to \Mm \sqcup \tilde S
            $. 
            So each automorphism $\phi: D\to D$ that commutes with $f$ induces
            both an automorphism
            $
                \Aa = \phi|_{\tilde D}: \tilde D\to \tilde D
            $
            that commutes with $\tilde f$ together with the data of a map
            $
                \Bb = \phi_{[d_0]}: [d_0] \to [d_0] 
            $
            that both commutes with $\beta$.  However, not every such pair of
            maps arises from a $\phi$.  For, in order for $\Aa \sqcup \Bb: D
            \to D$ to be an automorphism, it must respect the order structure
            of $D$.  In particular, if $x\leq_D y$ with $x \in [d_0]$ and $y
            \in \tilde D$, then we need
            $$
                \Bb(x) \leq_D \Aa(y)
            $$
            as well.  The
            pairs $(\Aa, \Bb)$ that thusly preserve order are in bijection with
            the $\phi \in \Aut_f(D)$.  There are $\wabs{\Aut_{\tilde f}(\tilde
            D)}$ many $\Aa$.  For each $\Aa$, there are as many $\Bb$ as there
            are sequences $(\sigma_i: i \in \tilde D)$ of permutations on
            $
                \{j\in [d_0]: j\leq_D i\} \subseteq [d_0]
            $ 
            that commute with $\Bb$.  These permutations may be chosen
            independently; there are 
            $
                \prod_{m\in \Mm}
                    \wabs{(\beta \times \nu)^{-1}(m, i)}!
            $
            many choices for $\sigma_i$.  Claim \ref{eqn:countclaim} follows,
            and with it the correctness of coefficients.
 
            The argument for generalization gaps parallels the above when we
            use $l-\sum_n l_n/N$ instead of $l$ as the value for $s$. 
            Theorem \ref{thm:pathint} is proved.
        \end{proof}

        \begin{rmk}[The Case of $E=B=1$ SGD]
            The grid of $E=B=1$ SGD permits all and only those
            embeddings that assign to each part of a diagram's partition  a
            distinct cell.  Such embeddings factor through a diagram
            ordering and are thus easily counted using factorials per
            Proposition \ref{prop:vanilla}.  That proposition immediately
            follows from the now-proven Theorem \ref{thm:pathint}.
        \end{rmk}

        \begin{prop} \label{prop:vanilla}
            The order $\eta^d$ contribution to the expected testing loss of
            one-epoch SGD with singleton batches is:
            \begin{equation*}\label{eq:sgdbasiccoef}
                \frac{(-1)^d}{d!} \sum_{D} 
                |\ords(D)| {N \choose P-1} {d \choose d_0,\cdots,d_{P-1}}
                \uvalue(D)
            \end{equation*}
            where $D$ ranges over $d$-edged diagrams.  Here, $D$'s parts have
            sizes $d_p: 0\leq p\leq P$, and $|\ords(D)|$ counts the total
            orderings of $D$ s.t.\ children precede parents and parts are
            contiguous.
        \end{prop}

    \subsection{Interlude: a review of M\"obius inversion}          \label{appendix:mobius}

        We say an embedding is \textbf{strict} if it assigns to each part
        a different datapoint $n$.
        Then, by M\"obius inversion (\cite{ro64}), a sum over strict embeddings
        of moment values (\S\ref{appendix:evaluate-embeddings}) matches 
        a sum over all embeddings of $\uvalue$s.

    \subsection{Theorems \ref{thm:resum} and \ref{thm:converge}}    \label{appendix:resum}

        The diagrams summed in Theorem \ref{thm:resum} and \ref{thm:converge}
        may be grouped by their geometric realizations.  Each nonempty class of
        diagrams with a given geometric realization has a unique element with
        minimally many edges, and in this way all and only linkless diagrams
        arise. 

        We encounter two complications: on one hand, that the sizes of
        automorphism groups might not be uniform among the class of diagrams
        with a given geometric realization.  On the other hand, that the
        embeddings of a specific member of that class might be hard to count.
        The first we handle using Orbit-Stabilizer.  The second we address as
        described by \S\ref{appendix:mobius} via M\"obius sums.
           
        \begin{proof}[Proof of Theorem \ref{thm:resum}]
            We apply M\"obius inversion (\S\ref{appendix:mobius}) to Theorem
            \ref{thm:pathint} (\S\ref{appendix:toward-diagrams}).  The result
            is that chains of embeddings  
            {\color{red} FILL IN}

            The difference in loss from the noiseless case is given by all the
            diagram embeddings with at least one fuzzy tie, where the fuzzy tie
            pattern is actually replaced by a difference between noisy and
            noiseless cases as prescribed by the preceding discussion on
            M\"obius Sums.  Beware that even relatively noiseless embeddings
            may have illegal collisions of non-fuzzily-tied nodes within a
            single grid (data) row.  Throughout the rest of this proof, we
            permit such illegal embeddings of the fuzz-less diagrams that arise
            from the aforementioned decomposition.  

            Because the Taylor series for analytic functions converge
            absolutely in the interior of the disk of convergence, the
            rearrangement of terms corresponding to a grouping by geometric
            realizations preserves the convergence result of Theorem
            \ref{thm:pathint}.  

            Let us then focus on those diagrams $\sigma$ with a given geometric
            realization represented by an linkless diagram $\rho$.  By
            Theorem \ref{thm:pathint}, it suffices to show that
            \begin{equation} \label{eq:hard}
                \sum_{f:\rho\to S}
                \sum_{\substack{
                    \tilde f:\sigma\to S \\
                    \exists i_\star: f=\tilde f \circ i_\star
                }}
                \frac{1}{\wabs{\Aut_{\tilde f}(\sigma)}}
                =
                \sum_{f:\rho\to S}
                \sum_{\substack{
                    \tilde f:\sigma\to S \\
                    \exists i_\star: f=\tilde f \circ i_\star
                }}
                \sum_{\substack{
                    i:\rho\to\sigma \\
                    f = \tilde f \circ i
                }}
                \frac{1}{\wabs{\Aut_{f}(\rho)}}
            \end{equation}
            Here, $f$ is considered up to an equivalence defined by
            precomposition with an automorphism of $\rho$.  We likewise
            consider $\tilde f$ up to automorphisms of $\sigma$.  And above,
            $i$ ranges through maps that induce isomorphisms of geometric
            realizations, where $i$ is considered equivalent to $\hat i$ when
            for some automorphism $\phi \in \Aut_{\tilde f}(\sigma)$, we have
            $\hat i = i \circ \phi$.  Name as $X$ the set of all such $i$s
            under this equivalence relation.

            In equation \ref{eq:hard}, we have introduced
            redundant sums to structurally align the two expressions on the
            page; besides this rewriting, we see that equation \ref{eq:hard}'s
            left hand side matches Theorem \ref{thm:pathint} resulting formula
            and tgat its right hand side is the desired formula of Theorem
            \ref{thm:resum}. 

            To prove equation \ref{eq:hard}, it suffices to show (for any
            $f, \tilde f, i$ as above) that
            $$
                \wabs{\Aut_f(\rho)}
                =
                \wabs{\Aut_{\tilde f}(\sigma)}
                \cdot
                \wabs{X}
            $$
            We will prove this using the Orbit Stabilizer Theorem by presenting
            an action of $\Aut_f(\rho)$ on $X$.  We simply use precomposition
            so that $\psi\in \Aut_f(\rho)$ sends $i\in X$ to $i\circ \psi$.
            Since $f\circ\psi = f$, $i\circ \psi \in X$.  Moreover, the action
            is well-defined, because if $i\sim \hat i$ by $\phi$, then $i \circ
            \psi \sim \hat i \circ \psi$ also by $\phi$.
            
            The stabilizer of $i$ has size $\wabs{\Aut_{\tilde f}(\rho)}$.
            For, when $i \sim i \circ \psi$ via $\phi \in \Aut_{\tilde
            f}(\rho)$, we have $i\circ \psi = \phi \circ i$.  This relation in
            fact induces a bijective correspondence: \emph{every} $\phi$
            induces a $\psi$ via $\psi = i^{-1} \circ \phi \circ i$, so we have
            a map $\text{stabilizer}(i) \hookleftarrow \Aut_{\tilde f}(\rho)$
            seen to be well-defined and injective because structure set
            morphisms are by definition strictly increasing and because $i$s
            must induce isomorphisms of geometric realizations.  Conversely,
            every $\psi$ that stabilizes enjoys \emph{only} one $\phi$ via
            which $i \sim i \circ \phi$, again by the same (isomorphism and
            strict increase) properties.  So the stabilizer has the claimed
            size.

            Meanwhile, the orbit is all of $\wabs{X}$.  Indeed, suppose $i_A,
            i_B \in X$.  We will present $\psi \in \Aut_f(\rho)$ such that $i_B
            \sim i_A \circ \psi$ by $\phi=\text{identity}$.  We simply define
            $\psi = i_A^{-1} \circ i_B$, well-defined by the aforementioned
            (isomorphisms and strict increase) properties.  It is then routine
            to verify that
            $
                f \circ \psi
                =
                \tilde f \circ i_A \circ i_A^{-1} \circ i_B
                =
                \tilde f \circ i_B
                = f.
            $
            So the orbit has the claimed size, and by the Orbit Stabilizer
            Theorem, the coefficients in the expansions of Theorems 
            \ref{thm:resum} and \ref{thm:pathint} match.
        \end{proof}

        \begin{proof}[Proof of Theorem \ref{thm:converge}]
            Since we assumed hessians are positive: for any $m$, the propagator
            $K^t = \wrap{(I-\eta H)^{\otimes m}}^t$ exponentially decays to $0$
            (at a rate dependent on $m$).  Since up to degree $d$ only a finite
            number of diagrams exist and hence only a finite number of possible
            $m$s, the exponential rates are bounded away from $0$.  Moreover,
            for any fixed $t_{\text{big}}$, the number of diagrams ---
            involving no exponent $t$ exceeding $t_{\text{big}}$ --- is
            eventually constant as $T$ grows.  Meanwhile, the number involving
            at least one exponent $t$ exceeding that threshold grows
            polynomially in $T$ (with degree $d$).  The exponential decay of
            each term overwhelms the polynomial growth in the number of terms,
            and the convergence statement follows.
        \end{proof}

    %\subsection{How to modify proofs to handle variants}            \label{appendix:prove-variants}

    \subsection{Proofs of corollaries}                              \label{appendix:corollaries}

        \subsubsection{Corollary \ref{cor:entropic}}

            \begin{proof}
                The relevant linkless diagram is $\sdia{c(01-2-3)(02-12-23)}$
                {color{red} (amputated as in the previous subsubsection)}.   
                An embedding of this diagram into $E=B=1$ SGD's grid 
                is determined by two durations --- 
                $t$ from {\color{moor}red} to {\color{moog}green} and
                $\tilde t$ from {\color{moog}green} to {\color{moob}blue} ---
                obeying $t+\tilde t \leq T$.
                The automorphism group of each embedding has size $2$: identity
                or switch the {\color{moor}red} nodes.  So the answer is: 
                $$
                    C_{\mu \nu}
                    J^{\rho\lambda}_{\sigma}
                    \wrap{\int_{t+\tilde t\leq T}
                        \wrap{\exp(-t \eta H) \eta}^{\mu\rho}
                        \wrap{\exp(-t \eta H) \eta}^{\nu\lambda}
                        \wrap{\exp(-\tilde t \eta H) \eta}^{\sigma\pi}
                    }
                $$
                Standard calculus then gives the desired result.
            \end{proof}

        \subsubsection{Corollary \ref{cor:overfit}'s first part}

            \begin{proof}[Proof.]
                The relevant linkless diagram is $\sdia{(01-2)(02-12)}$
                (which equals $\sdia{c(01-2)(02-12)}$ because we are at a test
                minimum).  This diagram has one embedding for each pair of
                same-row shaded cells, potentially identical, in a grid; for
                GD, the grid has every cell shaded, so each
                \emph{non-decreasing} pair of durations in $[0,T]^2$ is
                represented; the symmetry factor for the case where the cells
                is identical is $1/2$, so we lose no precision by interpreting
                a automorphism-weighted sum over the \emph{non-decreasing}
                pairs as half of a sum over all pairs.  Each of these may embed
                into $N$ many rows, hence the factor below of $N$.  The two
                integration variables (say, $t, \tilde t$) separate, and we
                have:
                $$
                    \frac{N}{B^{\text{degree}}}
                    \frac{C_{\mu\nu}}{2}
                    \int_t \wrap{\exp(-t \eta H)}^\mu_\lambda
                    \int_{\tilde t} \wrap{\exp(-\tilde t \eta H)}^\nu_\rho
                    \eta^{\lambda\sigma}
                    \eta^{\rho\pi}
                    H_{\sigma\pi}
                $$
                Since for GD we have $N=B$ and we are working to degree $2$,
                the prefactor is $1/N$.  Since $\int_t \exp(a t) = (I-\exp(-a
                T))/a$, the desired result follows. 
            \end{proof}

        \subsubsection{Corollary \ref{cor:overfit}'s second part}

            We apply the generalization gap modification (described in
            \S\ref{appendix:solve-variants}) to Theorem \ref{thm:resum}'s
            result about testing losses.

            \begin{proof}[Proof]
                The relevant linkless diagram is $\sdia{c(01)(01)}$.  This
                diagram has one embedding for each shaded cell of grid;
                for GD, the grid has every cell shaded, so each duration
                from $0$ to $T$ is represented.  So the generalization gap is,
                to leading order,
                $$
                    + \frac{C_{\mu\nu}}{N}
                    \int_t \wrap{\exp(-t \eta H)}^\mu_\lambda
                    \eta^{\lambda\nu}
                $$
                Here, the minus sign from the gen-gap modification canceled
                with the minus sign from the odd power of $-\eta$.  Integration
                finishes the proof.
            \end{proof}
 
        \subsubsection{Corollaries \ref{cor:epochs} and \ref{cor:batch}}

            Corollary \ref{cor:epochs} and Corollary \ref{cor:batch} follow
            from plugging appropriate values of $M, N, B$ into the following
            proposition.

            \begin{prop}\label{prop:ordtwo}
                To order $\eta^2$, the testing loss of SGD --- on $N$
                samples for $M$ epochs with batch size $B$ dividing $N$ and with any
                shuffling scheme --- has expectation
                {\small
                \begin{align*}
                                                            l              
                    &- MN                                   G_\mu G^\mu       
                     + MN\wrap{MN - \frac{1}{2}}            G_\mu H^{\mu}_{\nu} G^\nu \\
                    &+ MN\wrap{\frac{M}{2}}                 C_{\mu \nu} H^{\mu \nu}
                     + MN\wrap{\frac{M-\frac{1}{B}}{2}}     \wrap{\nabla_\mu C^{\nu}_{\nu}} G^\mu / 2
                \end{align*}
                }
            \end{prop}

            \begin{proof}[of Proposition \ref{prop:ordtwo}]
                To prove Proposition \ref{prop:ordtwo}, we simply count
                the embeddings of the diagrams, noting that the automorphism groups
                are all of size $1$ or $2$.  Since we use fuzzy outlines instead of
                fuzzy ties, we allow untied nodes to occupy the same row, since the
                excess will be canceled out by the term subtract in the definition of
                fuzzy outlines.  See Table \ref{tbl:ordtwo}.
                \begin{table}[h]
                    \centering
                    \begin{tabular}{cll}
                        diagram                 & embed.s w/ $\wabs{\Aut_f}=1$  & embed.s w/ $\wabs{\Aut_f}=2$   \\ \hline
                        $\sdia{(0)()}$          & $1$                           & $0$                            \\  
                        $\sdia{(0-1)(01)}$      & $MNB$                         & $0$                            \\                  
                        $\sdia{(0-1-2)(01-12)}$ & ${MNB\choose 2}$              & $0$                            \\
                        $\sdia{c(01-2)(01-12)}$ & $N{MB\choose 2}$              & $0$                            \\
                        $\sdia{(0-1-2)(02-12)}$ & ${MNB\choose 2}$              & $0$                            \\
                        $\sdia{c(01-2)(02-12)}$ & $N{MB\choose 2}$              & $MNB$                             
                    \end{tabular}
                    \label{tbl:ordtwo}
                \end{table}
            \end{proof}

        \subsubsection{Corollary \ref{cor:vsode}}

            The corollary's first part follows immediately from 
            Proposition \ref{prop:ordtwo}.

            \begin{proof}[Proof of second part]
                Because $\expct{\nabla l}$ vanishes at initialization, all
                diagrams with a degree-one vertex that is a singleton vanish.
                Because we work at order $\eta^3$, we consider $3$-edged
                diagrams.  Finally, because all first and second moments match
                between the two landscapes, we consider only diagrams with at
                least one partition of size at least $3$.  The only such test
                diagram is $\sdia{c(012-3)(03-13-23)}$.  This embeds in $T$
                ways (one for each grid cell) and has
                symmetry factor $1/3!$ for a total of
                $$
                    \frac{T \eta^3 }{6}
                    \expct{\nabla^3 l}
                    \expct{\nabla l_{n_{t_a}} \nabla l_{n_{t_b}} \nabla l_{n_{t_c}}}
                $$
            \end{proof}

    \subsection{Future topics}                                      \label{appendix:future}

        Our diagrams invite exploration of Lagrangian formalisms and curved
        backgrounds:\footnote{
            \cite{la60, la51} review these concepts.
        }
        \begin{quest}
            Does some least-action principle govern SGD; if not, what is an
            essential obstacle to this characterization?
        \end{quest}
        Lagrange's least-action formalism intimately intertwines with the
        diagrams of physics.  Together, they afford a modular framework for
        introducing new interactions as new terms or diagram nodes.  In fact,
        we find that some \emph{higher-order} methods --- such as the
        Hessian-based update
        $
            \theta \leftsquigarrow
            \theta -
            (\eta^{-1} + \lambda \nabla \nabla l_t(\theta))^{-1}
            \nabla l_t(\theta)
        $
        parameterized by small $\eta, \lambda$ --- admit diagrammatic analysis
        when we represent the $\lambda$ term as a second type of diagram node.
        Though diagrams suffice for computation, it is Lagrangians that most
        deeply illuminate scaling and conservation laws.

        Our work assumes a flat metric $\eta^{\mu\nu}$, but it might
        generalize to weight spaces curved in the sense of Riemann.\footnote{
            One may represent the affine connection as a node, thus giving
            rise to non-tensorial and hence gauge-dependent diagrams.
        }  Such curvature finds concrete application in the \emph{learning on
        manifolds} paradigm of \cite{ab07, zh16}, notably specialized to
        \cite{am98}'s \emph{natural gradient descent} and \cite{ni17}'s
        \emph{hyperbolic embeddings}.  While that work focuses on
        \emph{optimization} on curved weight spaces, in machine learning we
        also wish to analyze \emph{generalization}.
        %
        Starting with the intuition that ``smaller'' hypothesis classes
        generalize better and that curvature controls the volume of small
        neighborhoods, we conjecture that sectional curvature regularizes
        learning:
        \begin{conj}[Sectional curvature regularizes]
            If $\eta(\tau)$ is a Riemann metric on weight space, smoothly
            parameterized by $\tau$, and if the sectional curvature through
            every $2$-form at $\theta_0$ increases as $\tau$ grows, then
            the gen.\ gap attained by fixed-$T$ SGD with learning rate $c
            \eta(\tau)$ (when initialized from $\theta_0$) decreases as $\tau$
            grows, for all sufficiently small $c>0$.
        \end{conj}
        We are optimistic our formalism may resolve conjectures such as above.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Experiments  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\newpage
\section{Experimental methods}\label{appendix:experiments}

    \subsection{What artificial landscapes did we use?}             \label{appendix:artificial}

        We define three artificial landscapes, called
        \Gauss, \Helix, and \MeanEstimation.

        \subsubsection{\Gauss}
            Consider fitting a centered normal $\Nn(0, \sigma^2)$ to some
            centered standard normal data.  We parameterize the landscape by
            $h=\log(\sigma^2)$ so that the Fisher information matches the
            standard dot product \citep{am98}.   
            %
            More explicitly, the \Gauss\, landscape is a probability
            distribution $\Dd$ over functions $l_x:\RR^1\to \RR$ on
            $1$-dimensional weight space, indexed by standard-normally
            distributed $1$-dimensional datapoints $x$ and defined by the
            expression:
            $$
                l_x(h)
                \triangleq
                \frac{1}{2} \wrap{h + x^2 \exp(-h)}
            $$
            The gradient at sample $x$ and weight $\sigma$ is then $g_x(h) =
            (1-x^2\exp(-h))/2$.  Since $x\sim \Nn(0, 1)$, the gradient $g_x(h)$
            will be affinely related to a chi-squared, and in particular
            non-Gaussian.
            
            To measure overfitting, we initialize at the true test minimum
            $h=0$, then train and see how much the testing loss increases.  At
            $h=0$, the expected gradient vanishes, and the testing loss of SGD
            involves only diagrams that have no leaves of size one.
            
        \subsubsection{\Helix}
            The \Helix\ landscape has chirality, much like Archimedes'
            screw.
            %\cite{vi00}. 
            Specifically, the \Helix\ landscape has
            %
            weights     $\theta = (u,v,z) \in \RR^3$,
            %
            data points $x \sim \Nn(0, 1)$,
            %
            and loss:
            %
            $$
                l_x(\theta)
                \triangleq
                \frac{1}{2} H(\theta) + x \cdot S(\theta)
            $$
            %
            Here,
            $$
                H(\theta) = u^2 + v^2 + (\cos(z) u + \sin(z) v)^2
            $$
            is quadratic in $u, v$, and
            $$
                S(\theta) = \cos(z-\pi/4) u + \sin(z-\pi/4) v
            $$
            is linear in $u, v$.
            Also, since $x \sim \Nn(0,1)$, the $x \cdot S(\theta)$ term has
            expectation $0$.
            %
            In fact, the landscape has a three-dimensional continuous screw
            symmetry consisting of translation along $z$ and simulateous
            rotation in the $u-v$ plane.  Our experiments are initialized at
            $u=v=z=0$, which lies within a valley of global minima defined by
            $u=v=0$.  

            The paper body showed that SGD travels in \Helix' $+z$
            direction.  By topologically quotienting the weight space, say by
            identifying points related by a translation by $\Delta z = 200\pi$,
            we may turn the line-shaped valley into a circle-shaped valley.
            Then SGD eternally travels, say, counterclockwise.  Alternatively,
            one may preserve the homotopy type of the underlying weight space
            by Nash-embedding a flat solid torus
            $$
                [-10^1,+10^1]\times[-10^1,+10^1]\times[-10^3,+10^3]/((x,y,-10^3)\sim(x,y,+10^3))
            $$
            in a higher-dimensional Euclidean space and extending \Helix\ from
            that torus to the ambient space.

            Slightly modifying \Helix\ by adding a linear term $\alpha\cdot z$
            to $l$ for $\eta\alpha^2 \ll \eta^2/6$ leads SGD to perpetually ascend.
           
        \subsubsection{\MeanEstimation}
            The \MeanEstimation\, family of landscapes has $1$ dimensional
            weights $\theta$ and $1$-dimensional datapoints $x$.  It is defined
            by the expression:
            $$
                l_x(\theta)
                \triangleq
                \frac{1}{2} H \theta^2 + x S \theta
            $$
            Here, $H, S$ are positive reals parameterizing the family; they
            give the hessian and (square root of) gradient covariance,
            respectively.

            For our hyperparameter-selection experiment (Figure
            \ref{fig:takreg}\ofthree{2}) we introduce an $l_2$
            regularization term as follows:
            $$
                l_x(\theta, \lambda)
                \triangleq
                \frac{1}{2} (H + \lambda) \theta^2 + x S \theta
            $$
            Here, we constrain $\lambda\geq 0$ during optimization using
            projections; we found similar results when parameterizing $\lambda
            = \exp(h)$, which obviates the need for projection but necessitates
            a non-canonical choice of initialization.  We initialize
            $\lambda=0$.

    \subsection{What image-classification landscapes did we use?}   \label{appendix:natural}

        \subsubsection{Architectures}
            In addition to the artificial loss landscapes
            \Gauss, \Helix, and \MeanEstimation, 
            we tested our predictions on logistic linear regression
            and simple convolutional networks (2 convolutional weight layers
            each with kernel $5$, stride $2$, and $10$ channels, followed by
            two dense weight layers with hidden dimension $10$) for the
            CIFAR-10 \cite{kr09} and Fashion-MNIST datasets \cite{xi17}.  The
            convolutional architectures used $\tanh$ activations and Gaussian
            Xavier initialization.  To set a standard distance scale on weight
            space, we parameterized the model so that the
            Gaussian-Xavier initialization of the linear maps in each layer
            differentially pulls back to standard normal initializations of the
            parameters.
            
        \subsubsection{Datasets}
            For image classification landscapes, we regard the finite amount of
            available data as the true (sum of diracs) distribution $\Dd$ from
            which we sample testing and training sets in i.i.d.\ manner (and hence
            ``with replacement'').  We do this to gain practical access to a
            ground truth against which we may compare our predictions.  One
            might object that this sampling procedure would cause testing and
            training sets to overlap, hence biasing testing loss measurements.  In
            fact, testing and training sets overlap only in reference, not in
            sense: the situation is analogous to a text prediction task in
            which two training points culled from different corpora happen to
            record the same sequence of words, say, ``Thank you!''.  In any
            case, all of our experiments focus on the limited-data regime, e.g.
            $10^1$ datapoints out of $\sim 10^{4.5}$ dirac masses, so overlaps
            are rare.

    \subsection{Measurement process}                                \label{appendix:measure}

        \subsubsection{Diagram evaluation on real landscapes}
            We implemented the formulae of \S\ref{appendix:bessel} in order
            to estimate diagram values from real data measured at
            initialization from batch averages of products of derivatives.

        \subsubsection{Descent simulations}
            We recorded testing and training losses for each of the trials below.  To
            improve our estimation of average differences, when we compared two
            optimizers, we gave them the same random seed (and hence the same
            training sets).

            We ran $2 \cdot 10^5$ trials of \Gauss\, with SDE and SGD,
            initialized at the test minimum with $T=1$ and $\eta$ ranging from
            $5\cdot 10^{-2}$ to $2.5\cdot 10^{-1}$.
            We ran $5 \cdot 10^1$ trials of \Helix with SGD with $T=10^4$
            and $\eta$ ranging from $10^{-2}$ to $10^{-1}$.
            We ran $10^3$ trials of \MeanEstimation with GD and STIC
            with $T=10^2$, $H$ ranging from $10^{-4}$ to $4 \cdot 10^0$,
            a covariance of gradients of $10^2$, and the true mean $0$ or
            $10$ units away from initialization.

            We ran $5 \cdot 10^4$ trials of the CIFAR-10 convnet on each of $6$
            Glorot-Xavier initializations we fixed once and for all through
            these experiments for the optimizers SGD, GD, and GDC, with $T=10$
            and $\eta$ between $10^{-3}$ and $2.5 \cdot 10^{-2}$.  We did
            likewise for the linear logistic model on the one initialization of
            $0$.

            We ran $4 \cdot 10^4$ trials of the Fashion-MNIST convnet on each
            of $6$ Glorot-Xavier initializations we fixed once and for all
            through these experiments for the optimizers SGD, GD, and GDC with
            $T=10$ and $\eta$ between $10^{-3}$ and $2.5 \cdot 10^{-2}$.  We
            did likewise for the linear logistic model on the one
            initialization of $0$. 

    \subsection{Implementing optimizers}                            \label{appendix:optimizers}

        We approximated SDE by refining time discretization by a factor of
        $16$, scaling learning rate down by a factor of $16$, and introducing
        additional noise in the shape of the covariance in proportion as
        prescribed by the Wiener process scaling.

        Our GDC regularizer was implemented using the unbiased estimator
        $$
            \hat{C} \triangleq (l_x - l_y)_\mu {l_x}_\nu / 2
        $$
        
        For our tests of regularization based on Corollary \ref{cor:overfit},
        we exploited the low-dimensional special structure of the artificial
        landscape in order to avoid diagonalizing to perform the matrix
        exponentiation: precisely, we used that, even on training landscapes,
        the covariance of gradients would be degenerate in all but one
        direction, and so we need only exponentiate a scalar.

    \subsection{Software frameworks and hardware}                   \label{appendix:frameworks}

        All code and data-wrangling scripts can be found on
        {\color{mooteal}github.com/???????/perturb}.  This link will be made
        available after the period of double-blind review.
        %
        Our code uses PyTorch 0.4.0 \citep{pa19} on Python 3.6.7; there are no
        other substantive dependencies.  The code's randomness is parameterized
        by random seeds and hence reproducible.
        %
        We ran experiments on a Lenovo laptop and on our institution's
        clusters; we consumed about $100$ GPU-hours.

    \subsection{Unbiased estimators of landscape statistics}        \label{appendix:bessel}
        %
        We use the following method --- familiar to some of our colleagues but
        hard to find writings on --- for obtaining unbiased estimates for
        various statistics of the loss landscape.  The method is merely an
        elaboration of Bessel's factor \citep{ga23}.  For completeness, we
        explain it here. 
        
        Given samples from a joint probability space $\prod_{0\leq d<D} X_d$,
        we seek unbiased estimates of \emph{multipoint correlators} (i.e.\ products of
        expectations of products) such as $\wang{x_0 x_1 x_2}\wang{x_3}$.  Here,
        angle brackets denote expectations over the population. 
        For
        example, say $D=2$ and from $2S$ samples we'd like to estimate
        $\wang{x_0 x_1}$.  Most simply, we could use $\Avg_{0\leq s<2S}
        x_0^{(s)} x_1^{(s)}$, where $\Avg$ denotes averaging over the sample.  In fact, the
        following also works:
        %
        \begin{equation} \label{eq:bessel}
            S
            \wrap{\Avg_{0\leq s< S} x_0^{(s)}}
            \wrap{\Avg_{0\leq s< S} x_1^{(s)}}
            +
            (1-S)
            \wrap{\Avg_{0\leq s< S} x_0^{(s)}}
            \wrap{\Avg_{S\leq s<2S} x_1^{(s)}}
        \end{equation}
        %
        When multiplication is expensive (e.g. when each $x_d^{(s)}$ is a
        tensor and multiplication is tensor contraction), we prefer the latter,
        since it uses $O(1)$ rather than $O(S)$ multiplications.  This in turn
        allows more efficient use of batch computations on GPUs.  We now
        generalize this estimator to higher-point correlators (and $D\cdot S$
        samples).

        For uniform notation, we assume without loss that each of the $D$
        factors appears exactly once in the multipoint expression of interest;
        such expressions then correspond to partitions on $D$ elements, which
        we represent as maps $\mu:\wasq{D}\to \wasq{D}$ with $\mu(d)\leq d$ and
        $\mu\circ \mu=\mu$.  Note that $\wabs{\mu} \coloneqq \wabs{im(\mu)}$
        counts $\mu$'s parts.  We then define the statistic
        %
        $$
            \wurl{x}_\mu
            \triangleq
            \prod_{0\leq d<D} \Avg_{0\leq s<S} x_d^{(\mu(d)\cdot S + s)}
        $$
        %
        and the correlator $\wang{x}_\mu$ we define to be the expectation of 
        $\wurl{x}_\mu$ when $S=1$.  In this notation, \ref{eq:bessel} says: 
        $$
            \wang{x}_{\partitionbox{0}\partitionbox{1}}
            =
            \expct{
                S       \cdot \wurl{x}_{\partitionbox{0 1}} +
                (1-S)   \cdot \wurl{x}_{\partitionbox{0}\partitionbox{1}}
            }
        $$
        %
        Here, the boxes indicate partitions of $\wasq{D}=\wasq{2}=\{0,1\}$.
        Now, for general $\mu$, we have:
        %
        \begin{equation} \label{eq:newbessel}
            \expct{S^D \wurl{x}_\mu}
            =
            \sum_{\tau\leq \mu} \wrap{
                \prod_{0\leq d<D}
                    \frac{S!}{\wrap{S-\wabs{\tau(\mu^{-1}(d))}}!}
            }
            \wang{x}_\tau
        \end{equation}
        %
        where `$\tau \leq \mu$' ranges through partitions \emph{finer} than 
        $\mu$, i.e. maps $\tau$ through which $\mu$ factors.   
        In smaller steps, \ref{eq:newbessel} holds because
        %
        \begin{align*}
            \expct{S^D \wurl{x}_\mu}
            &=
            \expct{
                \sum_{(0\leq s_d<S) \in \wasq{S}^D}
                \prod_{0\leq d<D}
                x_d^{\wrap{\mu(d)\cdot S + s_d}}
            }\\
            &=
            \sum_{\substack{(0\leq s_d<S) \\ \in \wasq{S}^D}}
            \expct{
                \prod_{0\leq d<D}
                x_d^{\wrap{\min \wurl{
                    \tilde{d}~:~\mu(\tilde{d})\cdot S+s_{\tilde{d}} = \mu(d)\cdot S+s_d
                }}}
            }\\
            &=
            \sum_{\tau} \wabs{\wurl{\substack{
                (0\leq s_d<S)~\in~[S]^D~: \\
                \wrap{\substack{
                    \mu(d)=\mu(\tilde{d}) \\
                    \wedge~s_d=s_{\tilde{d}}
                }}
                \Leftrightarrow
                \tau(d)=\tau(\tilde{d})
            }}}
            \wang{x}_\tau \\
            &=
            \sum_{\tau\leq \mu} \wrap{
                \prod_{0\leq d<D}
                    \frac{S!}{\wrap{S-\wabs{\tau(\mu^{-1}(d))}}!}
            }
            \wang{x}_\tau
        \end{align*}

        Solving \ref{eq:newbessel} for $\wang{x}_\mu$, we find:
        %
        \begin{equation*}
            \text{\fbox{$
            \wang{x}_\mu
            =
            \frac{S^D}{S^{\wabs{\mu}}}
            \expct{
                \wurl{x}_\mu
            }
            -
            \sum_{\tau < \mu} \wrap{
                \prod_{d\in im(\mu)}
                \frac{\wrap{S-1}!}{\wrap{S-\wabs{\tau(\mu^{-1}(d))}}!}
            }
            \wang{x}_\tau
            $}}
        \end{equation*}
        %
        This expresses $\wang{x}_\mu$ in terms of the batch-friendly estimator
        $\wurl{x}_\mu$ as well as correlators $\wang{x}_\tau$ for $\tau$
        \emph{strictly} finer than $\mu$.  We may thus (use dynamic programming
        to) obtain unbiased estimators $\wang{x}_\mu$ for all partitions $\mu$.
        Symmetries of the joint distribution and of the multilinear
        multiplication may further streamline estimation by turning a sum over
        $\tau$ into a multiplication by a combinatorial factor.  For example,
        in the case of complete symmetry:
        %
        $$
            \wang{x}_{\partitionbox{012}}
            =
            S^2
            \wurl{x}_{\partitionbox{012}}
            -
            \frac{(S-1)!}{(S-3)!}
            \wurl{x}_{\partitionbox{0}\partitionbox{1}\partitionbox{2}}
            -
            3\frac{(S-1)!}{(S-2)!}
            \wurl{x}_{\partitionbox{0}\partitionbox{12}}
        $$

    \subsection{Additional figures}                                 \label{appendix:figures}

        \begin{figure}[h] 
            \centering
            \centering
            \pmoo{3.5cm}{multi-fashion-logistic-0}
            \pmoo{3.5cm}{vs-sde}
            \pmoo{3.5cm}{tak-reg}
            \caption{
                \textbf{Further experimental results}.
                %
                \textbf{Left}: SGD with $2, 3, 5, 8$ epochs incurs greater test
                loss than one-epoch SGD (difference shown in I bars) by the
                predicted amounts (predictions shaded) for a range of learning
                rates.  Here, all SGD runs have $N=10$; we scale the learning
                rate for $E$-epoch SGD by $1/E$ to isolate the effect of
                inter-epoch correlations away from the effect of larger $\eta
                T$.
                %
                \textbf{Center}: SGD's difference from SDE after $\eta T
                \approx 10^{-1}$ with maximal coarseness on \Gauss.  Two
                effects not modeled by SDE --- time-discretization and
                non-Gaussian noise oppose on this landscape but do not
                completely cancel.  Our theory approximates the above curve
                with a correct sign and order of magnitude; we expect that the
                fourth order corrections would improve it further.
                %
                \textbf{Right}: Blue intervals regularization using Corollary
                \ref{cor:overfit}.  When the blue intervals fall below the
                black bar, this proposed method outperforms plain GD.  For
                \MeanEstimation\ with fixed $C$ and a range of $H$s, initialized
                a fixed distance \emph{away} from the true minimum, descent on
                an $l_2$ penalty coefficient $\lambda$ improves on plain GD for
                most Hessians.  The new method does not always outperform GD,
                because $\lambda$ is not perfectly tuned according to STIC but
                instead descended on for finite $\eta T$.
            }
            \label{fig:takreg}
        \end{figure}



\end{document}
