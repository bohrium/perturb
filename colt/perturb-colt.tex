%\documentclass[anon,12pt]{colt2021} % Anonymized submission
\documentclass[final,12pt]{colt2021} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

\usepackage{amsfonts, makerobust}
\usepackage{mathtools, nicefrac, xstring, enumitem}

%   The following reconciles COLT's style with \includegraphics:
%       (see tex.stackexchange.com/questions/520891)
\makeatletter
\let\Ginclude@graphics\@org@Ginclude@graphics
\makeatother

%---------------------  colors  -----------------------------------------------

\usepackage{xcolor, framed}
\definecolor{moolime}{rgb}{0.90,1.00,0.90}
\definecolor{moosky}{rgb}{0.90,0.90,1.00}
\definecolor{moopink}{rgb}{1.00,0.90,0.90}
\definecolor{moor}{rgb}{0.8,0.2,0.2}
\definecolor{moog}{rgb}{0.2,0.8,0.2}
\definecolor{moob}{rgb}{0.2,0.2,0.8}
\definecolor{mooteal}{rgb}{0.1,0.6,0.4}

%---------------------  intertext: footnotes and hyperlinks  ------------------ 

\usepackage[perpage]{footmisc}
\renewcommand*{\thefootnote}{
    \color{red}
    \arabic{footnote}
    %\fnsymbol{footnote}
} 

\usepackage{hyperref}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Custom Math Commands  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  expanding containers  ---------------------------------

\newcommand{\wrap}[1]{\left(#1\right)}
\newcommand{\wasq}[1]{\left[#1\right]}
\newcommand{\wang}[1]{\left\langle#1\right\rangle}
\newcommand{\wive}[1]{\left\llbracket#1\right\rrbracket}
\newcommand{\worm}[1]{\left\|#1\right\|}
\newcommand{\wabs}[1]{\left|#1\right|}
\newcommand{\wurl}[1]{\left\{#1\right\}}

\newcommand{\partitionbox}[1]{
    \text{
        \fboxsep=0.5pt
        \tiny
        \fbox{#1}
    }
}

%---------------------  special named objects  --------------------------------

\newcommand{\Free}{\mathcal{F}}
\newcommand{\Forg}{\mathcal{G}}
\newcommand{\Mod}{\mathcal{M}}
\newcommand{\Hom}{\text{\textnormal{Hom}}}
\newcommand{\Aut}{\text{\textnormal{Aut}}}
\newcommand{\image}{\text{\textnormal{im}}}
\newcommand{\uvalue}{\text{\textnormal{uvalue}}}
\newcommand{\rvalue}{\text{\textnormal{rvalue}}}
\newcommand{\edges}{\text{\textnormal{edges}}}
\newcommand{\ords}{\text{\textnormal{ords}}}
\newcommand{\parts}{\text{\textnormal{parts}}}
\newcommand{\SGD}{\text{\textnormal{SGD}}}
\DeclareMathOperator*{\Avg}{\text{\sffamily A}}
\newcommand{\expc}{\mathbb{E}}
\newcommand{\expct}[1]{\mathbb{E}\left[#1\right]}

%---------------------  fancy letters  ----------------------------------------

\newcommand{\Aa}{\mathcal{A}}
\newcommand{\Bb}{\mathcal{B}}
\newcommand{\Cc}{\mathcal{C}}   \newcommand{\CC}{\mathbb{C}}
\newcommand{\Dd}{\mathcal{D}}
\newcommand{\Ee}{\mathcal{E}}
\newcommand{\Ff}{\mathcal{F}}
\newcommand{\Gg}{\mathcal{G}}
\newcommand{\Hh}{\mathcal{H}}
\newcommand{\Ll}{\mathcal{L}}
\newcommand{\Mm}{\mathcal{M}}
\newcommand{\Nn}{\mathcal{N}}   \newcommand{\NN}{\mathbb{N}}
\newcommand{\Oo}{\mathcal{O}}
\newcommand{\Pp}{\mathcal{P}}
\newcommand{\Qq}{\mathcal{Q}}   \newcommand{\QQ}{\mathbb{Q}}
\newcommand{\Rr}{\mathcal{R}}   \newcommand{\RR}{\mathbb{R}}
\newcommand{\Ss}{\mathcal{S}}
\newcommand{\Tt}{\mathcal{T}}
\newcommand{\Uu}{\mathcal{U}}
\newcommand{\Vv}{\mathcal{V}}
\newcommand{\Ww}{\mathcal{W}}
\newcommand{\Xx}{\mathcal{X}}
\newcommand{\Yy}{\mathcal{Y}}
\newcommand{\Zz}{\mathcal{Z}}   \newcommand{\ZZ}{\mathbb{Z}}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Pictures  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  pictures with specified width or height  --------------

\newcommand{\plotmoow}[3]{\includegraphics[width=#2          ]{../#1}}
\newcommand{\plotmooh}[3]{\includegraphics[         height=#3]{../#1}}
\newcommand{\pmoo}[2]{\includegraphics[height=#1]{../plots/#2}}
\newcommand{\dmoo}[2]{\includegraphics[height=#1]{../diagrams/#2}}

%---------------------  inline diagrams of various sizes  ---------------------

\newcommand{\sizeddia}[2]{%
    \begin{gathered}%
        \includegraphics[scale=#2]{../diagrams/#1.png}%
    \end{gathered}%
}
\newcommand{\bdia}[1]{\protect \sizeddia{#1}{0.25}}
\newcommand{\dia} [1]{\protect \sizeddia{#1}{0.21}}
\newcommand{\mdia}[1]{\protect \sizeddia{#1}{0.17}}
\newcommand{\sdia}[1]{\protect \sizeddia{#1}{0.13}}

\newcommand{\mend}{\hfill $\Diamond$}

\newcommand{\Gauss}{\textsc{Gauss}}
\newcommand{\Archimedes}{\textsc{Archimedes}}
\newcommand{\MeanEstimation}{\textsc{Mean Estimation}}




\newcommand{\sS}{\hspace{0.43em}}
\newcommand{\sammail}{%
    C{\sS}%
    O{\sS}%
    L{\sS}%
    I{\tiny{@}}%
    M{\sS}%
    I{\sS}%
    T{\sS}%
    .{\sS}%
    e{\sS}%
    d{\sS}%
    u%
}

\title[SGD at Small Learning Rates]{SGD at Small Learning Rates}
\usepackage{times}
\coltauthor{%
    \Name{Samuel C.\ Tenka} \Email{\sammail}    \\
    \addr MIT, CSAIL
}

\begin{document}

    \maketitle
    
    \begin{abstract}%
        We quantify how gradient noise shapes the dynamics of stochastic
        gradient descent (SGD) by taking Taylor series in the learning rate.
        %
        We present in particular a new diagram-based notation that permits
        resummation to convergent results.
        %
        We employ our theory to contrast SGD against two popular
        approximations: deterministic descent and stochastic differential
        equations.  We find that SGD's trajectory avoids both gradient noise
        and minima that are sharp with respect to gradient noise.  Paired with
        our results that connect overfitting to the width of minima, these
        repulsions suggest a mechanism for the unexpected generalization of
        overparameterized learning scenarios.
    \end{abstract}
    
    \begin{keywords}%
        SGD, learning rates, generalization, gradient noise. 
    \end{keywords}
    \section{Introduction}
        \subsection{Questions about SGD}
    
        \subsection{Background, notation, assumptions}
   
        \subsection{Related work}
    
            %-- history of sgd  -----------------------------------------------

            It was \cite{ki52} who, in uniting gradient descent \citep{ca47}
            with stochastic approximation \citep{ro51}, invented SGD.  Since
            the development of back-propagation for efficient differentiation
            \citep{we74}, SGD has been used to train connectionist models,
            e.g.\ neural networks \citep{bo91}, recently to remarkable success
            \citep{le15}.
        
            %-- analyzing overfitting; relevance of optimization; sde errs  ---
        
            Several lines of work treat the overfitting of SGD-trained networks
            \citep{ne17a}.  For example, \cite{ba17} controls the Rademacher
            complexity of deep hypothesis classes, leading to
            optimizer-agnostic generalization bounds.  Yet SGD-trained networks
            generalize despite their ability to shatter large sets
            \citep{zh17}, so generalization must arise from not only
            architecture but also optimization \citep{ne17b}.  Others
            approximate SGD by SDE to analyze implicit regularization (e.g.\
            \cite{ch18}), but, per \cite{ya19a}, such continuous-time analyses
            cannot treat SGD noise correctly.
            %
            %-- we extend dan's approach  -------------------------------------
            %
            We avoid these pitfalls by Taylor expanding around $\eta=0$ as in
            \cite{ro18}.  Unlike that work, we generalize beyond order $\eta^1$
            and $T=2$,.  To support this generalization, we develop new 
            summation techniques with improved large-$T$ convergence.  Our
            interpretion of the resulting terms offers a new qualitative
            picture of SGD as a superposition of several simpler
            information-flow processes.
            
            %-- phenomenology of rademacher correlates such as hessians  ------
        
            Our predictions are vacuous for large $\eta$.  Other analyses treat
            large-$\eta$ learning phenomenologically, whether by finding
            empirical correlates of gen.\ gap \citep{li18}, by showing that
            \emph{flat} minima generalize (\cite{ho17}, \cite{ke17},
            \cite{wa18}), or by showing that \emph{sharp} minima generalize
            (\cite{st56}, \cite{di17}, \cite{wu18}).  Our theory reveals that 
            SGD's implicit regularization mediates between these seemingly
            clashing intuitions.
            
            %-- our work vs other perturbative approaches  --------------------
        
            Prior work analyzes SGD perturbatively: \cite{dy19} perturb in
            inverse network width, using 't Hooft diagrams to correct the
            Gaussian Process approximation for specific deep nets.  Perturbing
            to order $\eta^2$, \cite{ch18} and \cite{li17} are forced to assume
            uncorrelated Gaussian noise.  By contrast, we use Penrose diagrams
            to compute test losses to arbitrary order in $\eta$.  We allow
            correlated, non-Gaussian noise and thus \emph{any} smooth
            architecture.  For instance, we do not
    

    \section{Perturbative theory of SGD}
        \subsection{Trivial example}
        \subsection{Perturbation as technique}
        \subsection{Insights from diagrams}
        \subsection{Resummation}
    \section{Consequences of the theory}
        \subsection{SGD descends on a $C$-smoothed landscape and prefers minima
        flat w.r.t.\ $C$.} \subsection{Both flat and sharp minima overfit less}
        \subsection{High-$C$ regions repel small-$(E,B)$ SGD more than
        large-$(E,B)$ SGD} \subsection{Non-Gaussian noise affects SGD but not
        SDE}
    \section{Experiments}
        \subsection{A}
        \subsection{B}
        \subsection{C}
        \subsection{Overfitting and the width of minima}
    \section{Conclusion}
    
        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        %~~~~~~~~~  Summarize Contributions  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    
        We presented a diagram-based method for studying stochastic optimization on
        short timescales or near minima.
            Corollaries \ref{cor:entropic} and \ref{cor:overfit} together offer
            insight into SGD's success in training deep networks: SGD avoids
            curvature and noise, and curvature and noise control generalization.
    
        Analyzing $\sdia{c(01-2)(02-12)}$, we proved that \textbf{flat and sharp
        minima both overfit less} than medium minima.  Intuitively, flat minima are
        robust to vector noise, sharp minima are robust to covector noise, and
        medium minima robust to neither.  We thus proposed a regularizer enabling
        gradient-based hyperparameter tuning.
        %
        Inspecting $\sdia{c(01-2-3)(02-12-23)}$, we extended \cite{we19b} to
        nonconstant, nonisotropic covariance to reveal that \textbf{SGD descends on
        a landscape smoothed by the current covariance $C$}.
        As $C$ evolves, the
        smoothed landscape evolves, resulting in non-conservative dynamics.
        %
        Examining $\sdia{c(01-2)(01-12)}$, we showed that \textbf{GD may emulate
        SGD}, as conjectured by \cite{ro18}.  This is significant because, while
        small batch sizes can lead to better generalization \citep{bo91}, modern
        infrastructure increasingly rewards large batch sizes \citep{go18}.  
    
        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        %~~~~~~~~~  Anticipate Criticism of Limitations  ~~~~~~~~~~~~~~~~~~~~~~~~~~
    
            Since our predictions depend only on loss data near initialization,
            they break down after the weight moves far from initialization.  Our
            theory thus best applies to small-movement contexts, whether for long
            times (large $\eta T$) near an isolated minimum or for short times
            (small $\eta T$) in general.  Thus, the theory might help to analyze
            meta-learners based on fine-tuning (e.g.\ \cite{fi17}'s MAML).
    
            Much as meteorologists understand how warm and cold fronts interact
            despite long-term forecasting's intractability, we quantify how
            curvature and noise contribute to counter-intuitive dynamics governing
            each short-term interval of SGD's trajectory.  Equipped with our
            theory, practitioners may now refine intuitions --- e.g.\ that SGD
            descends on the training loss --- to account for noise.
    
        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        %~~~~~~~~~  Future Work  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    
        \subsection{Future work}
    
    \acks{
        We thank Sho Yaida, Joshua B.\ Tenenbaum, Dan A.\ Roberts, and Greg
        Wornell for generous mentorship.  It was Sho who suggested the question
        of how to resum; it was Dan who introduced us to the SGD literature and
        whose 2018 paper and private notes prove special cases of Corollary
        FILL IN \cite{ro18}.  We appreciate writerly feedback from Ben R.\
        Bray, Wenli Zhao, and Chloe Kleinfeldt.
        %
        We are grateful to our anonymous reviewers' for dramatically improving
        our writing's clarity.
    }
    
    \bibliography{perturb}
    
    \appendix
    
        \section{My Proof of Theorem 1}
        
            This is a boring technical proof.
        
        \section{My Proof of Theorem 2}
        
            This is a complete version of a proof sketched in the main text.
    
\end{document}
