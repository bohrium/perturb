\documentclass[anon,12pt]{colt2021} % Anonymized submission
%\documentclass[final,12pt]{colt2021} % Include author names

\usepackage{tikz}
% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

\usepackage{amsfonts, makerobust, float}
\usepackage{mathtools, nicefrac, xstring, enumitem, pdflscape, multicol}

\newcommand{\subthreesect}[1]{\par\noindent\textsc{#1} --- }

%   The following reconciles COLT's style with \includegraphics:
%       (see tex.stackexchange.com/questions/520891)
\makeatletter
\let\Ginclude@graphics\@org@Ginclude@graphics
\makeatother

\usepackage[export]{adjustbox}

%---------------------  graphics and figures  ---------------------------------
 \usepackage{wrapfig, caption}
\usepackage{hanging, txfonts, ifthen}

\newcommand{\ofsix}[1]{
    {\tiny \raisebox{0.04cm}{$\substack{
        \ifthenelse{\equal{#1}{0}}{{\color{moor}\blacksquare}}{\square}
        \ifthenelse{\equal{#1}{2}}{{\color{moor}\blacksquare}}{\square}    
        \ifthenelse{\equal{#1}{4}}{{\color{moor}\blacksquare}}{\square} \\
        \ifthenelse{\equal{#1}{1}}{{\color{moor}\blacksquare}}{\square}    
        \ifthenelse{\equal{#1}{3}}{{\color{moor}\blacksquare}}{\square}
        \ifthenelse{\equal{#1}{5}}{{\color{moor}\blacksquare}}{\square}
    }$}}%
}

\newcommand{\offive}[1]{
    {\tiny
        \raisebox{-0.04cm}{\color{gray}\scalebox{2.5}{$\substack{
            \ifthenelse{\equal{#1}{0}}{{\color{moor}\blacksquare}}{\square} 
        }$}}%
        \raisebox{0.04cm}{$\substack{
            \IfSubStr{#1}{1}{{\color{moor}\blacksquare}}{\square}   
            \IfSubStr{#1}{1}{{\color{moor}\blacksquare}}{\square} \\
            \IfSubStr{#1}{2}{{\color{moor}\blacksquare}}{\square}    
            \IfSubStr{#1}{2}{{\color{moor}\blacksquare}}{\square}    
        }$}%
    }%
}

\newcommand{\ofthree}[1]{
    {\tiny \raisebox{0.04cm}{$
        \ifthenelse{\equal{#1}{0}}{{\color{moor}\blacksquare}}{\square}
        \ifthenelse{\equal{#1}{1}}{{\color{moor}\blacksquare}}{\square}    
        \ifthenelse{\equal{#1}{2}}{{\color{moor}\blacksquare}}{\square}
    $}}%
}

\newcommand{\offour}[1]{
    {\tiny \raisebox{0.04cm}{$
        %\ifthenelse{\equal{#1}{0}}{{\color{moor}\blacksquare}}{\square}
        \ifthenelse{\equal{#1}{1}}{{\color{moor}\blacksquare}}{\square}    
        \ifthenelse{\equal{#1}{2}}{{\color{moor}\blacksquare}}{\square}
        \ifthenelse{\equal{#1}{3}}{{\color{moor}\blacksquare}}{\square}
    $}}%
}




%---------------------  colors  -----------------------------------------------

\usepackage{xcolor, framed}
\definecolor{moolime}{rgb}{0.90,1.00,0.90}
\definecolor{moosky}{rgb}{0.90,0.90,1.00}
\definecolor{moopink}{rgb}{1.00,0.90,0.90}

\definecolor{moogold}{rgb}{1.00,1.00,0.80}

\definecolor{moor}{rgb}{0.8,0.2,0.2}
\definecolor{moog}{rgb}{0.2,0.8,0.2}
\definecolor{moob}{rgb}{0.2,0.2,0.8}
\definecolor{mooteal}{rgb}{0.1,0.6,0.4}

\definecolor{spacetimered}{rgb}{0.8, 0.2, 0.0}
\definecolor{spacetimeorange}{rgb}{0.6, 0.4, 0.1}
\definecolor{spacetimelemon}{rgb}{0.4, 0.6, 0.1}
\definecolor{spacetimegreen}{rgb}{0.0, 0.8, 0.2}
\definecolor{spacetimeteal}{rgb}{0.1, 0.6, 0.4}
\definecolor{spacetimesky}{rgb}{0.1, 0.4, 0.6}
\definecolor{spacetimeblue}{rgb}{0.2, 0.0, 0.8}
\definecolor{spacetimeindigo}{rgb}{0.4, 0.1, 0.6}
\definecolor{spacetimepurple}{rgb}{0.6, 0.1, 0.4}

%\newcommand{\translucent}[2]{{\tikz\node[opacity=0.75,fill=#1]{#2};}}
\newcommand{\translucent}[2]{\colorbox{#1}{#2}}

%---------------------  intertext: footnotes and hyperlinks  ------------------ 

\usepackage[perpage]{footmisc}
\renewcommand*{\thefootnote}{%
    \color{red}%
    \arabic{footnote}%
    %\fnsymbol{footnote}%
} 

\usepackage{hyperref}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Theorem Environments  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  mathematical results  ---------------------------------

%\theoremstyle{plain}
    \newtheorem*{klem*}{Key Lemma}
    \newtheorem*{lem}{Lemma}
    \newtheorem{thm}{Theorem}
    \newtheorem*{thm*}{Theorem}
    \newtheorem{cor}{Corollary}
    \newtheorem{prop}{Prop}
    \newtheorem*{prop*}{Prop}
    \setcounter{prop}{-1}

%---------------------  mathematical questions  -------------------------------

    \newtheorem{conj}{Conjecture}
    \newtheorem{quest}{Question}
    \newtheorem*{quest*}{Question}
    \newtheorem*{quests*}{Questions}

%---------------------  definitions, answers, remarks  ------------------------

%\theoremstyle{definition}
    \newtheorem{dfn}{Definition}
    \newtheorem*{answ*}{Answer}
    \newtheorem{rmk}{Remark}
    \newtheorem*{midea*}{Main Idea}
    \newtheorem*{rmk*}{Remark}
    \newtheorem{exm}{Example}


%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Custom Math Commands  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\newcommand{\pr}{\prime}
\newcommand{\squish}{\vspace{-0.05cm}}
\newcommand{\squash}{\vspace{-0.15cm}}
\newcommand{\crunch}{\vspace{-0.45cm}}

%---------------------  expanding containers  ---------------------------------


\newcommand{\wrap}[1]{\left(#1\right)}
\newcommand{\wasq}[1]{\left[#1\right]}
\newcommand{\wang}[1]{\left\langle#1\right\rangle}
\newcommand{\wive}[1]{\left\llbracket#1\right\rrbracket}
\newcommand{\worm}[1]{\left\|#1\right\|}
\newcommand{\wabs}[1]{\left|#1\right|}
\newcommand{\wurl}[1]{\left\{#1\right\}}

\newcommand{\partitionbox}[1]{
    \text{
        \fboxsep=0.5pt
        \tiny
        \fbox{#1}
    }
}

%---------------------  special named objects  --------------------------------


        \newcommand{\del} { \partial}
        \newcommand{\nb} { \nabla }
        \newcommand{\lx} { l_x(\theta) }
        \newcommand{\teq} { \triangleq }
        \newcommand{\ex}[1] { \expc_x \wasq{#1} }


\newcommand{\Free}{\mathcal{F}}
\newcommand{\Forg}{\mathcal{G}}
\newcommand{\Mod}{\mathcal{M}}
\newcommand{\Hom}{\text{\textnormal{Hom}}}
\newcommand{\Aut}{\text{\textnormal{Aut}}}
\newcommand{\image}{\text{\textnormal{im}}}
\newcommand{\uvalue}{\text{\textnormal{uvalue}}}
\newcommand{\rvalue}{\text{\textnormal{rvalue}}}
\newcommand{\edges}{\text{\textnormal{edges}}}
\newcommand{\ords}{\text{\textnormal{ords}}}
\newcommand{\parts}{\text{\textnormal{parts}}}
\newcommand{\SGD}{\text{\textnormal{SGD}}}
\DeclareMathOperator*{\Avg}{\text{\sffamily A}}
\newcommand{\expc}{\mathbb{E}}
\newcommand{\expct}[1]{\mathbb{E}\left[#1\right]}

%---------------------  fancy letters  ----------------------------------------

\newcommand{\Aa}{\mathcal{A}}
\newcommand{\Bb}{\mathcal{B}}
\newcommand{\Cc}{\mathcal{C}}   \newcommand{\CC}{\mathbb{C}}
\newcommand{\Dd}{\mathcal{D}}
\newcommand{\Ee}{\mathcal{E}}
\newcommand{\Ff}{\mathcal{F}}
\newcommand{\Gg}{\mathcal{G}}
\newcommand{\Hh}{\mathcal{H}}
\newcommand{\Ll}{\mathcal{L}}
\newcommand{\Mm}{\mathcal{M}}
\newcommand{\Nn}{\mathcal{N}}   \newcommand{\NN}{\mathbb{N}}
\newcommand{\Oo}{\mathcal{O}}
\newcommand{\Pp}{\mathcal{P}}   \newcommand{\PP}{\mathbb{P}}
\newcommand{\Qq}{\mathcal{Q}}   \newcommand{\QQ}{\mathbb{Q}}
\newcommand{\Rr}{\mathcal{R}}   \newcommand{\RR}{\mathbb{R}}
\newcommand{\Ss}{\mathcal{S}}
\newcommand{\Tt}{\mathcal{T}}
\newcommand{\Uu}{\mathcal{U}}
\newcommand{\Vv}{\mathcal{V}}
\newcommand{\Ww}{\mathcal{W}}
\newcommand{\Xx}{\mathcal{X}}
\newcommand{\Yy}{\mathcal{Y}}
\newcommand{\Zz}{\mathcal{Z}}   \newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\omicron}{{\acute{o}}}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Pictures  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  pictures with specified width or height  --------------

\newcommand{\plotmoow}[3]{\includegraphics[width=#2          ]{../#1}}
\newcommand{\plotmooh}[3]{\includegraphics[         height=#3]{../#1}}
\newcommand{\pmoo}[2]{\includegraphics[height=#1]{../plots/#2}}
\newcommand{\dmoo}[2]{\includegraphics[height=#1]{../diagrams/#2}}

%---------------------  inline diagrams of various sizes  ---------------------

\newcommand{\sizeddia}[2]{%
    \begin{gathered}%
        \includegraphics[scale=#2]{../diagrams/#1.png}%
    \end{gathered}%
}
\newcommand{\bdia}[1]{\protect \sizeddia{#1}{0.22}}
\newcommand{\dia} [1]{\protect \sizeddia{#1}{0.18}}
\newcommand{\mdia}[1]{\protect \sizeddia{#1}{0.14}}
\newcommand{\sdia}[1]{\protect \sizeddia{#1}{0.10}}

\newcommand{\mend}{\hfill $\Diamond$}

\newcommand{\Gauss}{\textsc{Gauss}}
\newcommand{\Helix}{\textsc{Helix}}
\newcommand{\MeanEstimation}{\textsc{Mean Estimation}}



\newcommand{\sS}{\hspace{0.43em}}
\newcommand{\sammail}{%
    C{\sS}%
    O{\sS}%
    L{\sS}%
    I{\tiny{@}}%
    M{\sS}%
    I{\sS}%
    T{\sS}%
    .{\sS}%
    e{\sS}%
    d{\sS}%
    u%
}

\title[A Perturbative Analysis of SGD at Small Learning Rates]{
    Perturbation Theory of Stochastic Gradient Descent 
}
\usepackage{times}
\coltauthor{%
    \Name{Samuel C.\ Tenka} \Email{\sammail}    \\
    \addr MIT, CSAIL
}

\begin{document}

    \maketitle
    
    \begin{abstract}%
        We quantify how gradient noise shapes the dynamics of stochastic
        gradient descent (SGD) by taking Taylor series in the learning rate.
        %
        We present in particular a new diagram-based notation that permits
        resummation to convergent results.
        %
        We employ our theory to contrast SGD against two popular
        approximations: deterministic descent and stochastic differential
        equations.  We find that SGD's trajectory avoids regions of weight
        space with high gradient noise and avoids minima that are sharp with
        respect to gradient noise.
    \end{abstract}
    
    \begin{keywords}%
        SGD, learning rates, generalization, gradient noise, perturbation. 
    \end{keywords}

    \section{Introduction}\label{sect:intro}

        %\subsection{SGD}

            %--  object of study  ---------------------------------------------
            %
            Gradient estimates, measured on minibatches and thus noisy, form
            the primary learning signal when training deep neural nets.  While
            users of deep learning benefit from the intuition that such
            \emph{stochastic gradient descent} (SGD) approximates deterministic
            gradient descent (GD) \citep{bo91,le15}, SGD's gradient noise in
            practice alters training dynamics and testing losses
            \citep{go18, %zh19,
            wu20}.  This paper studies SGD on
            short timescales or near minima and shows that \textbf{gradient
            noise biases learning} toward low-curvature, low-noise regions of
            the loss landscape.

            %--  vs ode and sde  ----------------------------------------------
            %
            %
            Generalizing \cite{li18,we19b,zh19}, we model correlated,
            non-gaussian, non-isotropic, non-constant gradient noise and
            find qualitative differences in dynamics.  For example, we
            construct a non-pathological\footnote{%
                All higher derivatives exist and are quadratically bounded; the
                gradient noise at each weight vector is $1$-subgaussian.%
            } loss landscape on which SGD's trajectory
            \emph{ascends}.
            %We argue that our theory enhances practical intuitions.
            We
            verify our theory on convolutional CIFAR-10 and Fashion-MNIST loss
            landscapes.

            %--  soft benefits: retrospective  --------------------------------
            %
            \begin{wrapfigure}{r}{0.40\textwidth}
                \centering  
                \vspace{-0.40cm}
                \plotmoow{diagrams/paradigm}{0.99\linewidth}{}\vspace{-0.10cm}
                \caption{
                    \textbf{A sub-process of SGD}.  Timesteps index
                    columns; training data index rows.  The $5$th datum
                    participates in the $2$nd SGD update.  This
                    {\color{spacetimepurple}$(n=5,t=2)$ event} affects the
                    testing loss both directly and via the
                    {\color{spacetimeteal}$(1,12)$ event}, which is itself
                    modulated by the {\color{spacetimeindigo}$(2,5)$ event}. 
                }\vspace{+0.60cm}
                \label{fig:paradigm}
            \end{wrapfigure}
            Our theory offers a new physics-inspired perspective of SGD as a
            superposition of concurrent information-flow processes.  Indeed, we
            study the post-training testing loss by Taylor expanding it w.r.t.\
            the learning rate $\eta$.  We interpret the resulting terms as
            describing processes by which data influence weights.  E.g.\ an
            instance of the process\footnote{
                Throughout, colors help us refer to parts of diagrams; colors
                lack mathematical meaning.
            }
                \vspace{-0.25cm}
            $$
                \mdia{MOOc(01-2-3-4)(04-13-23-34)}
                \vspace{-0.65cm}
            $$
            is shown on the right.  Notating processes with such diagrams, we
            show in \S\ref{sect:main} how to compute the effect of each process
            and that summing the finitely many processes with $d$ or fewer
            edges suffices to answer dynamical questions to error $o(\eta^d)$.  
            We thus factor the analysis of SGD into the analyses of individual
            processes, a technique that may power future theoretical
            inquiries.  
            
    \section{Paradigmatic example: a non-conservative entropic force}\label{sect:entropic-example}
            This section illustrates our theory by sketching a path toward
            Corollary \ref{cor:entropic} while avoiding the generality and
            abstraction of \S\ref{sect:calculus}'s theorems and diagram
            notation.\squash\squash 

        \subsection{Notation and assumptions, I}\label{sect:setup}
            %--  the landscape  -----------------------------------------------
    
            We formalize the loss --- suffered by a fixed architecture on a
            random datapoint --- as a distribution $\Dd$ over functions from a
            space $\Mm$ of weights.  The \emph{testing loss} $l:\Mm\to\RR$ is
            $\Dd$'s mean.  We write $\theta\in\Mm$, $l_x\sim\Dd$ for generic
            elements.
            %
            We consider training sequences $(l_n: 0\leq n<N) \sim \Dd^N$.  We
            refer to $n$ and to $l_n$ as \emph{training points}.
            %
            Each initialization $\theta_0 \in \Mm$ then induces --- via SGD ---
            a distribution over trajectories $(\theta_t: 0\leq t \leq T)$.
            Specifically, SGD runs $T$ steps of $\eta$-steepest descent:
                \squash
            $$
                \textstyle
                \theta_{t+1}^\mu
                \coloneqq
                \theta_t^\mu -
                \sum_{\nu}
                \eta^{\mu\nu} \nabla_\nu l_{n_t}(\theta_t)
                \squash
            $$
            where each sequence $(n_t: kN\leq t<kN+N)$ is a permutation of $(n:
            0\leq n<N)$.  Here, Greek indices name components of
            $\theta,\eta,\nabla$ with respect to a fixed basis.  We view $\eta$
            as a bilinear form, not for generality but to constrain the natural
            operations to those of geometric significance.
            %

            Throughout this paper we assume:
            %
            \textbf{Derivative Bounds}:
            there compact sets $(K_k: k\geq 0)$ so that
            $\nabla^n l_x(\theta)\in K_k$ for all $\theta,l_x$. 
            Here $\nabla^k
            l_x(\theta)$ is a $k$th derivative, a tensor with $k$ axes.
            %
            \textbf{Analytic Moments}:
            any polynomial $p$ of the $l_x$ and its higher derivatives
            induces a random variable so that
            $\expct{p}:\Mm\to\RR$ (exists and) is analytic in $\theta$;
            moreover, $\expct{p}$'s radii of convergence are strictly bounded from $0$,
            even as $\theta$ varies.
            %
            %Thus, $\nabla \expct{p}=\expct{\nabla p}$.
            %
            We sometimes (each time explicitly) also assume
            \textbf{Local Strength}:
                on a nbhd.\ of $\theta_0$ some positive definite $Q_-,Q_+$
                bound every $l_x$'s hessian: ($Q_- < \nabla\nabla
                l_x(\theta)<Q_+$).

            We quote a well-known prop
            (\cite{ne04}, \S 2.1) to illustrate our notation:
            \begin{prop}\label{prop:nest}
                $G = \nabla l(\theta_0)$ controls the loss to leading order:
                $
                    \expc[l(\theta_T)-l(\theta_0)]^\mu =
                    - 
                    T \sum_{\mu\nu} G_\mu \eta^{\mu\nu} G_\nu
                    + o(\eta^1)
                $.
            \end{prop}
            Proof: induction on $T$.
            For noiseless linear landscapes, (i.e., when $\nabla l_x(\theta)$
            depends on neither $x$ nor $\theta$), this estimate is exact.
            %
            This paper identifies how noise and curvature correct Prop
            \ref{prop:nest} by developing and using large-$T$ techniques less
            opaque and more convergent than induction.
            %


        \subsection{Taylor series: method and challenges}\label{sect:challenges}
            %\subsubsection{Proving Prop \ref{prop:nest}}
            Let's study $\expct{\theta_T}, \expct{l(\theta_T)}$.  To warm
            up, we'll prove Prop \ref{prop:nest} (c.f.\ \cite{ne04,ro18}). 
            \begin{proof} %(of Prop \ref{prop:nest}).
                By gradient bounds: $\theta_T - \theta_0$ is $O(\eta^1)$.
                We \textbf{claim} that $(\theta_T - \theta_0)^\mu =
                -T\sum_\nu \eta^{\mu\nu}G_\nu + o(\eta^1)$.
                %
                The claim holds when $T=0$.  Say the claim holds for
                ${\tilde T}$-step SGD with
                $T = {\tilde T}+1$.  Then:\squash
                \begin{align*}
                    \wrap{\theta_{T} - \theta_{{\tilde T}}}^\mu
                    &= - \textstyle\sum_{\nu} \eta^{\mu\nu} \nabla_\nu l_{n_{\tilde T}}(\theta_{{\tilde T}}) \\
                    &= - \textstyle\sum_{\nu} \eta^{\mu\nu} \nabla_\nu \wrap{
                             l_{n_{\tilde T}}(\theta_0)
                             + \text{\translucent{moosky}{$\sum_{\xi} \nabla_\xi l_{n_{\tilde T}}(\theta_0) (\theta_{{\tilde T}} - \theta_0)^\xi$}}
                             + o(\theta_{{\tilde T}} - \theta_0)
                         } \\ 
                    &= - \textstyle\sum_{\nu} \eta^{\mu\nu} \nabla_\nu \wrap{
                                l_{n_{\tilde T}}(\theta_0)
                                + \nabla l_{n_{\tilde T}}(\theta_0) \cdot O(\eta^1) + o(O(\eta^1))
                            } \\
                    &= \textstyle\text{\translucent{moolime}{$- \sum_\nu \eta^{\mu\nu} \nabla_\nu l_{{\tilde T}}(\theta_0)$}} + o(\eta^1)
                \end{align*}
                Applying the induction hypothesis proves the claim.
                %
                We plug the claim into $l$'s Taylor series:
                \begin{align*}
                    \expc[l(\theta_T) - l(\theta_0)]
                    &= \textstyle \sum_\mu \nabla_\mu l(\theta_0) \text{\translucent{moopink}{$\expc[\theta_T - \theta_0)]^\mu$}} + \expc[o(\theta_T - \theta_0)] \\
                    &= \textstyle \sum_\mu \nabla_\mu l(\theta_0) (-T\eta G + o(\eta^1)) + o(O(\eta^1)) \\
                    &= \textstyle \text{\translucent{moogold}{$- \sum_{\mu\nu} T G_\mu \eta^{\mu\nu} G_\nu$}}+ o(\eta^1)
                \end{align*}
                Indeed, due to analytic moments, the above expectations of
                $o(\eta^1)$ terms are still $o(\eta^1)$.
            \end{proof}

            \subsubsection{What happens when we keep higher order terms?}

            \textsc{Multiple Moments} ---
            We used above
            that, to order $\eta^1$, $\expct{l(\theta_T)}$ depends on the
            training data only through the first moment
            \translucent{moopink}{$\expc[\theta_T - \theta_0]$}.\squish\squish\ But to compute  
            $\expct{l(\theta_T)}$ to higher order, we'd also need
            the $k$th moments $M_k^{\mu_0\mu_1\cdots} = \expc\wasq{\prod_i (\theta_T - \theta_0)^{\mu_i}}$.\squish\  We may achieve this by inductively
            proving multiple \textbf{claim}s, one for each moment. 

            \textsc{Tuples of Times} --- Complications arise even as we compute $M_1$\squish\ to order $\eta^2$.  We may not neglect the gradient
            correction $\nabla (\text{\translucent{moosky}{$\nabla
            l_{n_{\tilde T}}(\theta_0) \cdot (\theta_{{\tilde T}} -
            \theta_0)$}})$\squash\ at the $\tilde{T}$th induction step. As the
            displacement $\theta_{{\tilde T}} - \theta_0$ contains (to order
            $\eta^1$) $\tilde T$ terms, so will the correction.
            %
            Totalling the correction over time thus yields
            $\sum_{0\leq \tilde T<T}\tilde T = {T \choose 2}$\squish\
            summands, each (e.g.\ $\nabla\nabla l_{5} \nabla l_{2}$)
            involving a \emph{pair} of times.  Order-$d$
            corrections represent the joint influence of $d$-tuples of times.
            %
            Prop \ref{prop:nest}'s result $\sum_{\tilde T}
            \wrap{\text{\translucent{moolime}{$- \eta \nabla l_{{\tilde
            T}}(\theta_0)$}}}$\squish\squish\ is degree $1$ in $T$; but the
            order-$d$ displacement is a degree $d$ polynomial --- very divergent --- in $T$.

            \textsc{Factoring's Failure} --- To obtain \translucent{moogold}{$-TG\eta G$},\squish\ we multiplied
            $l$'s derivatives by the expectations of such summands.
            %
            In contrast to Prop \ref{prop:nest}, these expectations, even those of a fixed
            degree in $\eta$, now vary in form due to noise: some (e.g.\
            $\nabla\nabla l_{5} \nabla l_{2}$) have statistically independent
            factors that permit expectations to factor; others (e.g.\
            $\nabla\nabla l_{5} \nabla l_{5}$) do not.  This is how $\nabla
            l_x$'s higher cumulants (such as the covariance and skew of the gradient distribution) appear in our analysis.

            \textsc{Diverse Derivatives} --- 
            At order $\eta^3$, a hessian correction $\nabla((\theta_{{\tilde T}} -
            \theta_0) \cdot \nabla \nabla l_{n_{\tilde T}}(\theta_0) \cdot
            (\theta_{{\tilde T}} - \theta_0)/2)$ augments the gradient
            correction.
            %
            Then $M_1$'s order-$\eta^3$ summands vary in form, even when all
            expectations factor (as happens on noiseless landscapes).  For
            instance, the hessian and gradient corrections respectively induce
            order-$\eta^3$ summands of $\expct{l(\theta_T)}$ such as
            \squash
            $$
                \textstyle
                \sum_{\substack{\mu\nu\xi \\ \omicron\pi\rho}}
                    \eta^{\mu\omicron} \, \eta^{\nu\pi} \, \eta^{\xi\rho}
                    \,
                    (\nabla_\mu l_x)
                    \,
                    (\nabla_\nu l_y)
                    \,
                    (\nabla_\omicron\nabla_\pi\nabla_\rho l_z)
                    \,
                    (\nabla_\xi l)
                %
                \hspace{0.75cm}
                %
                \sum_{\substack{\mu\nu\xi \\ \omicron\pi\rho}}
                    \eta^{\mu\omicron} \, \eta^{\nu\pi} \, \eta^{\xi\rho}
                    \,
                    (\nabla_\mu l_x)
                    \,
                    (\nabla_\omicron\nabla_\nu l_y)
                    \,
                    (\nabla_\pi \nabla_\xi l_z)
                    \,
                    (\nabla_\rho l)
                \squash
            $$
            And $M_2, M_3$'s terms are yet more diverse.  In short, a Taylor
            expansion even to low degrees yields a combinatorial explosion of
            terms.  Our paper develops tools to organize and interpret these
            terms.

            \subsubsection{Diagrams in brief}\label{sect:diagrams-in-brief}

            That development begins with the observation that each $\eta$
            `connects' two $\nabla$ operators as indices prescribe.  So we
            draw
            $\eta$s as edges, $\nabla^k l$s as nodes, and each
            summand
            $$
                \sum_{\text{all Greek indices}} \wrap{\prod_{j\in J} \eta^{\mu_j\nu_j}}
                \wrap{\prod_{i\in I} \wrap{\prod_{k\in K_i} \nabla_{\xi_{i,k}}}
                l_{x_i}} \wrap{\prod_{k\in K_\star} \nabla_{\xi_{\star,k}}}
                l\,\,\,\,\,\,\,\,\text{(evaluated at $\theta=\theta_0$)}
            $$
            as an undirected graph with edges indexed by $j\in J$, nodes
            indexed by $i\in I\sqcup \{\star\}$, and an edge $j$ incident
            to a node $i$ when $\{\xi_{i,k}:k\in K_i\}$ meets
            $\{\mu_j,\nu_j\}$.  Per \textsc{factoring}, we also equip these
            graphs with a partition of nodes to account for correlation
            structure.
            %
            Such diagrams have advantages of compactness and of clarity as
            decimal numerals have over unary numerals.  Even more importantly,
            their topology has dynamical significance; diagrams thus permit a
            physical interpretation of SGD.

        \subsection{An entropic force with curl}\label{sect:entropic-curl}
            The displacement $M_1=\expc[\theta_T - \theta_0]$ contains many
            order-$\eta^3$ summands, including those of the form\squash
            $$
                \textstyle
                \Delta^\xi_{xyz} \propto 
                -
                \sum_{\substack{\mu\nu    \\ \omicron\pi\rho}}
                    \eta^{\mu\omicron} \, \eta^{\nu\pi} \, \eta^{\xi\rho}
                    \,
                \expct{
                    (\nabla_\mu l_x)
                    \,
                    (\nabla_\nu l_y)
                    \,
                    (\nabla_\omicron\nabla_\pi\nabla_\rho l_z)
                }
                \squash
            $$
            where $0\leq x,y,z<N$ label datapoints.
            Let $\Delta_\circ = \expc[\Delta_{xxz}-\Delta_{xyz}]$ for $x,y,z$
            distinct.\squash\  $l_x, l_y, l_z$ are i.i.d., so:
            $ 
                \Delta_\circ^\xi \propto 
                -
                \sum_{\cdots}
                \eta^{\mu\omicron} \, \eta^{\nu\pi} \, \eta^{\xi\rho}
                C_{\mu\nu} J_{\omicron\pi\rho}
            $
            or, schematically, 
                $\boxed{\Delta_\circ^\xi
                \propto -\eta^3 C\nabla H}$.\squish\ 
            Here, $C_{\mu\nu} = \expc_x[\nabla_{\mu} l_x \nabla_{\nu} l_x] - G_\mu G_\nu$ is the covariance of gradients,
            $H_{\pi\rho} = \nabla_\pi\nabla_\rho l$ is $l$'s hessian, and
            $J_{\omicron\pi\rho} = \nabla_\omicron H_{\pi\rho}$.
            %is $l$'s `jerk' (\href{https://www.iso.org/obp/ui/\#iso:std:iso:2041:ed-3:v1:en}{ISO 2041 (2009)}, \S1).
            %\footnote{
            %    `\textbf{J}erk' is a standard term for third derivatives in dynamical systems.  .
            %}

            \begin{wrapfigure}{r}{0.3\textwidth}
                \centering
                \crunch\squash
                \plotmoow{colt/cubic}{0.3\textwidth}{}
                \caption{%
                    {Gradient noise pushes SGD toward minima flat w.r.t.\ $C$.}%$\mathbf{C}$.}
                        \small
                        A 2D loss near
                        a valley of minima.  Red densities show typical
                        $\theta$s, perturbed by noise ($C$),
                        in two cross sections of the valley.  The hessian
                        changes across the valley: $J \neq 0$.  
                }
                \label{fig:cubic}
            \end{wrapfigure}
            The expression $-\eta^3 C\nabla H$ suggests that SGD moves
            toward flat minima (see  Figure \ref{fig:cubic}).
            %
            The bilinear form
            $F^{\mu\nu}=\textstyle\sum_{\xi\omicron}\eta^{\mu\xi}
            \eta^{\nu\omicron} C_{\xi\omicron}$ determines which $H$s
            are `large' or `small'.  E.g.: if $F$
            degenerates along a covector $v$, then $H$'s 
            $v$-component does not affect $\Delta_\circ$.
            %\footnote{
            %    Explicitly: if
            %        $\sum_{\mu\nu} v_\mu F^{\mu\nu} v_\nu = 0$,
            %    then replacing $H$ by $\tilde H_{\mu\nu} = H_{\mu\nu} + 42 v_\mu v_\nu$ 
            %    will not change $\Delta_\circ$.
            %}
            %
            Diagram techniques establish\squash\
            this `entropic force'\footnote{
                Thermal systems tend toward disorder as if pushed by an
                `entropic force'.
                So arises the tension of rubber
                bands: their polymers can wreathe and coil in
                many ways; can be straight in only one.
                Such `forces' characteristically depend linearly on the
                temperature (i.e., noise scale).  As the Corollary
                scales with $C$, we regard it as describing an entropic force.
            }\footnote{
                Our result ($T\gg 1$) is $\Theta(\eta^2)$; \cite{ya19b}'s
                ($T=2$) is $\Theta(\eta^3)$.  We
                integrate noise over time, amplifying $C$'s
                effect. 
            }
            as dominant when $G=0$, $N=T$.  
            Evaluating a single diagram
            ($
                \sdia{c(01-2-3)(02-12-23)}
            $) yields:\squash\squash
            %
            \begin{cor}%[Computed from $\sdia{c(01-2-3)(02-12-23)}$]
                \label{cor:entropic}
                Start SGD at a minimum of $l$ with $H>0$, $N=T$ and use
                an eigenbasis of $K=\eta H$.  
                For any $T$, the final displacement is
                \squish\squish
                $$
                    M_1^\xi =
                    -
                    {\textstyle\sum_{\substack{\mu\nu    \\ \omicron\pi\rho}}}
                        C_{\mu\omicron}
                        {\color{gray}{\mathcal P}_T(K_{\mu\mu} + K_{\omicron\omicron})}
                        \eta^{\mu\nu}\eta^{\omicron\pi}
                        J_{\nu\pi\xi}
                        {\color{gray}{\mathcal P}_T(K_{\xi\xi})}\eta^{\xi\rho}/2
                    + o(\eta^3)
                    \squish\squish
                $$
                %with ${\mathcal P}_T(s) = \lim_{s^\pr\to s}(1 - \exp(-Ts^\pr))/s^\pr$.
                with ${\mathcal P}_T(s) = (1 - \exp(-Ts))/s$.
                Assuming Local Strength (\S\ref{sect:setup}), 
                %the error decays uniformly, i.e.,
                $\lim_{T\to\infty} \epsilon(\eta,T)$ exists and is $o(\eta^2)$
                for
                $\epsilon(\eta,T) = M_1 + \sum \eta^3 C {\mathcal P} J
                {\mathcal P}/2$.
            \end{cor}\squash\squash

            Consider a 1D valley of near-minima wherein $\eta H$ has spectrum
            $\lambda_0 \ll 1/T \ll \lambda_1 \leq \cdots$ for eigenvectors
            $v_i$.  Let's ignore diffusion along the valley: $(\eta C) v_0 =
            0$.
            %
            Then ${\mathcal P}_T(\lambda_0)\approx T$ and every
            $C_{ij}{\mathcal P}_T(\lambda_i+\lambda_j)$ is $O(T^0)$.  So $M_1$
            scales linearly with $T$.  We thus expect SGD to move with
            velocity $-f(\eta,H,T)\cdot C\nabla H/2$ toward flat minima.  
            Observe that $\nabla ({\frak G}_C \star l) = \nabla l+C\nabla
            H/2 + o(C)$, where ${\frak G}_C \star$ denotes convolution with a
            \emph{fixed} centered $C$-shaped Gaussian; we conclude with the
            intuition that \emph{SGD descends on a $C$-smoothed landscape that
            changes as $C$ does}.  Since the smoothed $l$ may itself evolve,
            SGD might eternally circulate.
      
        %\subsection{Curl in the entropic force}
            \begin{figure}[h!]
                \centering
                \rotatebox[origin=c]{-90}{\plotmoow{plots/from-above}{0.19\textwidth}{}}
                \plotmooh{colt/screw-trajectory-cropped}{}{0.19\textwidth} 
                \plotmooh{plots/neurips-thermo-linear-screw}{}{0.19\textwidth}
                \squash
                \caption{%
                    \textbf{Leftmost}: \Helix\ is defined on a 3D $\Mm$ that
                    extends into and out of the page.  A helical
                    level surface $S$ (orange-green) of $l$ winds around 
                    a 1D valley of minima orthogonal to the
                    page.  $l$ $\gg$ $1$ outside $S$.  Gradient noise
                    is parallel to the page and to the line between outer tubes.
                    %
                    Thanks to
                    \href{https://www.monroecc.edu/faculty/paulseeburger/calcnsf/CalcPlot3D/}{CalcPlot3D}.
                    %
                    \textbf{Center}: SGD moves
                    into the page.  In green: SGD's trajectory over 
                    cross sections of $\Mm$ that descend progressively
                    into the page.  In blue: $l$'s contours; $l$'s valley
                    intersects each pane's center.  Dotted
                    curves help compare adjacent panes.  Red bi-arrows: $C$'s
                    major axis.
                    %
                    Gradient noise kicks $\theta$ from A; $\theta$ then falls
                    (\hspace{-0.08cm}\protect\offour{1}) to B in \protect\offour{2}.  At C,
                    noise kicks $\theta$ uphill (\hspace{-0.08cm}\protect\offour{3}); $\theta$
                    thus never settles and the phenomena depicted here continue
                    forever.
                    {\bf Right}: Predictions near minima excel for
                    large $\eta T$.
                }
                \squash\squash
                \label{fig:archimedes}
            \end{figure}

            To test Corollary \ref{cor:entropic}'s $C$-dependence,
            \S\ref{appendix:artificial} constructs a landscape, \Helix, on
            whose valley of global minima $C$ varies (Figure
            \ref{fig:archimedes}).  As in Rock-Paper-Scissors, each point
            $\theta$ has a neighbor that is more attractive (flatter) with
            respect to $C(\theta)$.  This permits eternal motion into the page
            despite the landscape's discrete translation symmetry in that
            direction.  Corollary \ref{cor:entropic} predicts a velocity of
            $+\eta^2/6$ per timestep, while \cite{ch18}'s SDE-based analysis
            predicts a constant velocity of $0$.\footnote{
                Indeed, \Helix' velocity is $\eta$-perpendicular to the image
                of $(\eta C)^\mu_\nu$ in tangent space.
            }
            One may add a small linear term to \Helix\ to make SGD eternally
            ascend; one may wrap \Helix\ in a loop to make SGD circulate,
            witnessing a velocity field with curl (\S\ref{appendix:artificial}).
            %; this is
            %possible because $C\nabla H$, unlike $\nabla(CH)$, is not a total
            %derivative. 
            %
            %In avoiding \cite{we19b}'s constant-$C$ assumption, we
            %find that SGD's velocity field typically has curl. 
            
    \section{Perturbative theory of SGD}\label{sect:calculus}

        %\S\ref{sect:exegesis} illustrates by example a Taylor series approach
        %to studying SGD dynamics.
        \S\ref{sect:diagrams} introduces a diagram
        notation for the formulae that thus appear.  \S\ref{sect:using} shows
        how to evaluate diagrams to obtain numbers.  \S\ref{sect:main} states
        our main result: that diagram-based analysis is correct.

        \subsection{Notation and assumptions, II}\label{sect:background}

            %--  names of sgd parameters  -------------------------------------

            \begin{wrapfigure}{r}{0.4\textwidth}
                \squash\squash
                \begin{tabular}{lclcl}
                    $G_\mu$         &$=$& $\expc[\nb_\mu\lx]$                           &$\leftrightsquigarrow$& $\mdia{MOO(0)(0)}       $                  \\
                    $H_{\mu\nu}$    &$=$& $\expc[\nb_\mu\nb_\nu\lx]$                    &$\leftrightsquigarrow$& $\mdia{MOO(0)(0-0)}     $                  \\ 
                    $J_{\mu\nu\xi}$ &$=$& $\expc[\nb_\mu\nb_\nu\nb_\xi\lx]$             &$\leftrightsquigarrow$& $\mdia{MOO(0)(0-0-0)}   $\squash           \\
                    $C_{\mu\nu}$    &$=$& $\expc[(\nb\lx - G)^{\otimes 2}]_{\mu\nu}$    &$\leftrightsquigarrow$& $\mdia{MOOc(01)(0-1)}   $\squash\squash    \\
                    $S_{\mu\nu\xi}$ &$=$& $\expc[(\nb\lx - G)^{\otimes 3}]_{\mu\nu\xi}$ &$\leftrightsquigarrow$& $\mdia{MOOc(012)(0-1-2)}$
                \end{tabular}
                \crunch
                \caption{
                    \textbf{Named tensors}, typically evaluated at
                    initialization ($\theta=\theta_0$).  Def.\
                    \ref{dfn:uvalue-body} explains how diagrams depict tensors.
                }
                \squash\squash
                \label{fig:tensor}
            \end{wrapfigure}
            \textsc{Batches and Epochs} --- Generalizing \S\ref{sect:setup}, our theory describes SGD with
            any number
                {$\mathbf{N}$ of training points},
                {$\mathbf{T}$ of updates}, and 
                {$\mathbf{B}$ of points per batch}.
            SGD runs $T$ updates (hence
                {$\mathbf{E}=TB/N$ epochs} or
                {$\mathbf{M}=T/N$ updates per training point}) of the form
            %    \translucent{moolime}{$\mathbf{N}$ of training points},
            %    \translucent{moolime}{$\mathbf{T}$ of updates}, and 
            %    \translucent{moolime}{$\mathbf{B}$ of points per batch}.
            %Specifically, SGD runs $T$ many updates (hence
            %    \translucent{moolime}{$\mathbf{E}=TB/N$ epochs} or
            %    \translucent{moolime}{$\mathbf{M}=T/N$ updates per training point}) of the form
            $$
                %\textstyle
                \theta_{t+1}^\mu
                \coloneqq
                \theta_t^\mu -
                \sum_\nu
                \eta^{\mu\nu} \nabla_\nu
                    \sum_{n\in \Bb_t} l_n(\theta) / B
            $$
            where in each epoch %$(k(N/B)\leq t<(k+1)(N/B))$,
            we sample the
            $t$th batch $\Bb_t$ without replacement from the training sequence.
            %
            %We especially study
            %the \textbf{final testing loss} $\expc[l(\theta_T)]$.
            %and the \textbf{final displacement} $\expc[\theta_T-\theta_0]$.

            %--  tensor conventions  ------------------------------------------

            \textsc{Tensors} --- We make heavy use Figure \ref{fig:tensor}'s tensors;
            $G, H, J, C, S$ have $1, 2, 3, 2, 3$ many axes.%, respectively.

            \textsc{Vectors vs Covectors} --- 
                Those axes all transform under change-of-basis like covectors.
                \emph{What does this mean}?  Imagine an air-conditioned
                hallway's temperature gradient ($0.1^\circ
                \text{K}/\text{meter}$) and length ($20\,\text{meters}$).
                When we switch units from m to cm, the temperature
                gradient numerically \emph{decreases} (to $0.001$) but the length
                numerically \emph{increases} (to $2000$).  So gradients
                (covectors) and displacements (vectors) are distinct geometric
                concepts.  We respect this distinction throughout; that's why our $\eta$ is a tensor (whose two
                axes transform like vectors).
                %
                \cite{mi73} (\S2.5) illustrates the distinction(=it) with
                helpful pictures; \cite{ko93} (\S7,14) pinpoints the sense in
                which to ignore it is unnatural; \cite{cu87} (\S1.4) relates
                it to common tensors in statistics.

            \textsc{Little-{\rm o} Notation} --- A (potentially tensor)
            quantity $q$ \emph{vanishes to order $\eta^d$} when
            for some homogeneous degree-$d$ polynomial $p$
            $\lim_{\eta\to 0} q/p(\eta) = 0$; we then say $q\in o(\eta^d)$.

            %--  example  -----------------------------------------------------

            %and how evaluation of a single diagram 
            %    \squash
            %$$
            %    \sdia{c(012-3)(03-13-23)}
            %    \crunch
            %$$
            %gives a case of
            %%the leading order result
            %Corollary \ref{cor:vsode}, quoted
            %here assuming isotropic curvature ($\eta H = cI$) and $E=B=1$:\squash
            %\begin{prop}\label{prop:splash}
            %    Assume Local Strength and initialize at a minimum of $l$.  Then
            %    in the large-$T$ limit, the skewness $S$
            %    contributes 
            %    $
            %        - \sum_{\substack{\mu\nu\xi\\ \omicron\rho\pi}} \eta^{\mu\omicron}\eta^{\nu\rho}\eta^{\xi\pi} S_{\mu\nu\xi}
            %        J_{\omicron\rho\pi} / (18 c) + o(\eta^2)
            %    $
            %    to the final testing loss $\expc[l(\theta_T)]$. 
            %    \squash
            %\end{prop}
            %So skewness affects loss in proportion to the logarithmic
            %derivative $J/H$ of curvature.  Prop \ref{prop:splash} is order
            %$\eta^2$ and thus a leading correction
            %to Prop \ref{prop:nest}.  Gaussian approximations (e.g.\
            %SDE) miss this effect. 

            %\newpage
        \subsection{Diagrams arise from Taylor series and depict information-flow processes}\label{sect:diagrams}

            We formalize \S\ref{sect:diagrams-in-brief}'s identification of
            terms (in $\expc[l(\theta_T)]$'s Taylor expansion) with diagrams.
            (Throughout, colors help us refer to parts of diagrams; colors lack
            mathematical meaning.)
            \begin{dfn}[\S\ref{appendix:toward-diagrams}]
                \emph{A \translucent{moolime}{\textbf{diagram}} is a rooted tree equipped with an
                    equivalence relation on (i.e.\ a partition of) its non-root
                    nodes.  We orient the tree left-to-right so that children
                    precede parents; the root is thus rightmost.  We draw the
                    partition structure with minimally many outlined ties.}\mend 
            \end{dfn}
            \begin{table}[h!]
                \centering
                \crunch
                \adjustbox{width=\textwidth}{
                \begin{tabular}{ccc}
                    examples: of valid diagrams
                    &\hspace{0.2cm}& of invalid diagrams \\ \hline\squash 
                    $\mdia{c(0-1)(01)},
                    \mdia{c(0-1-2)(02-12)},
                    \mdia{c(012-3)(02-12-23)},
                    \mdia{c(01-2-3)(02-12-23)},
                    \mdia{c(0-1-2-3)(01-12-23)}, 
                    \dia{MOOc(03-12-4)(03-13-23-34)}$
                        &\hspace{0.2cm}&
                    $\mdia{MOOc(01)(0-1)},  
                     \mdia{MOOc(01)(01)}, 
                     \dia{MOOc(0-1-2)(01-02-12)},
                     \dia{MOOc(0-1-2)(01-02)}, 
                     \dia{MOOc(0-1-2)(02)},$
                     \adjustbox{trim=0 0 {.5\width} 0}{$\dia{MOOc(02-012-3)(02-12-23)}$}
                     ~~
                     \squash
                \end{tabular}
                }
                \crunch
            \end{table}
            Since a diagram is just a rooted tree and partition,
            $
                \sdia{c(01-2-3)(01-13-23)} = 
                \sdia{c(02-1-3)(02-13-23)} = 
                \sdia{c(0-12-3)(03-12-23)} 
            $ are the same diagrams.

            \squash
            \begin{dfn}[\S\ref{appendix:evaluate-embeddings}]\label{dfn:uvalue-body}
                \emph{
                    A diagram $D$'s \emph{un-resummed value} (\translucent{moolime}{\textbf{uvalue}})
                    is a product with one factor of $l_x$'s $d$th derivative
                    for each degree-$d$ node, grouped
                    under cumulant symbols $\CC$ (think: expectation symbols $\expc$)\footnote{
                        Inconsequential technicality: uvalues are products of
                        \emph{cumulants} such as $C$, not of moments such as
                        $GG+C$.
                        %this will not concern us
                        %until the Appendix. See
                        \S\ref{appendix:evaluate-embeddings}.
                    }
                    per $D$'s gray outlines, and tensor-contracted 
                    via a %per $D$'s edges.  We thus have a
                    factor $\eta^{\mu\nu}$ for each edge.
                }\mend
            \end{dfn}
            The cumulant symbol $\CC[a]$ is $a$'s mean; $\CC[a\cdot b]$ is
            $a,b$'s covariance instead of $(ab)$'s mean; in general, we
            define higher cumulants $\CC[\prod_i a_i]$ recursively to `center
            them with respect to lower cumulants' (see 
            \S\ref{appendix:evaluate-embeddings};
            this centering won't be crucial to our discussion).  For example:
                \newcommand{\AAA}{{\color{black}\nabla_\mu}}
                \newcommand{\BBB}{{\color{black}\nabla_\nu}}
                \newcommand{\CCC}{{\color{black}\nabla_\xi}}
                \newcommand{\DDD}{{\color{black}\nabla_\omicron}}
                \newcommand{\EEE}{{\color{black}\nabla_\pi}}
                \newcommand{\FFF}{{\color{black}\nabla_\rho}}
                \newcommand{\ww}[1]{\,\CC[#1]\,}%\wasq{#1}}
                \newcommand{\rRr}[1]{{\color{moor}#1}}
                \newcommand{\gGg}[1]{{\color{moog}#1}}
                \newcommand{\bBb}[1]{{\color{moob}#1}}
                \newcommand{\sixsum}{\textstyle\sum_{{\mu\nu\xi\omicron\pi\rho}} }
            \begin{figure}[H]
                \centering  
                \crunch
                \begin{tabular}{rcl}
                    $\text{uvalue}(\mdia{c(01-2-3)(02-13-23)})$ &
                    $=$ &
                    $\sixsum \eta^{\mu\xi} \eta^{\nu\pi} \eta^{\omicron\rho} \, \ww{(\rRr{\AAA l_x}) \cdot (\rRr{\BBB l_x})}       \ww{(\gGg{\CCC \DDD      l_x})} \ww{(\bBb{\EEE \FFF l_x})}$\\
                    \crunch\squash\squash
                    \\                   
                    $\text{uvalue}(\mdia{c(01-2-3)(02-12-23)})$ &
                    $=$ &                                       
                    $\sixsum \eta^{\mu\xi} \eta^{\nu\omicron} \eta^{\pi\rho} \, \ww{(\rRr{\AAA l_x}) \cdot (\rRr{\BBB l_x})}       \ww{(\gGg{\CCC \DDD \EEE l_x})} \ww{(\bBb{     \FFF l_x})}$\\
                    \crunch\squash\squash
                    \\
                    $\text{uvalue}(\mdia{c(012-3)(02-12-23)})$ &
                    $=$ &
                    $\sixsum \eta^{\mu\xi} \eta^{\nu\omicron} \eta^{\pi\rho} \,\, \ww{(\rRr{\AAA l_x}) \cdot (\rRr{\BBB l_x})  \cdot     (\rRr{\CCC \DDD \EEE l_x})} \ww{(\gGg{     \FFF l_x})}$
                \end{tabular}
                \crunch
                %\caption{
                %    Illustration of \textbf{Def.\ 
                %    \ref{dfn:uvalue-body}}.
                %}
                \label{fig:uvalue-example}
            \end{figure}

            There are dozens of small diagrams.  In many analyses, only a few
            diagrams are relevant.  Examples: for fixed $T$, \textbf{to order
            $d$ we may neglect diagrams with more than $d$ edges};
            %
            if $E=B=1$ (\S\ref{sect:background}), each diagram with an ancestor-descendant pair in the
            same part contributes zero to $\expc[l(\theta_T)]$; for $\theta_0$ a minimum of
            $l$, all diagrams vanish that contain a leaf node
            participating in no gray outline.%\footnote{
                %Recall that $E,T,B,M,N;G,H,J,C,S;\Dd,\Mm,l_x,\eta,\theta$
                %are defined in \S\ref{sect:background}.
            %}

        \subsection{Diagrams overcome the challenges of using Taylor series to study SGD}\label{sect:using}

            %-----  embeddings and patterns of influence  ---------------------
            %\subsubsection{Embeddings}
            Having expressed terms in $\expc[l(\theta_T)]$'s Taylor expansion %(\ref{eq:dyson})
            as uvalues of
            diagrams, we seek the coefficient for each uvalue.  Intuitively, a
            diagram represents a process (as in Figure \ref{fig:paradigm}) and
            a diagram's contribution scales with the number of ways that
            process may occur.
            Specifically, we count \emph{embeddings}, defined below with
            respect to given SGD hyperparameters $N,E,B$.\footnote{
                and w.r.t.\ a deterministic selector of the
                $t$th batch.  One may take expectations over such
                algorithms --- \S\ref{appendix:draw-spacetime}. 
            }
            We then re-state (\ref{eq:dyson}) as a sum over embeddings. 
            %This enables us to restate (\ref{eq:dyson}) into what intuitively
            %is a ``sum over histories'', where each history is a concrete
            %instantiation of the abstract process represented by a diagram: 
            \begin{dfn}
                \emph{An \textbf{embedding} of a diagram is an assignment of non-root
                nodes to $\emph{(n,t)}$ pairs such that: the $\emph{n}$th
                training point participates in the $t$th batch; parents'
                $\emph{t}$s strictly exceed their children's $\emph{t}$s; and
                any two nodes' $\emph{n}$s are equal if and only if the nodes
                are in the same part of the partition.}\mend 
            \end{dfn}
            \begin{klem*}%\emph{(\textbf{restated})} %\label{thm:pathint}
                For all $T$: for $\eta$ sufficiently small, the final testing
                loss is:
                \vspace{-0.15cm}
                \begin{equation}\label{eq:sgdcoef}
                    \expc[l(\theta_T)]=
                    \sum_{\substack{D~\text{a} \\ \text{diagram}}}
                    ~
                    \sum_{\substack{f~\text{an embed-} \\ \text{-ding of}~D}}
                    ~
                        \frac{1}{\wabs{\Aut_f(D)}}
                    \frac{\uvalue(D)}{(-B)^{|\edges(D)|}}
                \end{equation}

                \vspace{-0.25cm}
                \noindent
                Here, $\wabs{\Aut_f(D)}$ counts the graph automorphisms of $D$
                that preserve $f$. (Typically $\wabs{\Aut_f(D)}=1$.)% (see \ref{appendix:sum-embeddings} for  ).
            \end{klem*}

            For example, $\sdia{(0-1)(01)}$ has just one non-root node.  It has
            as many embeddings as there are $(n,t)$ cells where $n$ participates
            in the $t$th update, i.e.,
            $B\cdot T$ embeddings.  Since $\sdia{(0-1)(01)}$ is the only
            diagram with one edge, it gives the full $\eta^1$ contribution to
            final testing loss.  We thus obtain Prop
            \ref{prop:nest}:
                \vspace{-0.15cm}
            $$
                -(\text{\# of embeddings of~}\sdia{(0-1)(01)}) \cdot \uvalue(\sdia{(0-1)(01)}) / B
                =
                -T \cdot \textstyle\sum_{\mu\nu} G_\mu \eta^{\mu\nu} G_\nu
            $$
                \vspace{-0.65cm}

            %-----  isolate effect of tensors; crossing symmetries  -----------
            Diagrams streamline analysis of SGD because it is in practice
            straightforward to count a diagram's embeddings.  
            %
            Also, the topology of diagrams has dynamical significance: the $t^d
            T^{-p}$-th order correction\footnote{We compare ODE integrated to
            time $t$ to $T$ steps of SGD with $\eta = \eta_\star t/T$ and
            $E=B=1$, and we assume $p\neq 0$.} to the ODE approximation of SGD
            is given by diagrams with $d$ edges and $p$ many outlined ties.
            Likewise, if we seek to isolate the effect, say, of $C$ or $H$ or
            $S$ or $J$, we may consider only those diagrams that contain the
            corresponding subgraph in Figure \ref{fig:tensor}.

            %
            %Moreover, we may
            %compute the effect of the skewness of gradient noise in isolation
            %by evaluating only those diagrams containing
            %$\sdia{MOOc(012)(0-1-2)}$.    

            %We interpret edges as carrying influence from the training set
            %toward the test measurement (figure \ref{fig:paradigm}). 

            %\begin{wrapfigure}{r}{0.4\textwidth}
            %    \centering  
            %    \plotmoow{diagrams/spacetime-f}{0.99\linewidth}{}
            %    \caption{
            %        %\textbf{Edges carry information}.
            %        Embedding of $\mdia{MOOc(01-2-3-4)(04-13-23-34)}$.
            %    }
            %    \vspace{-0.20cm}
            %    \label{fig:intuition1}
            %\end{wrapfigure}

            %    Moreover, a diagram's uvalue depends only on its
            %    graph and partition structures (not on its root), so, e.g.:\footnote{The physics-oriented reader
            %    will recognize this as a \emph{crossing symmetry}.}
            %    $$\uvalue(\sdia{c(0-1-2)(01-12)})=
            %    \uvalue(\sdia{c(0-1-2)(02-12)})$$ 
            %    %These relations reduce the effort of evaluating (\ref{eq:dyson}).
            %    These relate the contribution of a process (e.g.\
            %    $\sdia{c(0-1-2)(01-12)}$, whose degree-two node temporally
            %    separates its neighbors) to a time-distorted version of the process
            %    (e.g.\ $\sdia{c(0-1-2)(02-12)}$, whose
            %    degree-two node succeeds its neighbors). 


            %\newpage
            %-----  resummation  ----------------------------------------------
            \subsubsection{Resummation}\label{sect:resummation}
            So far, diagrams have been a convenient but dispensible
            book-keeping tool; so far, \S\ref{sect:challenges}'s polynomial
            divergence remains in (\ref{eq:sgdcoef}).  We now show how
            diagrams enable us to tame this divergence.

            Let us collect similar diagrams, where our notion of
            `similar' permits chains to grow or shrink (see Definition
            \ref{dfn:link}).  We obtain lists (each conveniently represented by
            its smallest member) such as: 
            \vspace{-0.30cm}
            \begin{align*}
                \sdia{c(0-1)(01)},
                \sdia{c(0-1-2)(01-12)},
                \sdia{c(0-1-2-3)(01-12-23)},
                \mdia{MOOc(0-1-2-3-4)(01-12-23-34)},\cdots
                &&
                \sdia{c(01-2)(02-12)},
                \sdia{c(02-1-3)(01-13-23)},
                \mdia{MOOc(03-1-2-4)(01-12-24-34)},
                \mdia{MOOc(02-1-3-4)(01-14-23-34)},\cdots
            \end{align*}
            \vspace{-0.60cm}


            \begin{wrapfigure}{r}{0.35\textwidth}
                \centering  
                \vspace{-0.50cm}
                \dmoo{2.5cm}{spacetime-g}
                %\dmoo{3cm}{spacetime-h}
                \caption{
                    \textbf{Resummation propagates information damped by
                    curvature}.
                    %\textbf{Left}:
                    Each resummed value (here, for $\sdia{c(0-1)(01)}$)
                    represents many un-resummed values, four shown here, each
                    modulated by the Hessian ($\sdia{MOOc(0)(0-0)}$) in a
                    different way.
                    %
                    %\textbf{Right}: One of many un-resummed terms
                    %captured by a single resummed term for
                    %$\sdia{c(01-2)(02-12)}$.
                    %Because two nodes appear at $(n=2,t=2)$, the process shown
                    %is an effect of the 2nd cumulant of the gradient noise
                    %distribution.
                }
                \vspace{-1.25cm}
                \label{fig:resumintuition}
            \end{wrapfigure}
            \noindent
            We will express in closed form the total contribution to
            (\ref{eq:sgdcoef}) of all diagrams in such a list.  The idea is that
            the uvalues of chains are powers of hessians --- e.g.\
            $\uvalue(\sdia{MOOc(0-1-2-3-4)(01-12-23-34)}) = GH^3G$ --- so we
            may sum over chain lengths via geometric series.

            %This is the \emph{resummed value} (\emph{rvalue}) of the list's
            %smallest member.
            We define an embedded diagram's \emph{resummed value} or
            \emph{\textbf{rvalue}} as we defined
            the $\uvalue$, except that we use $(I-\eta H)^{\Delta t-1}\eta$
            instead of $\eta$ to contract a pair of tensors embedded 
            $\Delta t$ timesteps apart.
            %
            For example, take Figure \ref{fig:resumintuition}'s embedding of
            $\sdia{c(0-1)(01)}$ (topmost of four).  The associated uvalue  is
            $\sum_{\mu\nu} G_\mu\eta^{\mu\nu}G_\nu$: a $G$ for each degree-one node and an
            $\eta$ for each edge.  By contrast, the associated rvalue is
            $\sum_{\mu\nu} G_\mu((I-\eta H)^{11-1}\eta)^{\mu\nu}G_\nu$ since the edge spans
            $11$ timesteps.  Distributing out this expression reveals uvalues for
            embeddings of $\sdia{c(0-1-2)(01-12)}$, etc.
            %A sum over embeddings is a sum of terms with a
            %range of exponents in place of $12-1$.

            \squash\squash
            \begin{dfn}\label{dfn:link}
                \emph{A \textbf{link} is a degree-$2$ non-root node that
                participates in no gray outlines.  
                E.g.\ $\sdia{c(02-1-3)(01-13-23)}$ has one link (green).
                To \emph{reduce} at a link, we
                replace the link by a black edge connecting the link's two
                neighbors.  E.g.\ $\sdia{c(02-1-3)(01-13-23)}\rightsquigarrow\sdia{c(01-2)(02-12)}$.  Reduction generates an equivalence relation on
                diagrams. Each equivalence class contains exactly one
                \textbf{linkless} diagram.  }\mend
            \end{dfn}

        \subsection{Main result}\label{sect:main}
    
            %-----  recipe for test loss  -------------------------------------

            Theorem \ref{thm:resum} expresses SGD's testing loss as a sum over
            diagrams.  A diagram with $d$ edges scales as $O(\eta^d)$, so the
            following is a series in $\eta$.  In practice, we truncate the
            series to small $d$ (invoking Theorem \ref{thm:converge} when
            possible), thus focusing on few-edged diagrams.  Here we state a special case:
            \begin{thm} \label{thm:resum}
                For any $T$: for $\eta$ small enough, the final testing loss is
                a sum over \emph{linkless} diagrams: 
                \begin{equation*} \label{eq:resum}
                    \expc[l(\theta_T)]=
                    \sum_{\substack{D~\text{a linkless} \\ \text{diagram}}}
                    ~
                    \sum_{\substack{f~\text{an embed-} \\ \text{-ding of}~D}}
                    ~
                    \frac{1}{\wabs{\Aut_f(D)}}
                    \,
                    \frac{{\rvalue_f}(D)}{(-B)^{|\edges(D)|}}
                \end{equation*}
            \end{thm}
    
            %-----  simplifications  ------------------------------------------
     
            \begin{rmk} \label{rmk:integrate}
                \emph{In practice, we replace sums over embeddings by
                integrals over $t$; $(I-\eta H)^t$, by $\exp(- \eta H t)$.
                To thus reduce to a routine integration of exponentials
                costs
                an error factor $1 + o(\eta)$.}\mend
                %--- and thus preserving
                %leading-order results.}\mend
            \end{rmk}
    
            %-----  convergence  ----------------------------------------------
     
            \begin{thm} \label{thm:converge}
                If $\theta_\star$ is a non-degenerate local minimum of $l$
                (i.e., $H(\theta_\star) > 0$), then for
                constant-$M$ SGD initialized close to $\theta_\star$, the
                $d$th-order truncation of Theorem \ref{thm:resum} converges as
                $T\to \infty$.
                Assuming Local Strength, the truncation's large-time error
                $\lim_{T\to\infty} \epsilon(\eta,T)$ exists for small $\eta$
                and is $o(\eta^d)$
            \end{thm}
            \begin{rmk}
                \emph{
                Even without Local Strength, we have not in practice observed the
                pathologies (of non-commuting limits, i.e., of consequentially non-uniform
                little-$o$s) that Thm \ref{thm:converge} precludes.  We take
                as a physically plausible and empirically supported hypothesis 
                that Thm \ref{thm:resum} predicts SGD's behavior
                for practical (finite but large) $T$s and landscapes.}\mend
                %Caution: the $T\to \infty$ limit in Theorem \ref{thm:converge}
                %might not measure any well-defined limit of SGD, since the
                %limit might not commute with the infinite sum.  We see no such
                %pathologies in practice, so we will freely speak of ``SGD in
                %the large-$T$ limit'' as informal shorthand when referencing
                %this Theorem.
            \end{rmk}

    \section{Consequences of the theory}\label{sect:consequences}

        %\begin{samepage}
        By Cor.s \ref{cor:batch} and \ref{cor:epochs}, gradient noise repels SGD.
        By Cor.\ \ref{cor:vsode}, SGD senses changes in $H$ more than
            SDE; in fact, (Cor.\ \ref{cor:entropic}) SGD seeks small-$H$
            weights.
        Cor.\ \ref{cor:overfit} relates
            $C$ and $H$ to overfitting.  These
            results do not exhaust our theory's scope;
            \S\ref{appendix:future} sketches 
            extensions to Hessian methods and natural GD.

        \subsection{Gradient noise repels SGD}\label{subsect:epochs-batch}
        %\end{samepage}
            \begin{wrapfigure}{r}{3.3cm}
                \vspace{-0.60cm}
                \plotmooh{colt/chladni-drift}{}{3.2cm}
                \vspace{-0.20cm}
                \caption{
                    Chladni drift on $\Mm=\RR^2$.  Red bi-arrows depict
                    $C(\theta)$'s major axis.  SGD updates (green) tend toward
                    small $C$.
                }
                \vspace{-1.00cm}
            \end{wrapfigure}
            Physical intuition suggests that noise repels SGD: if two
            neighboring regions of weight space have high and low levels of
            gradient noise, respectively, then
            %we expect
            the rate at which
            $\theta$ jumps from the former to the latter
            exceeds%to exceed
            the opposite rate.
            %
            There is thus a net movement toward regions of small $C$.\footnote{
                This is the same mechanism by which sand on a vibrating
                plate accumulates in quiet regions \citep{ch87}.  We thus dub
                the SGD phenomenon the
                \href{http://dataphys.org/list/chladni-plates/}{Chladni
                drift}.
            }
            %
            Our theory makes this precise; $\theta$ drifts in the direction
            $-\nabla C$, and the effect is weaker when gradient noise is
            averaged out by large batch sizes:
            \begin{cor}[Computed from $\sdia{c(01-2)(01-12)}$] \label{cor:batch}
                SGD with $E=B=1$ avoids high-$C$ regions more than GD:
                $
                    \expct{\theta_{GD} - \theta_{SGD}}^\mu 
                        =
                    T \cdot \frac{N-1}{4 N}
                    \nabla^\mu C^{\nu}_{\nu} + o(\eta^2)
                $.
            \end{cor}
            \noindent
            \cite{ro19} obtained a version of this Corollary with a nearly
            equal error of $O(\eta^2/N)\vee o(\eta^2)$.  The Corollary's proof
            implies that if $\hat{l_c}$ is a
            smooth unbiased estimator of $\frac{N-1}{4 N}
            C^{\nu}_{\nu}$, then GD on $l + \hat{l_c}$ has an expected testing
            loss that agrees with SGD's to order $\eta^2$.  We call this method
            \textbf{GDC}.

            An analogous form of averaging occurs over multiple epochs.  For a
            tight comparison, we scale the learning rates appropriately so
            that, to leading order, few-epoch and many-epoch SGD agree.  Then
            few-epoch and many-epoch SGD differ, to leading order, in their
            sensitivity to $\nabla C$:
            \begin{cor}[$\sdia{c(01-2)(01-12)}$] \label{cor:epochs}
                SGD with $E=B=1$, $\eta=\eta_0$ avoids high-$C$ regions more
                than SGD with $E=E_0$, $B=1$, $\eta=\eta_0/E_0$.  Precisely:
                $
                    \expct{\theta_{E=E_0} - \theta_{E=1}}^\mu
                        =
                    \wrap{\frac{E_0-1}{4 E_0}} N
                    \wrap{\nabla^\mu C^{\nu}_{\nu}}
                    + o(\eta^2)
                $.
            \end{cor}

            In sum, high-$C$ regions repel small-$(E,B)$ SGD more than
            large-$(E,B)$ SGD.  We thus extend the $T=2$ result of \cite{ro18}
            and resolve some questions posed therein.    

        \subsection{SGD and SDE respond differently to changes in curvature}
            %Non-Gaussian noise affects 
    
            Ordinary and stochastic differential equations (ODE and SDE: see
            \cite{li18}) are a popular models of SGD, but
            they differ from SGD in several ways.  For instance, the inter-epoch
            noise correlations in multi-epoch SGD measurably affect SGD's final
            testing loss (Corollary \ref{cor:epochs}), but SDE assumes
            uncorrelated gradient updates.  Even if we restrict to single-epoch
            SGD, time discretization and non-Gaussian noise lead SGD and SDE to
            respond differently to changes in curvature.  The following treats
            SGD with $E=B=1$.   
                        %
            \begin{cor}[$\sdia{c(01-2)(02-12)}$, $\sdia{c(012-3)(03-13-23)}$] \label{cor:vsode}
                For fixed $T$, SGD's final testing loss exceeds both ODE's and
                SDE's by
                $
                    \frac{T}{2} C_{\mu\nu} H^{\mu\nu} + o(\eta^2)
                $.  The skewness of gradient
                noise contributes (we work in an eigenbasis of $\eta H$): 
                \begin{align*}
                    -\frac{\eta^3}{3!}
                    \sum_{\mu\nu\lambda}
                        S_{\mu\nu\lambda}
                        \frac{
                            1 - \exp(-T\eta (H_{\mu\mu} + H_{\nu\nu} + H_{\lambda\lambda}))
                        }{
                            \eta (H_{\mu\mu} + H_{\nu\nu} + H_{\lambda\lambda})
                        }
                        J_{\mu\nu\lambda}
                        + o(\eta^3)
                \end{align*}
                to the excess final testing loss over SDE.  This 
                expression specializes to Prop \ref{prop:splash}.
            \end{cor}

        \subsection{SGD descends on a landscape smoothed by the current $C$ {\rm --- See \S\ref{sect:entropic-curl}.}}

        \subsection{Both flat and sharp minima overfit less}
            \label{subsect:curvature-and-overfitting}%


            \begin{wrapfigure}{r}{0.3\textwidth}
                \centering
                \crunch\squash
                \plotmoow{colt/spring}{0.3\textwidth}{}
                \caption{%
                    Both curvature and noise structure affect overfitting.
                    \small
                    In each pane, the  $\leftrightarrow$
                    axis represents weight space and the $\updownarrow$
                    axis represents loss.  Noise (blue) transforms
                    the testing loss (thin curve) into the observed loss
                    (thick curve).  Red dots mark the testing loss at the
                    arg-min of the observed loss.  \protect\offive{1}:
                    \emph{covector}-perturbed landscapes favor large $H$s.
                    \protect\offive{2}: \emph{vector}-perturbed landscapes
                    favor small $H$s.  SGD's implicit regularization
                    interpolates between these rows (Corollary
                    \ref{cor:overfit}).
                }
                \label{fig:spring}
            \end{wrapfigure}
            Intuitively, sharp minima are robust to slight changes in the
            average \emph{gradient} and flat minima are robust to slight
            \emph{displacements} in weight space (Figure
            \ref{fig:spring}\protect\offive{12}).  However, as SGD by
            definition equates displacements with gradients, it may be unclear
            how to reason about overfitting in the presence of curvature.
            %
            Our theory, by accounting for the implicit
            regularization of fixed-$T$ descent, shows that both effects play
            a role.  In fact, by routine calculus on the left hand side of
            Corollary \ref{cor:overfit}, overfitting is maximized for medium
            minima with curvature $H \sim (\eta T)^{-1}$.
            %
            \begin{cor}[from $\sdia{c(01-2)(02-12)}$, $\sdia{c(01)(01)}$]\label{cor:overfit}
                Initialize GD at a non-degenerate test minimum $\theta_\star$.
                The overfitting (testing loss minus $l(\theta_\star)$) and generalization
                gap (testing minus training loss) due to training are:
                $$
                    \wrap{\frac{C/N}{2H}}_{\mu\nu}^{\rho\lambda} ~
                        \wrap{(I - \exp(-\eta T H))^{\otimes 2}}^{\mu\nu}_{\rho\lambda}
                        + o(\eta^2)
                $$
                and
                $$
                    \wrap{\frac{C/N}{H}}_{\mu\nu}^{\mu\lambda} ~
                        \wrap{I - \exp(-\eta T H)}^{\nu}_{\lambda}
                        + o(\eta)
                $$
            \end{cor}
            The generalization gap tends  
            to $C_{\mu\nu}(H^{-1})^{\mu\nu}/N$ as $T\to\infty$.  For maximum
            likelihood (ML) estimation in well-specified models near the ``true''
            minimum, $C=H$ is the Fisher metric, so we recover the AIC:
            $(\textnormal{model dimension})/N$.  Unlike AIC, our more general
            expression is descendably smooth, may be used with MAP or ELBO tasks
            instead of just ML, and does not assume a well-specified model.
    
   
    \section{Experiments}
        We have no theoretical bounds for the domain and rate of convergence in
        Theorems \ref{thm:resum} and \ref{thm:converge}. 
        %
        We thus test our theory by experiment.  We perceive support for our
        theory in drastic rejections of the null hypothesis.  For instance, in
        Figure \ref{fig:vanilla}\ofsix{4}, \cite{ch18} predict a velocity of
        $0$ while we predict a velocity of $\eta^2/6$.
        %
        Here, \texttt{I} bars, \texttt{+} signs, and shaded regions all mark
        $95\%$ confidence intervals based on the standard error of the mean.
        \S\ref{appendix:experiments} describes neural architectures, 
        artificial landscapes, sample sizes, and further plots.

        \subsection{Training time, epochs, and batch size; $C$ repels SGD more
        than GD}
            %-----  vanilla sgd  ---------------------------------------------- 
            We test Theorem \ref{thm:resum}'s third-order truncation on
            smooth convnets for CIFAR-10 and Fashion-MNIST.  Theory agrees with
            experiment through timescales long enough for accuracy to increase
            by $0.5\%$ (Figure \ref{fig:vanilla}\ofsix{0},\ofsix{1}).
            %-----  epochs and overfitting  -----------------------------------
            \S\ref{appendix:figures} supports Corollary \ref{cor:epochs}'s
            predictions about epoch number.
            %-----  emulating small batches with large ones  ------------------
            Figure \ref{fig:vanilla}\ofsix{2} tests Corollary \ref{cor:batch}'s
            claim that, relative to GD, high-$C$ regions \emph{repel} SGD.
            This is significant because $C$ controls the rate at which the
            generalization gap (testing minus training loss) grows (Corollary
            \ref{cor:overfit}, Figure \ref{fig:vanilla}\ofsix{3}).
            %%%%\pmoo{3.5cm}{multi-fashion-logistic-0}
            %%%%\pmoo{3.5cm}{vs-sde}
            %%%%\pmoo{3.5cm}{tak-reg}
            %%%%\caption{
            %%%%    \textbf{Further experimental results}.
            %%%%    %

            \begin{figure}[h!] 
                \centering
                \pmoo{3.85cm}{neurips-test-small} \hfill \pmoo{3.85cm}{new-big-bm-new}          \hfill \pmoo{3.85cm}{multi-fashion-logistic-0}
                \pmoo{3.85cm}{neurips-test-large} \hfill \pmoo{3.85cm}{neurips-gen-cifar-lenet} \hfill \pmoo{3.85cm}{neurips-tak}
                \caption{
                    {\bf Experiments on natural and artificial landscapes.}
                    \texttt{rvalue} refers to Theorem \ref{thm:resum}'s
                    predictions, approximated as in Remark \ref{rmk:integrate}.
                    %\texttt{uvalue} indicates polynomial approximations
                    %to Theorem \ref{thm:resum}'s result (see
                    %\S\ref{appendix:sum-embeddings}).
                    \texttt{uvalue}s are
                    simpler but (see\protect\ofsix{4})
                    less accurate.
                    %%%
                    %%%
                    \newline
                    {\bf Left: Perturbation models SGD for small $\eta
                    T$.} Fashion-MNIST convnet's testing loss vs learning rate.
                    In this small $T$ setting, we choose to use our theory's
                    simpler un-resummed values
                    (\ref{appendix:evaluate-embeddings}) instead of the more
                    precise $\rvalue$s.
                    %
                    \protect\ofsix{0}: For all initializations tested ($1$
                    shown, $11$ unshown), the order $3$ prediction agrees with
                    experiment through $\eta T \approx 10^0$, corresponding to
                    a decrease in $0\mbox{-}1$ error of $\approx 10^{-3}$.
                    %
                    \protect\ofsix{1}: For large $\eta T$, our predictions
                    break down.  Here, the order $3$ prediction holds until the
                    $0\mbox{-}1$ error improves by $5\cdot 10^{-3}$.  Beyond
                    this, $2$nd order agreement with experiment is
                    coincidental.  
                    %%%
                    %%%
                    \newline
                    {\bf Center: $C$ controls generalization gap.}%.and distinguishes GD from SGD.}
                    %
                    With equal-scaled axes, \protect\ofsix{2} shows that GDC
                    matches SGD (small vertical variance) better than GD
                    matches SGD (large horizontal variance) in testing loss for
                    a range of $\eta$ ($\approx 10^{-3}-10^{-1}$) and initializations
                    (zero and several Xavier-Glorot trials) for logistic
                    regression and convnets.  Here, $T=10$. 
                    %
                    \protect\ofsix{3}: CIFAR-10 generalization gaps.  For all
                    initializations tested ($1$ shown, $11$ unshown), the degree-$2$
                    prediction agrees with experiment through $\eta T \approx
                    5\cdot 10^{-1}$.
                    %%%
                    %%%
                    \newline
                    {\bf Right: Predictions near minima excel for
                    large $\eta T$.} \protect\ofsix{4}: 
                    SGD with $2, 3, 5, 8$ epochs incurs greater test
                    loss on Fashiion-MNIST than one-epoch SGD (difference shown in I bars) by the
                    predicted amounts (predictions shaded) for a range of learning
                    rates.  Here, all SGD runs have $N=10$; we scale the learning
                    rate for $E$-epoch SGD by $1/E$ to isolate the effect of
                    inter-epoch correlations away from the effect of larger $\eta
                    T$.
                    %
                    \protect\ofsix{5}: For \MeanEstimation\, with fixed $C$ and
                    a range of $H$s, initialized at the truth, the testing
                    losses after fixed-$T$ GD are smallest for very sharp and
                    very flat $H$.  Near $H=0$, our predictions improve on AIC,TIC
                    \citep{di18}.
                    %%%
                    %%%
                }
                \label{fig:vanilla}
            \end{figure}
        %----------------------------------------------------------------------
        %           Thermodynamic Engine                        
        %----------------------------------------------------------------------
            \newpage
        %----------------------------------------------------------------------
        %           Sharp vs Flat Minima                        
        %----------------------------------------------------------------------
        \subsection{Sharp and flat minima both overfit less than medium minima} \label{subsect:overfit}
    
            Prior work (\S\ref{sect:related}) finds both that SGD leads to
            overfits less near \emph{sharp} minima (for, $l^2$ regularization
            sharpens minima) or that SGD overfits less near \emph{flat} minima
            (for, flat minima are robust to small displacements).  In fact,
            both phenomena occur, and noise structure determines which
            dominates (Corollary \ref{cor:overfit}).  This effect appears even
            in \MeanEstimation\, (\S\ref{appendix:artificial}): Figure
            \ref{fig:vanilla}\ofsix{5}.
            %
            \S\ref{appendix:figures} also presents preliminary results
            suggesting that Corollary \ref{cor:overfit}'s generalization gap
            estimate may serve as a regularizer term enabling descent-based
            tuning of hyperparameters such as a model's $l_2$ coefficients.
            
    \section{Conclusion}

        %\subsection{Contributions}

            %-----  summarize contributions  ----------------------------------

            This paper presents a new physics-inspired perspective on SGD.  We
            use diagrams to study stochastic optimization on short timescales
            or near minima.  Corollaries \ref{cor:entropic} and
            \ref{cor:overfit} together show that SGD avoids curvature and
            noise, which to leading order control generalization.

            Analyzing $\sdia{c(01-2)(02-12)}$, we proved that \textbf{flat and
            sharp minima both overfit less} than medium minima.  Intuitively, flat
            minima are robust to vector noise, sharp minima are robust to covector
            noise, and medium minima robust to neither.  We thus proposed a
            regularizer enabling gradient-based hyperparameter tuning.
            %
            Inspecting $\sdia{c(01-2-3)(02-12-23)}$, we extended \cite{we19b} to
            nonconstant, non-isotropic covariance to reveal that \textbf{SGD
            descends on a landscape smoothed by the current covariance $C$}.  As
            $C$ evolves, the smoothed landscape evolves, resulting in
            non-conservative dynamics.
            %
            %Corollaries \ref{cor:entropic} and
            %\ref{cor:overfit} together potentially illuminate SGD's success in
            %training deep networks: SGD avoids curvature and noise, which
            %control generalization.
            %
            Examining $\sdia{c(01-2)(01-12)}$, we showed that \textbf{GD may
            emulate SGD}, as suggested by \cite{ro18}.  This is significant
            because, while small batch sizes can lead to better generalization
            \citep{bo91}, modern infrastructure increasingly rewards large
            batch sizes \citep{go18}.  

            %-----  anticipate criticism of limitations  ----------------------
    
            Since our predictions depend only on loss data near initialization,
            they break down after the weight moves far from initialization.  Our
            theory thus best applies to small-movement contexts, whether for long
            times (large $\eta T$) near an isolated minimum or for short times
            (small $\eta T$) in general.
            %
            Thus, the theory might aid future analysis of fine-tuners such as 
            \cite{fi17}'s MAML.
    
                Much as meteorologists understand how warm and cold fronts
                interact despite long-term forecasting's intractability, we
                quantify how curvature and noise contribute to
                counter-intuitive dynamics governing each short-term interval
                of SGD's trajectory.  Equipped with our theory, users of deep
                learning may refine intuitions --- e.g.\ that SGD descends on
                the training loss --- to account for noise.

        \subsection{Related work}\label{sect:related}
    
            %--  history of sgd  ----------------------------------------------

            \cite{ki52} united gradient descent \citep{ca47} with stochastic
            approximation \citep{ro51} to invent SGD.  Since the development of
            back-propagation for efficient differentiation \citep{we74}, SGD
            has been used to train connectionist models, e.g.\ neural networks
            \citep{bo91}, recently to remarkable success \citep{le15}.
        
            %--  analyzing overfitting; relevance of optimization; sde errs  --
        
            Several lines of work treat the overfitting of SGD-trained networks
            \citep{ne17a}.  For example, \cite{ba17} controls the Rademacher
            complexity of deep hypothesis classes, leading to
            optimizer-agnostic generalization bounds.  Yet SGD-trained networks
            generalize despite their ability to shatter large sets
            \citep{zh17}, so generalization must arise from not only
            architecture but also optimization \citep{ne17b}.  

            Some analyses of optimization's implicit regularization use a
            Langevin dynamics or SDE approximation (e.g.\ \cite{ch18,zh19}),
            but, per \cite{ya19a}, such continuous-time or uncorrelated-noise
            analyses treat SGD noise incorrectly.
            %
            %--  we extend dan's approach  ------------------------------------
            %
            We avoid these pitfalls by Taylor expanding around $\eta=0$ as in
            \cite{ro18}.  Unlike that work, we generalize beyond order $\eta^1$
            and $T=2$.  To do so, we develop new summation techniques with
            improved large-$T$ convergence.  Our interpretion of the resulting
            terms offers a new qualitative picture of SGD as a superposition of
            simpler information-flow processes.
            
            %--  double descent  ----------------------------------------------

            Other research focuses on \emph{double descent} and suggests that
            some highly overparameterized models share implicit regularization
            properties with linear least-squares models \citep{be19}, for
            example by bounding log-determinants (and hence the effective
            dimensions) of feature matrices and weight spaces
            \citep{me20}.\footnote{
                \cite{me20}'s eq.\ 75 bounds a log-determinant defined in eq.\
                61 of a transformed feature matrix.  Compare to linear
                Representer Theorems \citep{mo18b}.
            }
            %
            Our work reveals new dynamics toward and within valleys of minima,
            dynamics that may also reduce the effective dimension of model space.
            However, our focus on the structure of gradient noise may be
            overspecific, since recent work finds that GD and SGD may both
            converge to the same global minima \citep{zo20} or that
            noise covariance but not higher moments are relevant to
            regularization \citep{wu20}. 

            %--  phenomenology of rademacher correlates such as hessians  -----
        
            Our predictions are vacuous for large $\eta$.  Other work treats
            large-$\eta$ learning phenomenologically, whether by finding
            empirical correlates of the generalization gap \citep{li18}, by
            showing that \emph{flat} minima generalize \citep{ho17,ke17,wa18},
            or by showing that \emph{sharp} minima generalize
            \citep{st56,di17,wu18}.  SGD's implicit regularization mediates
            between these seemingly clashing intuitions (\S
            \ref{subsect:overfit}).
            
            %--  our work vs other perturbative approaches  -------------------
        
            Prior work analyzes SGD perturbatively: \cite{dy19} perturb in
            inverse network width, using 't Hooft diagrams to correct the
            Gaussian Process approximation for specific nets.  Perturbing
            to order $\eta^2$, \cite{ch18} and \cite{li17} assume uncorrelated
            Gaussian noise.  By contrast, we use Penrose diagrams \cite{pe71} to compute
            testing losses to arbitrary order in $\eta$.  We allow correlated,
            non-Gaussian noise and thus \emph{any} smooth architecture.  E.g.\
            we assume no information-geometric relationships between $C$ and
            $H$,\footnote{
                Disagreement of $C$ and
                $H$ is typical in modern learning \citep{ro12, ku19}
            } so we may model VAEs. 
            \nopagebreak
            %\S\ref{sect:entropic-example}'s remainder states results but not
            %the diagrams that led to them.  So we won't relate diagrams to the
            %summands (i.e., the `un-resummed values') they represent; we'll use
            %none of
            %these examples:
            %\nopagebreak
            %    \newcommand{\AAA}{{\color{black}\nabla_\mu}}
            %    \newcommand{\BBB}{{\color{black}\nabla_\nu}}
            %    \newcommand{\CCC}{{\color{black}\nabla_\xi}}
            %    \newcommand{\DDD}{{\color{black}\nabla_\omicron}}
            %    \newcommand{\EEE}{{\color{black}\nabla_\pi}}
            %    \newcommand{\FFF}{{\color{black}\nabla_\rho}}
            %    \newcommand{\ww}[1]{\CC\wasq{#1}}
            %    \newcommand{\rRr}[1]{{\color{moor}#1}}
            %    \newcommand{\gGg}[1]{{\color{moog}#1}}
            %    \newcommand{\bBb}[1]{{\color{moob}#1}}
            %    \newcommand{\sixsum}{\textstyle\sum_{{\mu\nu\xi\omicron\pi\rho}} }
            %\begin{figure}[H]
            %    \centering  
            %    \crunch
            %    \begin{tabular}{rcl}
            %        $\text{uvalue}(\mdia{c(01-2-3)(02-13-23)})$ &
            %        $=$ &
            %        $\sixsum \eta^{\mu\xi} \eta^{\nu\pi} \eta^{\omicron\rho} \,\, \ww{\rRr{\AAA l_x} \rRr{\BBB l_x}} \ww{\gGg{\CCC \DDD      l_x}} \ww{\bBb{\EEE \FFF l_x}}$\\
            %        \crunch\squash\squash
            %        \\
            %        $\text{uvalue}(\mdia{c(01-2-3)(02-12-23)})$ &
            %        $=$ &
            %        $\sixsum \eta^{\mu\xi} \eta^{\nu\omicron} \eta^{\pi\rho} \,\, \ww{\rRr{\AAA l_x} \rRr{\BBB l_x}} \ww{\gGg{\CCC \DDD \EEE l_x}} \ww{\bBb{     \FFF l_x}}$\\
            %        \crunch\squash\squash
            %        \\
            %        $\text{uvalue}(\mdia{c(012-3)(02-12-23)})$ &
            %        $=$ &
            %        $\sixsum \eta^{\mu\xi} \eta^{\nu\omicron} \eta^{\pi\rho} \,\, \ww{\rRr{\AAA l_x} \rRr{\BBB l_x}      \rRr{\CCC \DDD \EEE l_x}} \ww{\gGg{     \FFF l_x}}$
            %    \end{tabular}
            %    \caption{
            %        Illustration of \textbf{Def.\ 
            %        \ref{dfn:uvalue-body}}.
            %    }
            %    \label{fig:uvalue-example}
            %\end{figure}



    \newpage
    \acks{
        We are deeply grateful to Sho Yaida and Dan A.\ Roberts for their
        generous mentorship and to Joshua B.\ Tenenbaum for patiently
        granted autonomy.  Dan introduced us to the SGD literature, taught us
        Taylor series street smarts, and inspired this project.  Sho stoked and
        usefully channeled our interest in physics, galvanized our search for a
        resummation technique, and made time for wide-ranging chats.  We also
        appreciate technical discussions with Greg Wornell, David Schwab, and
        Wenli Zhao as well as writerly advice from Ben R.\ Bray, Chloe and Ryan
        Kleinfeldt-Britton, and Karl Winsor.
        %
        We thank our anonymous reviewers for incisive feedback toward our
        writing's clarity.
    }
    
    \bibliography{perturb}
    
    %APPENDIX
    \appendix

    \newpage
    \section*{Organization of the appendices}
        The following three appendices serve three respective functions:
        \setlist{nolistsep}
        \begin{itemize}[noitemsep]
            \item to explain how to calculate using diagrams;
            \item to prove our results (and pose a conjecture);
            \item to specify our experimental methods and results.
        \end{itemize}
        In more detail, we organize the appendices as follows.\\
    
        {\bf
        \par\noindent A ~ Tutorial: how to use diagrams}                        \hfill {\bf page \pageref{appendix:tutorial}}
        \par\indent     A.1 ~~ An example calculation: the effect of epochs     \hfill \pageref{appendix:example}
        \par\indent     A.2 ~~ How to identify the relevant grid                \hfill \pageref{appendix:draw-spacetime} 
        \par\indent     A.3 ~~ How to identify the relevant diagram embeddings  \hfill \pageref{appendix:draw-embeddings}
        \par\indent     A.4 ~~ How to evaluate each embedding                   \hfill \pageref{appendix:evaluate-embeddings}
        \par\indent     A.5 ~~ How to sum the embeddings' values                \hfill \pageref{appendix:sum-embeddings}
        %\par\indent     A.6 ~~ Interpreting diagrams intuitively                \hfill \pageref{appendix:interpret-diagrams}
        \par\indent     A.6 ~~ How to solve variant problems                    \hfill \pageref{appendix:solve-variants}
        \par\indent     A.7 ~~ Do diagrams streamline computation?              \hfill \pageref{appendix:diagrams-streamline}
    
        {\bf
        \par\noindent B ~ Mathematics of the theory}                            \hfill {\bf page \pageref{appendix:math}}
        \par\indent     B.1 ~~ Setting and assumptions                          \hfill \pageref{appendix:assumptions}
        \par\indent     B.2 ~~ A key lemma \`a la Dyson                         \hfill \pageref{appendix:key-lemma}
        \par\indent     B.3 ~~ From Dyson to diagrams                           \hfill \pageref{appendix:toward-diagrams}
        %\par\indent     B.4 ~~ Interlude: a review of M\"obius inversion       \hfill \pageref{appendix:mobius}
        \par\indent     B.4 ~~ Proof of Theorem \ref{thm:resum}                 \hfill \pageref{appendix:resum}
        \par\indent     B.5 ~~ Proof of Theorem \ref{thm:converge}              \hfill \pageref{appendix:converge}
        \par\indent     B.6 ~~ Proofs of corollaries                            \hfill \pageref{appendix:corollaries}
        \par\indent     B.7 ~~ Future topics                                    \hfill \pageref{appendix:future}
    
        {\bf
        \par\noindent C ~ Experimental methods}                                 \hfill {\bf page \pageref{appendix:experiments}}
        \par\indent     C.1 ~~ What artificial landscapes did we use?           \hfill \pageref{appendix:artificial}  
        \par\indent     C.2 ~~ What image-classification landscapes did we use? \hfill \pageref{appendix:natural}
        \par\indent     C.3 ~~ Measurement process                              \hfill \pageref{appendix:measure}
        \par\indent     C.4 ~~ Implementing optimizers                          \hfill \pageref{appendix:optimizers}
        \par\indent     C.5 ~~ Software frameworks and hardware                 \hfill \pageref{appendix:frameworks}
        \par\indent     C.6 ~~ Unbiased estimators of landscape statistics      \hfill \pageref{appendix:bessel}
        \par\indent     C.7 ~~ Additional figures                               \hfill \pageref{appendix:figures}

        {\bf
        \par\noindent D ~ Review of Tensors}                                    \hfill {\bf page \pageref{appendix:tensors}}
        \par\indent     D.1 ~~ What is a tensor?                                \hfill \pageref{appendix:what-tensor}  
        \par\indent     D.2 ~~ Vectors versus covectors                         \hfill \pageref{appendix:tensor-variance}
        \par\indent     D.3 ~~ Contraction of tensors                           \hfill \pageref{appendix:tensor-contraction}
        \par\indent     D.4 ~~ Linear maps as tensors                           \hfill \pageref{appendix:tensor-hom}

\newpage
\section{Tutorial: how to use diagrams}                \label{appendix:tutorial}
    This paper presents a new technique for calculating the expected learning
    curves of SGD in terms of statistics of the loss landscape near
    initialization.  Here, we explain this technique.
    %New combinatorial objects --- \emph{grids} --- arise as we
    %relax the paper body's assumption that $E=B=1$.  This, too, we will
    %explain.
    There are are {\bf four steps} to computing the expected testing loss, or
    other quantities of interest, after a specific number of gradient updates: 
    \begin{itemize}
        \item {\bf Specify, as a grid}, the batch size, training set
            size, and number of epochs. 
        \item {\bf Draw embeddings}, of diagrams into the
            grid, as needed for the desired precision.
        \item {\bf Evaluate each diagram embedding}, whether exactly
            (via $\rvalue$s)
            or roughly
            (via $\uvalue$s).
        \item {\bf Sum the embeddings' values} to obtain the quantity of
              interest as a function of $\eta$.
    \end{itemize}
    \noindent
    After presenting two example calculations that follow these four steps, we
    detail each step individually.  Though we focus on the computation of
    expected testing losses, we describe how the four steps may give us other
    quantities of interest: variances instead of expectations, training
    statistics instead of testing statistics, or weight displacements instead
    of losses.  

        \subsection{Two example calculations}\label{appendix:example}
            We illustrate the four step procedure above by using it to 
            answer the following two questions.

            %\subsubsection{Leading order effect of gradients}
                Our first example calculation reproduces Prop \ref{prop:nest}.
                In other words, it answers the question:
                \begin{quest}[Leading order effect of gradients]\label{qst:grad}
                    What's the leading order loss decrease
                    $\expc[l(\theta_T)-l(\theta_0)]$?
                    We seek an answer expressed in terms of the landscape
                    statistics at initialization: $G,H,C, \cdots$.  We expect
                    only $G$ to be relevant. 
                \end{quest}

            %\subsubsection{Leading order effect of epochs}
                Our second example is (an illustrative case of)
                Corollary \ref{cor:epochs}.
                \begin{quest}[Leading order effect of epochs]\label{qst:multi}
                    How does multi-epoch SGD differ from single-epoch SGD?
                    Specifically, what is the difference between the final
                    testing losses of the following two versions of SGD?
                    \begin{itemize}
                        \item SGD over $T=M_0 \times N$ time steps, learning rate $\eta_0/M$, and
                            batch size $B=1$
                        \item SGD over $T=N$ time steps, learning rate $\eta_0$, and batch size $B=1$
                    \end{itemize}
                    We seek an answer expressed in terms of the landscape statistics
                    at initialization: $G,H,C, \cdots$.
                \end{quest}
                To make our discussion concrete, we will set $M_0=2$; our analysis 
                generalizes directly to larger $M_0$.

                We scaled the above two versions of SGD deliberately, to create an
                interesting comparison.  Specifically, on a noiseless linear
                landscape $l_x=l \in (\RR^n)^*$, the versions attain equal testing
                losses, namely $l(\theta_0) - T l_\mu \eta^{\mu\nu}$.
                %
                So Question \ref{qst:multi}'s answer will be second-order (or
                higher-order) in $\eta$.



    %MOOO
    \begin{landscape}
        \subsubsection{Computations: Grids}\label{sect:gridss}
            We begin by asking a question about the
            final testing loss of some form of SGD.
            %
            We specify the batch size, training set size, and number of epochs
            of the setting under analysis by drawing an appropriate grid.
            That is, we 
            \par \indent $\bullet$ \textbf{draw an $N\times T$ grid} and
            \par \indent $\bullet$ shade its cells, shading
            the $(n,t)$th cell \textbf{when the $t$th batch includes the $n$th data
            point}.
            \par\noindent
            Thus, each column contains $B$ (batch size) many shaded
            cells and each row contains $E$ (epoch number) many shaded cells.
        %
        \newline
        \par\noindent
        \begin{tabular}{p{0.48\linewidth}p{0.48\linewidth}}
            \textsc{Effect of Gradients (Question
            \ref{qst:grad})}&\textsc{Effect of Epochs (Question
            \ref{qst:multi})}\\ \hline Question \ref{qst:grad} does not specify
            a batch size, epoch number, or training set size and so does not
            specify a grid.  In fact, we wish to answer the Question for any
            choice of those hyperparameters.  E.g.\ we'll answer the
            Question for SGD with hyperparameters $B,E,N=2,4,8$:
            \begin{center}
            \par\noindent\parbox{0.90\linewidth}{
                \begin{center}
                \dmoo{3.00cm}{spacetime-b2-e4-nosh}
                \end{center}
                \par
                    \textbf{A grid for SGD} with batch size $B=2$ run for $E=4$
                    epochs on $N=8$ training points for a total of $T=16$
                    timesteps. 
            }
            \end{center}
          &
            Two grids are relevant to Question \ref{qst:multi}: one for
            multi-epoch sgd and another for single-epoch SGD.  See below.
            \newline
            \begin{center}
            \par\noindent\parbox{0.90\linewidth}{
                \dmoo{3.00cm}{spacetime-b1-e2-nosh}
                \hfill
                \dmoo{3.00cm}{spacetime-b1-e1-nosh}
                \par
                    \textbf{Grids for single-epoch and multi-epoch
                    SGD}. Both grids depict $N=7$ training points
                    and batch size $B=1$.
                    %\newline
                    \textbf{Left}: SGD with $M=2$ update per training
                    sample for a total of $T = MN = 2N$ many updates.
                    %\newline
                    \textbf{Right}: SGD with $M=1$ update per training
                    sample for a total of $T = MN = N$ many updates.
            }
            \end{center}
        \end{tabular}

    \end{landscape}
    \begin{landscape}
        \subsubsection{Computations: Embeddings of diagrams into grids}\label{sect:exampleembed}
        Say we permit order-$d$ errors.  We draw all relevant
        diagrams with $d$ or fewer edges and then characterize the embeddings
        of those diagrams in \S\ref{sect:gridss}'s grid.
        %
        An \emph{embedding} of a diagram $D$ in a grid is an
        assignment of $D$'s non-root nodes to shaded cells $(n,t)$ obeying
        the following criteria:
        \par \indent $\bullet$ \textbf{time-ordering condition}: the times $t$
                               strictly increase along each path from leaf to
                               root; and
        \par \indent $\bullet$ \textbf{correlation condition}: if two nodes are
                               in the same part of $D$'s partition, then they
                               are assigned to the same datapoint $n$.
        \par\noindent
        We draw embeddings by placing nodes in their assigned shaded
        $(n,t)$cells; we draw the root nodes outside the
        grids (at arbitrary positions). 
        %
        %Drawn on each of the two grids are examples of embeddings.
        \newline
        \par\noindent
        \begin{tabular}{p{0.48\linewidth}p{0.48\linewidth}}
            \textsc{Effect of Gradients (Question \ref{qst:grad})}&\textsc{Effect of Epochs (Question \ref{qst:multi})}\\
            \hline
            We seek an order $1$ result and thus consider
            one-edged diagrams; there is only one: 
            $\sdia{c(0-1)(01)}$.  We now describe the embeddings of this diagram: 
            \begin{center}\parbox{0.90\linewidth}{
                \begin{center}
                    \dmoo{3.75cm}{spacetime-b2-e4-nosh-populated}
                \end{center}
                \par
                    \textbf{$\protect\sdia{c(0-1)(01)}$ has one embedding for
                    each shaded cell}.  An embedding of a non-root node at cell
                    $(n,t)$ represents the influence of the datapoint $n$ on
                    the testing loss due to the $t$th update.  To first order
                    (i.e.\ one-edged diagrams), the influences of different
                    timesteps do not interact.  The combinatorics of embeddings
                    is thus straightforward.
            }\end{center}
                        &
            We seek an order $2$ result and thus consider
            two-edged diagrams; there are four: 
            $\sdia{c(0-1-2)(02-12)}$,
            $\sdia{c(01-2)(02-12)}$,
            $\sdia{c(0-1-2)(01-12)}$, and
            $\sdia{c(01-2)(01-12)}$.
            The figure below shows some embeddings of order-$1$ and
            order-$2$ diagrams (i.e. one-edged and two-edged diagrams) into the
            grid relevant to Question \ref{qst:multi}.
            %
            Specifically, from top to bottom in each grid, the five 
                diagrams embedded are
                $\protect\sdia{c(01-2)(01-12)}$ (or $\protect\sdia{c(0-1-2)(01-12)}$), 
                $\protect\sdia{c(0-1)(01)}$,
                $\protect\sdia{c(0-1-2)(01-12)}$, 
                $\protect\sdia{c(0-1-2)(02-12)}$, and 
                $\protect\sdia{c(01-2)(02-12)}$ (or $\protect\sdia{c(0-1-2)(02-12)}$).
            The diagram $\protect\sdia{c(0-1-2)(01-12)}$ may be embedded
            wherever the diagram $\protect\sdia{c(01-2)(01-12)}$ may
            be embedded, but not vice versa.  Likewise for
            $\protect\sdia{c(0-1-2)(02-12)}$
            and
            $\protect\sdia{c(01-2)(02-12)}$.
            %
            \begin{center}\parbox{0.90\linewidth}{
                \dmoo{2.75cm}{spacetime-d}\hfill\dmoo{2.75cm}{spacetime-c}
                \par
                    Here, \textbf{$\protect\sdia{c(01-2)(01-12)}$ embeds
                        into the multi-epoch but not single-epoch grid.} 
                    \textbf{Left}: $\protect\sdia{c(01-2)(01-12)}$
                        embeds into the multi-epoch grid.
                    \textbf{Right}: $\protect\sdia{c(01-2)(01-12)}$ cannot
                        embed into the single-epoch grid:  
                        the correlation condition forces both red nodes into 
                        the same row and thus the same cell; the
                        time-ordering condition forces the red nodes into
                        distinct columns and thus distinct cells.
            }\end{center}
        \end{tabular}
    \end{landscape}

    \begin{landscape}
        \subsubsection{Computations: Evaluating each diagram embedding}\label{sect:example-eval}
            We evaluate \S\ref{sect:exampleembed}'s diagrams.  We choose here to
            compute $\uvalue$s (apt for fixed $T$), not
            $\rvalue$s.  These rules translate diagrams to numbers:
            \par \indent $\bullet$
                \textbf{Node rule}: Replace each degree $d$ node by $\nabla^d
                l_x$.
            \par \indent $\bullet$
                \textbf{Outline rule}: surround the nodes in each part of the
                partition by a ``cumulant bracket''.  If a part contains one
                node $x$, the cumulant bracket is the expectation: $\expc[x]$.
                If the part contains two nodes $x,y$, the cumulant bracket is
                the covariance: $\expc[xy]-\expc[x]\expc[y]$.\footnote{
                    The general pattern is that the cumulant bracket $\CC[\prod_{i\in I} x_i]$
                    of a product indexed by $I$ is (here, $P$ ranges over partitions of $I$ with at least 
                    two parts; $I = \sqcup_{p\in P} p$):
                    $$
                        \CC[\prod_{i\in I} x_i] = \expc[\prod x_i] - \sum_{\text{partition}~P} \prod_{p\in P} \CC[\prod_{i\in p} x_i]
                    $$
                }
            \par \indent $\bullet$
                \textbf{Edge rule}: insert a $\eta^{\mu\nu}$ for each
                edge.  The indices $\mu, \nu$ should match the corresponding
                indices of the two nodes incident to the edge.
        \newline
        \par\noindent
        \begin{tabular}{p{0.48\linewidth}p{0.48\linewidth}}
            \textsc{Effect of Gradients (Question \ref{qst:grad})}&\textsc{Effect of Epochs (Question \ref{qst:multi})}\\
            \hline
            In \S\ref{sect:exampleembed} we determined the embeddings of
            $\sdia{c(0-1)(01)}$.  Now we evaluate $\uvalue(\sdia{c(0-1)(01)})$. 
            The node rule suggests that we begin with
            $$
                \nabla_\mu l_x \nabla_\nu l_x
            $$
            We have two factors, each with one derivative because the diagram
            has two nodes, each of degree one.  Note that the number of
            indices (here, two) is the total degree over all nodes and thus also
            twice the number (here, one) of edges.
            The outline rule transforms this to
            $$
                \expct{\nabla_\mu l_x}
                \expct{\nabla_\nu l_x}
                =
                G_\mu
                G_\nu
            $$
            since all parts in the diagram's partition have size one.
            The edge rule inserts a factor $\eta^{\mu\nu}$
            to yield:
            $$
                \uvalue(\sdia{c(0-1)(01)})
                =
                G_\mu
                G_\nu
                \eta^{\mu\nu}
                =
                G_\mu G^\mu
            $$
          &
            In \S\ref{sect:exampleembed} we saw that
            $\sdia{c(0-1-2)(02-12)}$ embeds similarly into multi-epoch
            and single-epoch grids: its multi-epoch
            embeddings correspond by a $M_0^2:1$ map to its single-epoch
            embeddings.  Since we scaled the learning rate of the two SGD
            versions by a factor of $M_0$, and since %$2$-edged diagrams such as
            $\sdia{c(0-1-2)(02-12)}$ (being two-edged) scales as $\eta^2$, \emph{the total
            $\uvalue$ of its multi-epoch embeddings will match the
            total $\uvalue$ of its single-epoch embeddings.}  So 
            we need not compute $\sdia{c(0-1-2)(02-12)}$'s contribution.
            \newline
            We see that this cancellation happens for all of the order-$2$
            diagrams \emph{except} for $\sdia{c(01-2)(01-12)}$.
            %
            Therefore, we must only compute $\uvalue(\sdia{c(01-2)(01-12)})$.
            %The answer to Question \ref{qst:multi} will be (some multiple of)
            %this uvalue.
            \par
            The node rule suggests that we begin with
            $
                \nabla_\mu l_x \nabla_\nu \nabla_\lambda l_x
                \nabla_\rho l_x
            $.
            The outline rule transforms this to
            $$
                \wrap{\expct{\nabla_\mu l_x \nabla_\nu \nabla_\lambda l_x}-\expct{\nabla_\mu l_x}\expct{\nabla_\nu \nabla_\lambda l_x}}
                \expct{\nabla_\rho l_x}
                =
                (\nabla_\nu C_{\mu\lambda} / 2)
                G_\rho
            $$
            The edge rule inserts a factor $\eta^{\mu\lambda} \eta^{\nu\rho}$
            to yield:
            $$
                \uvalue(\sdia{c(01-2)(01-12)})
                =
                (\nabla_\nu C_{\mu\lambda} / 2)
                G_\rho
                \eta^{\mu\lambda}
                \eta^{\nu\rho}
                =
                G^\nu \nabla_\nu C^\mu_\mu / 2
            $$
        \end{tabular}
    \end{landscape}

    \begin{landscape}
        \subsubsection{Computations: Summing the embeddings' values}
        Our Key Lemma('s restatement) says that to compute a testing loss,
        \par\indent$\bullet$ we sum \S\ref{sect:example-eval}'s uvalues, each weighted by the number of ways its diagram
        embeds in the grid,
        \par\indent$\bullet$ where embeddings with $s$ many symmetries count only $1/s$ much
        toward the total number of embeddings.
        \par\noindent
        A symmetry of an embedding $f$
        of a diagram $D$, i.e.\ an element of $\Aut_f(D)$, is defined to be a
        relabeling of $D$'s nodes that simultaneously preserves $D$'s rooted
        tree structure, $D$'s partition structure, and $f$'s assignment of
        nodes to $(n,t)$ cells of the grid.  This is a strong constraint, so
        there will typically be no symmetries except for the identity, meaning
        that $s=1$.  
        % In sum:
        %\par\indent$\bullet$ For each diagram under consideration, compute the sum $\gamma$ of all the $1/|\Aut_f(D)|$ where $f$ ranges over embeddings.
        %\par\indent$\bullet$ Sum the uvalues of all diagrams under consideration, weighted by the aforementioned sums $\gamma$.  This is the answer.
        \newline
        \par\noindent
        \begin{tabular}{p{0.48\linewidth}p{0.48\linewidth}}
            \textsc{Effect of Gradients (Question \ref{qst:grad})}&\textsc{Effect of Epochs (Question \ref{qst:multi})}\\
            \hline
            Referring again to \S\ref{sect:exampleembed}, we see that
            $D=\sdia{c(0-1)(01)}$ has $TB$ many embeddings ($B$ many embeddings
            per column for $T$ many columns).  Since $D$ has no non-trivial
            automorphisms (i.e.\ no non-trivial relabeling of nodes that
            preserves the root, the graph structure, and the equivalence
            relation on non-root nodes), $D$ has no non-trivial automorphisms
            that preserve any given embedding.  Thus $|\Aut_f(D)|=1$ for each
            embedding of $D$.  We conclude that the Restated Key Lemma's
            expression (\ref{eq:sgdcoef})
            \begin{equation*}
                \sum_{\substack{D~\text{a} \\ \text{diagram}}}
                ~
                \sum_{\substack{f~\text{an embed-} \\ \text{-ding of}~D}}
                ~
                \frac{(-B)^{-|\edges(D)|}}{\wabs{\Aut_f(D)}}
                \,
                {\uvalue}(D)
            \end{equation*}
            has as its contribution from $D=\sdia{c(0-1)(01)}$ the value
            \begin{align*}
                (\#\text{of embeddings}~f)
                \cdot
                \frac{(-B)^{-1}}{1}
                \,
                G_\mu G^\mu
                = TB \cdot (- G_\mu G^\mu/B)
            \end{align*}
            Prop \ref{prop:nest}'s expression $-T G_\mu G^\mu$ follows.
          &
            Referring again to \S\ref{sect:exampleembed}, we see that
            $\sdia{c(01-2)(01-12)}$ has ${M_0 \choose 2} \, N$ many embeddings
            into the multi-epoch grid (one embedding per pair of distinct
            epochs, per row) --- and no embeddings into the single-epoch grid.
            Moreover, each embedding of $\sdia{c(01-2)(01-12)}$ has
            $\wabs{\Aut_f(D)}= 1$.  
            %
            We conclude that the testing loss of $M=M_0$ SGD exceeds the
            testing loss of $M=1$ SGD by this much:
            $$
                {M_0 \choose 2} \, N \cdot
                \frac{(-1)^2}{1} \cdot
                (\nabla_\nu C_{\mu\lambda} / 2)
                G^\rho
                \eta^{\mu\lambda}
                \eta^{\nu\rho}
                + o(\eta^2)
            $$
            Since Question \ref{qst:multi} defines $\eta^2 = \eta_0^2/M_0^2$,
            we can rewrite our answer as:
            $$
                l(\theta_{M=M_0,\eta=\eta_0/M_0}) - l(\theta_{M=1,\eta=\eta_0})
                =
                \frac{M_0-1}{4 M_0} N \cdot
                G^\nu (\nabla_\nu C_\mu^\mu)
                + o(\eta_0^2)
            $$
            where we use $\eta_0$ to raise indices.
            This completes the example problem.
        \end{tabular}
    \end{landscape}



    %\subsection{An example calculation: the effect of epochs}       \label{appendix:example}

    %    \begin{quest}\label{qst:multi}
    %        How does multi-epoch SGD differ from single-epoch SGD?
    %        Specifically, what is the difference between the expected
    %        testing losses of the following two versions of SGD?
    %        \begin{itemize}
    %            \item SGD over $T=M_0 \times N$ time steps, learning rate $\eta_0/M$, and
    %                batch size $B=1$
    %            \item SGD over $T=N$ time steps, learning rate $\eta_0$, and batch size $B=1$
    %        \end{itemize}
    %        We seek an answer expressed in terms of the landscape statistics
    %        at initialization: $G,H,C, \cdots$.
    %    \end{quest}
    %    To make our discussion concrete, we will set $M_0=2$; our analysis 
    %    generalizes directly to larger $M_0$.

    %    We scaled the above two versions of SGD deliberately, to create an
    %    interesting comparison.
    %    Specifically, on a noiseless
    %    linear landscape $l_x=l \in (\RR^n)^*$, the versions
    %    attain equal testing losses, namely $l(\theta_0) - T l_\mu \eta^{\mu\nu}$.
    %    %
    %    So Question \ref{qst:multi}'s answer will be second-order (or
    %    higher-order) in $\eta$.

    %    \subsubsection{Grids}
    %        We take an $N\times T$ grid and shade its cells, shading the
    %        $(n,t)$th cell when the $t$th update involves the $n$th data point.
    %        Thus, each column contains $B$ (batch size) many shaded cells and
    %        each row contains $E$ (epoch number) many shaded cells.
    %        This is SGD's \textbf{grid}.
    %        Two grids are relevant to Question \ref{qst:multi}: one for
    %        multi-epoch SGD and another for single-epoch SGD --- see Figure
    %        \ref{fig:spacetimes-epoch}.
    %        \begin{figure}[h!] 
    %            \centering
    %            \dmoo{3.55cm}{spacetime-b1-e2-nosh}
    %            ~~~~~
    %            \dmoo{3.55cm}{spacetime-b1-e1-nosh}
    %            \caption{
    %                \textbf{The grids of single-epoch and of
    %                multi-epoch SGD.}  A cell at row $n$ and column $t$ is
    %                shaded provided that the $n$th training sample inhabits the
    %                $t$th batch.  Both grids depict $N=7$
    %                training points and batch size $B=1$; neither
    %                depicts training-set permutation between epochs.
    %                \newline
    %                \textbf{Left}:
    %                    SGD with $M=2$ update per training sample for a total
    %                    of $T = MN = 2N$ many updates.
    %                \newline
    %                \textbf{Right}:
    %                    SGD with $M=1$ update per training sample for a total
    %                    of $T = MN = N$ many updates.
    %            }
    %            \label{fig:spacetimes-epoch}
    %        \end{figure}

    %    \newpage
    %    \subsubsection{Embeddings of diagrams into a grid}
    %        There are four two-edged diagrams: 
    %        $\sdia{c(0-1-2)(02-12)}$,
    %        $\sdia{c(01-2)(02-12)}$,
    %        $\sdia{c(0-1-2)(01-12)}$, and
    %        $\sdia{c(01-2)(01-12)}$.
    %        %We permit the diagram $\sdia{c(01-2)(02-12)}$, which violates the
    %        %path condition mentioned in \S\ref{sect:calculus}, because we are
    %        %no longer restricting to the special case $E=B=1$.
    %        %
    %        An \emph{embedding} of a diagram $D$ into a grid is an
    %        assignment of $D$'s non-root nodes to shaded cells $(n,t)$ obeying
    %        the following criteria:
    %        \begin{itemize}
    %            \item \textbf{time-ordering condition}: the times $t$ strictly increase 
    %                along each path from leaf to root; and
    %            \item \textbf{correlation condition}: if two nodes are in the same
    %                part of $D$'s partition, then they are assigned to the same
    %                datapoint $n$.
    %        \end{itemize}

    %        We may conveniently draw embeddings by placing nodes in the shaded
    %        cells to which they are assigned.  Figure
    %        \ref{fig:multi-embeddings} shows some embeddings of order-$1$ and
    %        order-$2$ diagrams (i.e. one-edged and two-edged diagrams) into the
    %        grid relevant to Question \ref{qst:multi}.
    %        \begin{figure}[h!] 
    %            \centering
    %            \dmoo{3.55cm}{spacetime-d}
    %            ~~~~~
    %            \dmoo{3.55cm}{spacetime-c}
    %            \caption{
    %                \textbf{The diagram $\protect\sdia{c(01-2)(01-12)}$ embeds
    %                    into multi-epoch but not single-epoch grid.}
    %                Drawn on each of the two grids are examples of embeddings.
    %                The black nodes external to the grids are positioned
    %                arbitrarily. 
    %                From top to bottom in each grid, the five 
    %                    diagrams embedded are
    %                    $\protect\sdia{c(01-2)(01-12)}$ (or $\protect\sdia{c(0-1-2)(01-12)}$), 
    %                    $\protect\sdia{c(0-1)(01)}$,
    %                    $\protect\sdia{c(0-1-2)(01-12)}$, 
    %                    $\protect\sdia{c(0-1-2)(02-12)}$, and 
    %                    $\protect\sdia{c(01-2)(02-12)}$ (or $\protect\sdia{c(0-1-2)(02-12)}$).
    %                The diagram $\protect\sdia{c(0-1-2)(01-12)}$ may be embedded
    %                wherever the diagram $\protect\sdia{c(01-2)(01-12)}$ may
    %                be embedded, but not vice versa.  Likewise for
    %                $\protect\sdia{c(0-1-2)(02-12)}$
    %                and
    %                $\protect\sdia{c(01-2)(02-12)}$.
    %                \textbf{Left}: $\protect\sdia{c(01-2)(01-12)}$
    %                    embeds into multi-epoch grid.
    %                \textbf{Right}: $\protect\sdia{c(01-2)(01-12)}$ cannot
    %                    embed into single-epoch grid.  Indeed, 
    %                    the correlation condition forces both red nodes into 
    %                    the same row and thus the same cell, while the
    %                    time-ordering condition forces the red nodes into
    %                    distinct columns and thus distinct cells.
    %            }
    %            \label{fig:multi-embeddings}
    %        \end{figure}

    %    \subsubsection{Values of the embeddings}

    %        We choose to compute $\uvalue$s instead of $\rvalue$s.  The former
    %        are an approximation of the latter, appropriate when $T$ is fixed
    %        instead of taken to infinity.  $\uvalue$s have the same asymptotic
    %        error as $\rvalue$s with respect to $\eta$.  Moreover, $\uvalue$s
    %        are simpler to calculate, since their numeric values depend only on
    %        diagrams, not on embeddings.  So to compute a testing loss, we
    %        multiply each diagram's $\uvalue$ by the number of ways that
    %        diagram embeds.

    %        Figure \ref{fig:multi-embeddings} shows us that the diagram
    %        $\sdia{c(0-1-2)(02-12)}$ embeds similarly into multi-epoch
    %        and single-epoch spacetimes.  More precisely,
    %        its multi-epoch embeddings correspond by a $M_0^2:1$ map to
    %        its single-epoch embeddings.  Since we scaled the learning rate of
    %        the two SGD versions by a factor of $M_0$, and since $2$-edged
    %        diagrams such as $\sdia{c(0-1-2)(02-12)}$ scale as $\eta^2$, the
    %        total $\uvalue$ of the diagram's multi-epoch embeddings will match
    %        the total $\uvalue$ of the diagram's single-epoch embeddings. 
    %        %
    %        In fact, Figure \ref{fig:multi-embeddings} shows that this
    %        cancellation happens for all of the order-$2$ diagrams
    %        \emph{except} for $\sdia{c(01-2)(01-12)}$.
    %        %
    %        Therefore, to second order, the answer to Question \ref{qst:multi}
    %        will be (some multiple of) $\uvalue(\sdia{c(01-2)(01-12)})$.

    %        To compute $\sdia{c(01-2)(01-12)}$'s value, we follow the rules
    %        in Section \ref{sect:calculus}; the edge rule for $\uvalue$s is
    %        that each edge becomes an $\eta$.
    %        So
    %        $$
    %            \uvalue(\sdia{c(01-2)(01-12)}) =
    %            \expct{\nabla_\mu l_x \nabla_\nu \nabla_\lambda l_x}
    %            \expct{\nabla_\rho l_x}
    %            \eta^{\mu\lambda}
    %            \eta^{\nu\rho}
    %            =
    %            (\nabla_\nu C_{\mu\lambda} / 2)
    %            G^\rho
    %            \eta^{\mu\lambda}
    %            \eta^{\nu\rho}
    %        $$

    %    \subsubsection{Sum of the values}

    %        Referring again to Figure \ref{fig:multi-embeddings}, we see that
    %        $\sdia{c(01-2)(01-12)}$ has ${M_0 \choose 2} \, N$ many embeddings
    %        into the multi-epoch grid (one embedding per pair
    %        of distinct epochs, per row) --- and no embeddings into the 
    %        single-epoch grid.  Moreover, each embedding of $\sdia{c(01-2)(01-12)}$
    %        has $\wabs{\Aut_f(D)}= 1$.  Now we plug into the overall formula
    %        for testing loss: 
    %        \begin{equation*}
    %            \sum_{\substack{D~\text{a} \\ \text{diagram}}}
    %            ~
    %            \sum_{\substack{f~\text{an embed-} \\ \text{-ding of}~D}}
    %            ~
    %            \frac{(-B)^{-|\edges(D)|}}{\wabs{\Aut_f(D)}}
    %            \,
    %            {\uvalue}(D)
    %        \end{equation*}
    %        We conclude that the testing loss of $M=M_0$ SGD exceeds the testing loss
    %        of $M=1$ SGD by this much:
    %        $$
    %            {M_0 \choose 2} \, N \cdot
    %            \frac{(-1)^2}{1} \cdot
    %            (\nabla_\nu C_{\mu\lambda} / 2)
    %            G^\rho
    %            \eta^{\mu\lambda}
    %            \eta^{\nu\rho}
    %            + o(\eta^2)
    %        $$
    %        Since Question \ref{qst:multi} defines $\eta^2 = \eta_0^2/M_0^2$,
    %        we can rewrite our answer as:
    %        $$
    %            l(\theta_{M=M_0,\eta=\eta_0/M_0}) - l(\theta_{M=1,\eta=\eta_0})
    %            =
    %            \frac{M_0-1}{4 M_0} N \cdot
    %            G^\nu (\nabla_\nu C_\mu^\mu)
    %            + o(\eta_0^2)
    %        $$
    %        where we use $\eta_0$ to raise indices.
    %        This completes the example problem.

    %        \begin{rmk*}
    %            An essentially similar
    %            argument proves Corollary \ref{cor:epochs}.
    %            \mend
    %        \end{rmk*}

    \vfill
    \subsection{How to identify the relevant grid}            \label{appendix:draw-spacetime}

        Diagrams tell us about the loss landscape but not about
        SGD's batch size, number of epochs, and training set size
        We encode this SGD data as a set of pairs $(n,t)$, where we have
        one pair for each participation of the $n$th datapoint in the $t$th
        update.  For instance, full-batch GD has $NT$ many pairs, and
        singeleton-batch SGD has $T$ many pairs.  We will draw these
        $(n,t)$ pairs as shaded cells in an $N\times T$ grid; we will call
        the shaded grid the SGD's \textbf{grid}.  See Figure
        \ref{fig:spacetimes}.  

        \begin{figure}[h!] 
            \centering
            %\dmoo{3.55cm}{spacetime-b1-e2-shuf}
            \dmoo{3.55cm}{spacetime-b1-e2-nosh}
            \hfill
            \dmoo{3.55cm}{spacetime-b2-e4-nosh}
            \caption{
                \textbf{The grids of two SGD variants.}
                Shaded cells show $(n,t)$ pairs (see text).
                %\newline
                \textbf{Left}: Two epoch SGD with batch size one.
                    %The training set is permuted between epochs.
                %\newline
                \textbf{Right}: Four epoch SGD with batch size
                    two.  %The training set is not permuted between epochs.
            }
            \label{fig:spacetimes}
        \end{figure}

        \noindent
        \translucent{moolime}{\parbox{\textwidth}{
            \textsc{When using the} diagram method to solve a problem relating
            to SGD with batch size $B$ and $E$ many epochs (over $T$ many time
            steps and on $N$ many training samples), one shades the 
            cells of an $N\times T$ grid with $B$ shaded cells per column and
            $E$ shaded cells per row.
        }}
        \newline
        \par
        \begin{wrapfigure}{r}{5cm}
            \vspace{-0.5cm}
            \dmoo{3.00cm}{spacetime-b1-e2-shuf}
        \end{wrapfigure}
        \textbf{Note}:
        A grid may also depict the inter-epoch permuting of
        training sets due to which the $b$th batch in one epoch differs from
        the $b$th batch in a different epoch.  For instance, see the grid to
        the right.
        %\par
        Since each grid commits to a concrete sequence of training set
        permutations, we may analyze SGD with randomized permutations by taking
        expectations over multiple grids.  However, the corollaries in this
        text are invariant to inter-epoch training set permutations, so we will
        not focus on this point.%\footnote{A routine check shows that for fixed
        %$T$, inter-epoch shuffling yields only an $o(\eta^3)$ effect on testing
        %losses.})

    %\newpage
    \subsection{How to identify the relevant diagram embeddings}    \label{appendix:draw-embeddings}

        We explain below what the words mean in this green summary box.
        \par\noindent
        \translucent{moolime}{\parbox{\textwidth}{
            \textsc{When using the} diagram method to compute SGD's final testing loss
            to order $o(\eta^d)$, we consider all
            diagrams with $d$ or fewer edges and that have a non-zero number of
            embeddings into the relevant grid.
            \par\hspace{0.5cm}
            If we seek the isolated contribution due to a landscape statistic
            (e.g.\ due to $J=\mdia{MOO(0)(0-0-0)}$), we may ignore all diagrams
            that contain that subgraph.  If we are in a setting where a certain
            landscape statistic vanishes, (e.g., at a minimum:
            $G(\theta_0)=\mdia{MOO(0)(0)}$ vanishes), we may neglect all
            diagrams that contain that subgraph.
            \par\hspace{0.5cm}
            If we are using $\rvalue$s (see next section for discussion of
            $\rvalue$s and $\uvalue$s), then we consider only the linkless
            diagrams.  For each diagram, we must enumerate the embeddings,
            i.e.\ the assignments of the diagram's nodes to grid cells that
            obey both the time-ordering condition and correlation condition.
        }}

        A \emph{diagram} is a finite rooted tree equipped with a partition of
        its nodes, such that the root node occupies a part of size $1$.
        %Note
        %that this definition generalizes the special case reported in the paper
        %body; in particular, we no longer require the paper body's ``path
        %condition'' to hold.
        For example, there are four diagrams with two
        edges:
        $\protect\sdia{c(0-1-2)(02-12)}$,
        $\protect\sdia{c(01-2)(02-12)}$,
        $\protect\sdia{c(0-1-2)(01-12)}$, and
        $\protect\sdia{c(01-2)(01-12)}$.
        As always, we specify a diagram's root by drawing it rightmost.

        A diagram is \emph{linkless} when each of its degree-$2$ nodes is in
        a part of size one.  Intuitively, this rules out multi-edge chains
        unadorned by fuzzy ties.
        Thus, only the first diagram in the list 
        $\sdia{c(0-1)(01)}, \sdia{c(0-1-2)(01-12)},
        \sdia{c(0-1-2-3)(01-12-23)}, \cdots$
        is linkless.  Only the first diagram in the list
        $\sdia{c(01-2)(01-12)}, \sdia{c(01-2-3)(01-12-23)}, \cdots$
        is linkless.
        Only the first diagram in the list
        $\sdia{c(0-1-2)(02-12)}, \sdia{c(0-1-2-3)(01-13-23)}, \cdots$
        is linkless.

        An \emph{embedding} of a diagram $D$ into a grid is an
        assignment of $D$'s non-root nodes to shaded cells $(n,t)$ that
        obeys the following two criteria:
        \begin{itemize}
            \item \textbf{time-ordering condition}: the times $t$ strictly increase 
                along each path from leaf to root; and
            \item \textbf{correlation condition}: if two nodes are in the same
                part of $D$'s partition, then they are assigned to the same
                datapoint $n$.
        \end{itemize}
        We may conveniently draw embeddings by placing nodes in the shaded
        cells to which they are assigned.  Then, the time-ordering condition 
        forbids (among other things) intra-cell edges, and the correlation
        condition demands that fuzzily tied nodes are in the same row.  See
        Figure \ref{fig:embeddings}.
        \begin{figure}[h] 
            \centering  
            \plotmooh{diagrams/spacetime-e}{}{0.26\columnwidth}
            \caption{
                Embeddings, legal and illegal.
                \textbf{Left}: illegal embedding of $\sdia{c(0-1-2)(01-12)}$,
                    since the time-ordering condition is not obeyed. 
                    For the same reason, not a legal embedding of $\sdia{c(01-2)(01-12)}$.
                \textbf{Middle}: an embedding of $\sdia{c(0-1-2)(01-12)}$.
                Also an embedding of $\sdia{c(01-2)(01-12)}$,
                since the correlation condition is obeyed.
                \textbf{Right}: a legal embedding of $\sdia{c(0-1-2)(01-12)}$.
                    Not an embedding of
                    $\sdia{c(01-2)(01-12)}$, since the correlation condition is
                    not obeyed.
            }
            \label{fig:embeddings}
        \end{figure}

        In principle, the relevant diagrams for a calculation with error
        $o(\eta^d)$ are the diagrams with at most $d$ edges.  For $d$ greater
        than $2$, there will be many such diagrams.  However, in practice
        we gain insight even from considering one diagram at a time:
        \begin{rmk*}
            In this paper's corollaries, we seek to extract the specific effect
            of a specific landscape or optimization feature such as skewed
            noise (Prop \ref{prop:splash}) or multiple epochs
            (\S\ref{appendix:example}).  In these cases, it is usually the case
            that most diagrams are irrelevant.  For example, because a diagram
            evaluates to a product of its components, the only way the skewness
            of gradient noise can appear in our calculations is through 
            diagrams such as $\sdia{c(012-3)(03-13-23)}$ that have a part of
            size $3$. 
            %Thus, the analysis in Example \ref{exm:first} was able
            %to ignore diagrams such as $\sdia{c(01-2)(02-12)}$. 
            Likewise, in \S\ref{appendix:example} we argued by considering 
            which embeddings that the only diagram relevant to Question
            \ref{qst:multi} is $\sdia{c(01-2)(01-12)}$.  
            \mend
        \end{rmk*}

        Here are some further examples.  Table \ref{tab:scatthree} shows the
        $6$ diagrams that may embed into the grid of $E=B=1$.  It
        shows each diagram in multiple ways to underscore that diagrams are
        purely topological and to suggest the ways in which these diagrams may
        embed into a grid.
        \begin{table}[h!]
            \centering 
            %\resizebox{\columnwidth}{!}{%
            \begin{tabular}{ccccc}
                {\large $\Theta\left((\eta N)^3 N^{-0}\right)$} &&
                {\large $\Theta\left((\eta N)^3 N^{-1}\right)$} &&
                {\large $\Theta\left((\eta N)^3 N^{-2}\right)$} \\ \hline
                \begin{tabular}{c}
                    \begin{tabular}{ll}
                        $\mdia{c(0-1-2-3)(01-12-23)}$ & $\mdia{c(0-1-2-3)(01-13-23)}$
                    \end{tabular} \\
                    \begin{tabular}{ll}
                        $\mdia{c(0-1-2-3)(02-13-23)}$ & $\mdia{c(0-1-2-3)(03-12-23)}$
                    \end{tabular} \\ \hline
                    \begin{tabular}{ll}
                        $\mdia{c(0-1-2-3)(03-13-23)}$ & $\mdia{c(0-1-2-3)(02-12-23)}$
                    \end{tabular}
                \end{tabular}
                &&
                \begin{tabular}{c}
                    \begin{tabular}{ll}
                        $\mdia{c(01-2-3)(02-13-23)}$ & $\mdia{c(01-2-3)(03-12-23)}$
                    \end{tabular} \\ \hline
                    \begin{tabular}{ll}
                        $\mdia{c(0-12-3)(01-13-23)}$ & $\mdia{c(0-12-3)(02-13-23)}$
                    \end{tabular} \\ \hline
                    \begin{tabular}{lll}
                        $\mdia{c(01-2-3)(03-13-23)}$ & $\mdia{c(0-12-3)(03-13-23)}$ & $\mdia{c(01-2-3)(02-12-23)}$ 
                    \end{tabular}
                \end{tabular}
                &&
                \begin{tabular}{c}
                    \begin{tabular}{l}
                        $\mdia{c(012-3)(03-13-23)}$
                    \end{tabular}
                \end{tabular}
            \end{tabular}
            %}
            \caption{
                \textbf{Multiple ways to draw the $6$ distinct degree-$3$
                diagrams for $B=E=1$ SGD's testing loss.}
                Because the grid of $B=E=1$ SGD has only one cell per row and
                one cell per column, the only diagrams that have a non-zero
                number of embeddings are the diagrams such that each
                ancestor-descendant pair in the rooted tree occupie two
                different parts of the partition.
                We show $(4+2)+(2+2+3)+(1)$ ways to draw the $6$ diagrams.
                In fact, these drawings show all of the time-orderings of the
                diagrams' nodes that are consistent with the time-ordering
                condition.
                %
                \textbf{Organization}:
                We organize the diagrams into columns by the number
                of parts in their partitions.  Because partitions (fuzzy
                outlines) indicate correlations between nodes (i.e. noise), 
                diagrams with fuzzy outlines show deviations of SGD away from
                deterministic ODE.  The big-$\Theta$ notation that heads the
                columns gives the asymptotics of the sum-over-embeddings of 
                each diagram's $\uvalue$s 
                (for $N$ large and $\eta$ small even relative to $1/N$).
                {\bf Left}: Diagrams for ODE behavior.
                {\bf Center}: $1$st order deviation of SGD
                away from ODE.
                {\bf Right}: $2$nd order deviation of SGD
                from ODE with appearance of non-Gaussian statistics.
            }
            \label{tab:scatthree}
        \end{table}
   
    \subsection{How to evaluate each embedding}                     \label{appendix:evaluate-embeddings}
        We will discuss how to compute both $\rvalue$s and $\uvalue$s.  Both
        are ways of turning a diagram embedding into a number.  The paper body
        mainly mentions $\rvalue$s.  $\uvalue$s are simpler to calculate, since
        they depend only on a diagram's topology, not on the way it is
        embedded.  $\rvalue$s are more accurate; in particular, when we
        initialize near a non-degenerate local minimum, $\rvalue$s do not
        diverge to $\pm \infty$ as $T\to\infty$.

        We will explain the following green summary box.
        \par
        \noindent
        \translucent{moolime}{\parbox{\textwidth}{
            \textsc{Turn an embedding of a diagram} into its uvalue or rvalue
            by applying the following rules. 
            \par \indent $\bullet$
                \textbf{Node rule}: Replace each degree $d$ node by $\nabla^d
                l_x$.
            \par \indent $\bullet$
                \textbf{Outline rule}: surround the nodes in each part of the
                partition by a ``cumulant bracket''.  If a part contains one
                node $x$, the cumulant bracket is the expectation: $\expc[x]$.
                If the part contains two nodes $x,y$, the cumulant bracket is
                the covariance: $\expc[xy]-\expc[x]\expc[y]$.\footnote{
                    The general pattern is that the cumulant bracket $\CC[\prod_{i\in I} x_i]$
                    of a product indexed by $I$ is (here, $P$ ranges over partitions of $I$ with at least 
                    two parts; $I = \sqcup_{p\in P} p$):
                    $$
                        \CC[\prod_{i\in I} x_i] = \expc[\prod x_i] - \sum_{\text{partition}~P} \prod_{p\in P} \CC[\prod_{i\in p} x_i]
                    $$
                }
            \par \indent $\bullet$  If we wish to compute a uvalue, then we apply:
                \textbf{Edge rule for uvalues}: insert a $\eta^{\mu\nu}$ for each
                edge.  The indices $\mu, \nu$ should match the corresponding
                indices of the two nodes incident to the edge.
            \par \indent $\bullet$ If we wish to compute an rvalue, then we apply
                \textbf{Edge rule for rvalues}: if an edge's endpoints are embedded to times
                $t, t^\prime$, insert a factor of $K^{\wabs{t^\prime-t}-1} \eta$,
                where $K \triangleq (I-\eta H)$.  Here, we consider the root node
                as embedded to the time $T$.
        }}

        
        \subsubsection{Un-resummed values: $\uvalue(D)$}
            Each part in a diagram's partition looks like one of the following
            fragments (or one of the infinitely many analogous fragments):
            \begin{center}
            \begin{tabular}{p{6cm}p{6cm}}
                {\begin{align*}
                    G\triangleq\ex{\nb\lx}              &\triangleq \mdia{MOOc(0)(0)}            \\
                    H\triangleq\ex{\nb\nb\lx}           &\triangleq \mdia{MOOc(0)(0-0)}          \\
                    J\triangleq\ex{\nb\nb\nb\lx}        &\triangleq \mdia{MOOc(0)(0-0-0)}        \\
                    \ex{(\nb\lx - G)(\nb\nb\lx - H)}    &\triangleq \mdia{MOOc(01)(0-1-1)}       \\
                    \ex{(\nb\nb\lx - H)(\nb\nb\lx - H)} &\triangleq \mdia{MOOc(01)(0-0-1-1)}     \\
                    \ex{(\nb\lx - G)(\nb\nb\nb\lx - J)} &\triangleq \mdia{MOOc(01)(0-1-1-1)}
                \end{align*}}
                &
                {\begin{align*}
                    C\triangleq\ex{(\nb\lx - G)^2}      &\triangleq \mdia{MOOc(01)(0-1)}         \\
                    S\triangleq\ex{(\nb\lx - G)^3}      &\triangleq \mdia{MOOc(012)(0-1-2)}      \\ 
                    \ex{(\nb\lx - G)^4} - 3C^2          &\triangleq \mdia{MOOc(0123)(0-1-2-3)}   \\ 
                    \ex{(\nb\lx - G)^5} - 10CS          &\triangleq \mdia{MOOc(01234)(0-1-2-3-4)}
                \end{align*}}
            \end{tabular}
            \end{center}
            The above examples illustrate the
            %
            \textbf{Node rule}: each degree $d$ node evaluates to 
            $\nabla^d l_x$.

            Fuzzy outlines dictate how to collect the $\nabla^d l_x$s into
            expectation brackets.  For example, we could collect the nodes
            within each part (of the partition) into a pair of expectation
            brackets $\expc_x\wasq{\cdot}$ --- call the result the
            \textbf{moment value}.
            However, this would yield (un-centered)
            moments such as $\ex{(\nb\lx)^2}$ instead of cumulants such as
            $C=\ex{(\nb\lx - G)^2}$.
            For technical reasons (\S\ref{appendix:mobius} and
            \S\ref{appendix:resum}), cumulants will be easier to work with than
            moments, so we will choose to define the values of diagrams
            slightly differently as follows.\footnote{
                This is just the standard M\"obius recursion for defining cumulants
                (see \cite{ro64}).
            }
            \par
                \textbf{Outline rule}: surround the nodes in each part of the
                partition by a ``cumulant bracket''.  The cumulant bracket
                $\CC[\prod_{i\in I} x_i]$ of a product indexed by $I$ is (here,
                $P$ ranges over partitions of $I$ with at least two parts; $I =
                \sqcup_{p\in P} p$):
                $$
                    \CC[\prod_{i\in I} x_i] = \expc[\prod x_i] - \sum_{\text{partition}~P} \prod_{p\in P} \CC[\prod_{i\in p} x_i]
                $$
            \par
            Thus, a cumulant bracket of a diagram is the moment bracket of that
            diagram minus other terms.  Those other terms are obtained by considering
            diagrams with the same graph structure but strictly more parts in their
            partition.  The recursive definition
            of $\CC$ grounds out because the maximal number of parts in a
            partition of a finite set is finite.
            %
            %\par
            %\emph{Outline rule}: a partition on nodes evaluates to the
            %difference $X-Y$, where $X$ is the moment-value of the partition
            %and $Y$ is the sum of the values of all strictly finer partitions.
            %\par
            %

            For example, if a part contains one
                node $x$, the cumulant bracket is the expectation: $\expc[x]$.
                If the part contains two nodes $x,y$, the cumulant bracket is
                the covariance: $\expc[xy]-\expc[x]\expc[y]$.
            If a part contains three nodes $x,y,z$, then the cumulant
                    bracket is
                    $$
                        \expc[xyz]-\expc[x]\expc[yz]-\expc[y]\expc[xz]-\expc[z]\expc[yz]+2\expc[x]\expc[y]\expc[z]
                    $$
            We visualize the above  in the following example:
            \begin{exm}
                \emph{
                For example, if we denote moment values by solid
                gray fuzzy ties (instead of fuzzy outlines), then: 
                \begin{align*}
                    \mdia{c(012-3)(01-13-23)}
                        &\triangleq
                    \mdia{(012-3)(01-13-23)}
                        -
                    \mdia{c(01-2-3)(01-13-23)}
                        -
                    \mdia{c(02-1-3)(01-13-23)}
                        -
                    \mdia{c(0-12-3)(01-13-23)}
                        -
                    \mdia{(0-1-2-3)(01-13-23)} \\
                        &\triangleq
                    \mdia{(012-3)(01-13-23)}
                        -
                    \mdia{(01-2-3)(01-13-23)}
                        -
                    \mdia{(02-1-3)(01-13-23)}
                        -
                    \mdia{(0-12-3)(01-13-23)}
                        +
                    2 \mdia{(0-1-2-3)(01-13-23)}
                \end{align*}
                We will use the concept of ``moment values'' again in \S\ref{appendix:mobius}.}
                \mend
            \end{exm}

            Finally, we come to edges. 
            \textbf{Edge rule}: insert a factor of $\eta^{\mu\nu}$ for each
            edge.  The indices $\mu, \nu$ should match the corresponding
            indices of the two nodes incident to the edge.

            \begin{exm}[Un-resummed value] \label{exm:unresum}
                \emph{Remember that
                $
                    \mdia{MOOc(01)(0-1)} = C_{\mu\nu}
                $ and
                $
                    \mdia{MOOc(0)(0-0)} = H_{\lambda\rho}
                $, so that
                $
                    \mdia{MOOc(01)(0-1)}
                    \mdia{MOOc(0)(0-0)}
                    = C_{\mu\nu} H_{\lambda\rho}
                $.
                Then 
                $$
                    \uvalue(\mdia{c(01-2)(02-12)})
                    = C_{\mu\nu} H_{\lambda\rho}
                    \eta^{\mu\lambda}
                    \eta^{\nu\rho}
                $$
                Here, $\mdia{c(01-2)(02-12)}$ has two edges, which correspond
                in this example to the tensor contractions via
                $\eta^{\mu\lambda}$ and via $\eta^{\nu\rho}$, respectively.}
                \mend
            \end{exm}

        \subsubsection{Resummed values: $\rvalue_f(D)$}
            The only difference between $\rvalue$s and $\uvalue$s is in their
            rule for evaluating edges.

            \textbf{Edge rule}: if an edge's endpoints are embedded to times
            $t, t^\prime$, insert a factor of $K^{\wabs{t^\prime-t}-1} \eta$,
            where $K \triangleq (I-\eta H)$.  Here, we consider the root node
            as embedded to the time $T$.

            \begin{exm}[Re-summed value] \label{exm:resum}
                \emph{Recall as in Example \ref{exm:unresum} that 
                $
                    \mdia{MOOc(01)(0-1)} = C_{\mu\nu}
                $ and
                $
                    \mdia{MOOc(0)(0-0)} = H_{\lambda\rho}
                $, so that
                $
                    \mdia{MOOc(01)(0-1)}
                    \mdia{MOOc(0)(0-0)}
                    = C_{\mu\nu} H_{\lambda\rho}
                $.
                Then if $f$ is an embedding of $\mdia{c(01-2)(02-12)}$ that
                sends the diagram's red part to a time $t$ (and its green root
                to $T$), we have:
                $$
                    \rvalue_f(\mdia{c(01-2)(02-12)})
                    = C_{\mu\nu} H_{\lambda\rho}
                    \wrap{K^{T-t-1} \eta}^{\mu\lambda}
                    \wrap{K^{T-t-1} \eta}^{\nu\rho}
                $$
                Here, $\mdia{c(01-2)(02-12)}$ has two edges, which correspond
                in this example to the tensor contractions via
                $\wrap{K^{\cdots}\eta}^{\mu\lambda}$ and via
                $\wrap{K^{\cdots}\eta}^{\nu\rho}$, respectively.}
                \mend
            \end{exm}

        \subsubsection{Overall}
            In sum, we evaluate an embedding of a diagram by using the 
            \textbf{node}, 
            \textbf{outline}, and
            \textbf{edge}
            rules to build an expression of $\nabla^d l_x$s, $\expc_x$s and
            $\eta$s.  The difference between $\uvalue$s and $\rvalue$s lies
            only in their edge rule.

    \subsection{How to sum the embeddings' values}                  \label{appendix:sum-embeddings}

        We give examples of automorphism groups and we illustrate the
        integration mentioned in this green summary box:
        \par
        \noindent
        \translucent{moolime}{\parbox{\textwidth}{
            \textsc{We obtain overall} final testing loss expressions
            by adding together the $\uvalue$s of all diagram embeddings, weighted
            by automorphism-group sizes as in \ref{thm:resum}.
            %
            \newline
            \par
            If we are using rvalues instead of uvalues, 
            we approximate sums over embeddings by
                integrals over $t$, we approximate $(I-\eta H)^t$ by $\exp(- \eta H t)$,
            and we apply:rule
                $$\int_{0\leq u<T} \, du \, \exp(-u A) = (I - \exp(-T A))/A$$ 
            When written in an eigenbasis of $\eta H$, this $A$'s coefficients are
            sums of one or more eigenvalues of $\eta H$ (one eigenvalue for each
            edge involved in the relevant degrees of freedom over which we
            integrate).  %As another example, see Example \ref{exm:first}.
        }}

        The Restated Key Lemma and Theorem \ref{thm:resum} together say %in the paper body generalizes to
        \begin{thm*}
            For any $T$: for $\eta$ small enough, SGD has expected testing loss
            \begin{equation*}
                \sum_{\substack{D~\text{a linkless} \\ \text{diagram}}}
                ~
                \sum_{\substack{f~\text{an embed-} \\ \text{-ding of}~D}}
                ~
                \frac{(-B)^{-|\edges(D)|}}{\wabs{\Aut_f(D)}}
                \,
                {\rvalue_f}(D)
            \end{equation*}
            which is the same as
            \begin{equation*}
                \sum_{\substack{D~\text{a} \\ \text{diagram}}}
                ~
                \sum_{\substack{f~\text{an embed-} \\ \text{-ding of}~D}}
                ~
                \frac{(-B)^{-|\edges(D)|}}{\wabs{\Aut_f(D)}}
                \,
                {\uvalue}(D)
            \end{equation*}
            Here, $B$ is the batch size.
            %We say an embedding is \emph{strict} if it assigns to each part
            %a different datapoint $n$.
        \end{thm*}

        How do we evaluate the above sum?
        Summing $\uvalue$s reduces to counting embeddings, which in all the
        applications reported in this text is a routine combinatorial exercise. 
        However, when summing $\rvalue$s, it is often convenient to replace
        a sum over embeddings by an integral over times, and
        the power $\wrap{I-\eta H}^{\Delta t-1}$ by
        the exponential $\exp{-\Delta t \eta H}$.  This incurs a term-by-term
        $1+o(\eta)$ error factor, meaning that it preserves leading order
        results. 

        \begin{exm}
            \emph{Let us return to $D=\mdia{c(01-2)(02-12)}$, embedded, say, in the
            grid of  one-epoch one-sample-per-batch SGD.
            From Example \ref{exm:resum}, we know that we want to sum the 
            following value over all embeddings $f$, i.e. over all $0\leq t<T$
            to which the red part of the diagram's partition may be assigned:
            $$
                \rvalue_f(\mdia{c(01-2)(02-12)})
                = C_{\mu\nu} 
                \wrap{K^{T-t-1} \eta}^{\mu\lambda}
                \wrap{K^{T-t-1} \eta}^{\nu\rho}
                H_{\lambda\rho}
            $$
            Each embedding has a factor 
                $(-B)^{-|\edges(D)|}/\wabs{\Aut_f(D)} = (-B)^{-2}/2$;
            we will multiply in this factor at the end so we now we focus on
            the $\sum_f$.
            So, using the aforementioned approximation, we seek to evaluate
            \begin{align*}
                \int_{0\leq t<T} \, dt \, 
                    C_{\mu\nu} 
                    \wrap{\exp\wrap{-(T-t)\eta H} \eta}^{\mu\lambda}
                    \wrap{\exp\wrap{-(T-t)\eta H} \eta}^{\nu\rho}
                    H_{\lambda\rho}
                = \\
                C_{\mu\nu} 
                \wrap{
                \int_{0\leq t<T} \, dt \, 
                    \exp\wrap{-(T-t)((\eta H)\otimes I + I \otimes (\eta H))}^{\mu\nu}_{\pi\sigma}
                }
                \eta^{\pi\lambda}
                \eta^{\sigma\rho}
                H_{\lambda\rho}
            \end{align*}
            We know from linear algebra and calculus that
            $\int_{0\leq u<T} \, du \, \exp(-u A) = (I - \exp(-T A))/A$ 
            (when $A$ is a non-singular linear endomorphism).
            Applying this rule for $u=T-t$ and $A=(\eta H)\otimes I + I \otimes
            (\eta H)$, we evaluate the integral as:
            \begin{align*}
                \cdots =
                C_{\mu\nu} 
                \wrap{\frac{I - \exp\wrap{-T ((\eta H)\otimes I + I \otimes (\eta H))}}
                           {(\eta H)\otimes I + I \otimes (\eta H)}
                     }^{\mu\nu}_{\pi\sigma}
                \eta^{\pi\lambda}
                \eta^{\sigma\rho}
                H_{\lambda\rho}
            \end{align*}
            This is perhaps easier to write in an eigenbasis of $\eta H$:
            \begin{align*}
                \cdots = 
                \sum_{\mu\nu}
                C_{\mu\nu} 
                \,
                \frac{1 - \exp\wrap{-T ((\eta H)^\mu_\mu + (\eta H)^\nu_\nu)}}{(\eta H)^\mu_\mu + (\eta H)^\nu_\nu}
                \,
                (\eta H \eta)^{\mu\nu}
            \end{align*}
            Multiplying this expression by the aforementioned $(-B)^{-2}/2$
            gives the contribution of $\mdia{c(01-2)(02-12)}$ to SGD's test
            loss.}
            \mend
        \end{exm}

        An \textbf{automorphism of $D$ that preserves an embedding $f$ of $D$}
        is a relabeling of $D$'s nodes that preserves the root, the graph
        structure, the equivalence relation on non-root nodes, and that at the
        same time respects $f$: $f$ must send a node to the same $(n,t)$ pair
        to which the node's relabeling is sent.  The sizes of automorphism groups appear
        in the denominators of our main results.  They are usually of size one
        (that is, they usually contain only the the identity relabeling).
        \begin{exm}[Automorphisms]
            \emph{
                We take as examples the diagram embeddings of \S\ref{sect:exampleembeds}
                (figure reproduced here).  All the embeddings in the \textbf{left
                figure} have $|\Aut_f(D)|=1$.  For instance, the bottom-most
                embedding (of $\sdia{c(01-2)(01-12)}$) has a non-trivial relabeling  
                (namely, swap the two non-root nodes)
                that obeys all of the automorphism conditions \emph{except}
                the ``respects $f$'' condition.  Indeed, the two non-root nodes are
                assigned to different cells in the grid, so we may not swap them
                without violating the  ``respects $f$'' condition.
            }\par
            \emph{
                By contrast, one of the embeddings (among the valid embeddings)
                shown in the \textbf{right figure} has a non-trivial automorphism
                group.  This is the bottom-most embedding (of
                $\sdia{c(01-2)(01-12)}$, again).  Observe that swapping that
                diagram's two non-root nodes preserves the root, preserves the
                graph structure, and preserves the equivalence relation on non-root
                nodes.  Moreover, such swapping respects $f$, since the two swapped nodes
                embed into the same cell.  Thus, in this case, $|\Aut_f(D)|=2$.
            }\mend
        \end{exm}
            \begin{center}\parbox{0.90\linewidth}{
                \dmoo{3.75cm}{spacetime-d}\hfill\dmoo{3.75cm}{spacetime-c}
                \par
            }\end{center}


    %\subsection{Interpreting diagrams intuitively}                  \label{appendix:interpret-diagrams}

    %    We may intuitively interpret edges as carrying influence from the
    %    training set toward the test measurement.  See Figure
    %    \ref{fig:intuition}.  From this perspective, we may intuitively
    %    interpret edges in an $\rvalue$ calculation as carrying influence from
    %    the training set toward the test measurement.  See Figure
    %    \ref{fig:intuition}.

    %    \begin{figure}[h!] 
    %        \centering  
    %        \plotmooh{diagrams/spacetime-f}{}{0.26\columnwidth}
    %        \caption{
    %            \textbf{Edges carry information}.
    %            Embedding of a $4$-edged diagram.
    %        }
    %        \label{fig:intuition}
    %    \end{figure}


    %    \begin{figure}[h!] 
    %        \centering  
    %        \dmoo{3cm}{spacetime-g}
    %        \dmoo{3cm}{spacetime-h}
    %        \caption{
    %            \textbf{Resummation propagates information, damped by
    %            curvature}.  Each resummed valuerepresents many un-resummed
    %            values, each modulated by the Hessian ($\sdia{MOOc(0)(0-0)}$)
    %            in a different way.
    %            \textbf{Left}: Here is one of many un-resummed terms captured by
    %            a single resummed embedding for $\sdia{c(0-1)(01)}$.
    %            \textbf{Left}: each resummed value represents many un-resummed
    %            values.  Here is one of many un-resummed terms captured by
    %            a single resummed embedding for $\sdia{c(01-2)(02-12)}$.
    %        }
    %        \label{fig:intuition}
    %    \end{figure}

    %%        \subsubsection{Chladni effect}
    %%(\S\ref{appendix:interpret-diagrams})
    %%            {\color{red} Make sure to also discuss Chladni effect
    %%            \cite{ch87}!!!}

    \subsection{How to solve variant problems}                      \label{appendix:solve-variants}
        In \S\ref{appendix:future}, we briefly discuss second-order methods
        and natural gradient descent.  Here, we briefly discuss modifications.
        We omit proofs, which would closely follow \S\ref{appendix:math}'s
        proof of the expectation-of-test-loss case.

        \subsubsection*{Variance (instead of expectation)}
            To compute variances instead of expectations (with respect to the
            noise in the training set), one considers generalized diagrams   
            that have ``two roots'' instead of one.  More precisely, to
            compute, say, the un-centered second moment of testing loss, one uses
            diagrams whose edge structures are not rooted trees but instead
            forests consisting of two rooted trees.  We require that the set of
            roots (now a set of size two instead of size one) is a part of the
            diagram's partition.  We draw the two roots rightmost. 
            %
            For example, the generalized diagrams $\mdia{MOOc(01)(01)}$ or
            $\mdia{MOOc(01-23)(02-13)}$ may appear in this computation.

        \subsubsection*{Measuring on the training (instead of test) set}

            To compute the training loss, we compute with all the same
            diagrams as the testing loss, and we also allow all the additional
            generalized diagrams that violate the constraint that a diagram's
            root should be in a part of size one.
            %
            Therefore, to compute the generalization gap (i.e.\ testing loss minus
            training loss), we sum over all the diagrams that expressly 
            violate this constraint (and then, since gen.\ gp is test minus
            train instead of train minus test, we multiply the whole answer
            by $-1$).
            %
            For example, the generalized diagrams $\mdia{MOOc(01)(01)}$ or
            $\mdia{MOOc(0-123)(02-12-23)}$ may appear in this computation.

        \subsubsection*{Weight displacement (instead of loss)}
            To compute displacements instead of losses, one considers
            generalized diagrams that have a ``loose end'' instead of a root.
            %
            For example, the generalized diagrams $\mdia{MOOc(0)(0)}$ or
            $\mdia{MOOc(01)(01-1)}$ may appear in this computation.

    \subsection{Do diagrams streamline computation?}                \label{appendix:diagrams-streamline}

        Diagram methods from Stueckelberg to Peierls have flourished in physics
        because they enable swift computations and offer immediate intuition
        that would otherwise require laborious algebraic manipulation.  We
        demonstrate how our diagram formalism likewise streamlines analysis of
        descent by comparing direct perturbation\footnote{
            By ``direct perturbation'', we mean direct application of our Key
            Lemma (\S\ref{appendix:key-lemma}).
        }
        to the new formalism on two sample problems.

        Aiming for a conservative comparison of derivation ergonomics, we lean
        toward explicit routine when using diagrams and allow ourselves to use
        clever and lucky simplifications when doing direct perturbation.  For
        example, while solving the first sample problem by direct perturbation,
        we structure the SGD and GD computations so that the coefficients (that
        in both the SGD and GD cases are) called $a(T)$ manifestly agree in
        their first and second moments.  This allows us to save some lines.

        Despite these efforts, the diagram method yields arguments about
        \emph{four times shorter} --- and strikingly more conceptual --- than
        direct perturbation yields.  
        %
        (We make no attempt to compare the re-summed version of our formalism to
        direct perturbation because the algebraic manipulations involved for
        the latter are too complicated to carry out.) 
        %
        These examples specifically suggest that:
        diagrams obviate the need for meticulous index-tracking, from the start
        focus one's attention on non-cancelling terms by making visually
        obvious which terms will eventually cancel, and allow immediate
        exploitation of a setting's special posited structure, for instance
        that we are initialized at a test minimum or that the batch size is
        $1$.  We regard these examples as evidence that diagrams offer a
        practical tool for the theorist.

        We now compare {\translucent{moolime}{Diagram Rules}} vs
        {\translucent{moosky}{Direct Perturbation}}.

        \subsubsection{Effect of batch size}
            We compare the testing losses of pure SGD and pure GD.  Because pure
            SGD and pure GD differ in how samples are correlated, their testing loss
            difference involves a covariance and hence occurs at order $\eta^2$.  

            \subthreesect{Diagram Method}
            \colorlet{shadecolor}{moolime}
            \begin{shaded}
                Since SGD and GD agree on noiseless landscapes, we consider only
                diagrams with fuzzy ties.  Since we are working to second order, we
                consider only two-edged diagrams.  There are only two such
                diagrams, $\sdia{(01-2)(02-12)}$ and $\sdia{(01-2)(01-12)}$.  The
                first diagram, $\sdia{(01-2)(02-12)}$, embeds in GD's space time in
                $N^2$ as many ways as it embeds in SGD's spacetime, due to
                horizontal shifts.  Likewise, there are $N^2$ times as many
                embeddings of $\sdia{(01-2)(02-12)}$ in distinct epochs of GD's
                spacetime as there are in distinct epochs of SGD's spacetime.
                However, each same-epoch embedding of $\sdia{(01-2)(01-12)}$ within
                any one epoch of GD's spacetime corresponds by vertical shifts to
                an embedding of $\sdia{(0-1-2)(01-12)}$ in SGD.  There are
                $MN{N\choose 2}$ many such embeddings in GD's spacetime, so GD's
                testing loss exceeds SGD's by 
                $
                    \frac{MN{N\choose 2}}{N^2}~
                    \sdia{c(01-2)(01-12)}
                $.
                Reading the diagram's value from its graph structure, we
                unpack that expression as:
                $$
                    \eta^2 \frac{M(N-1)}{4} G \nabla C 
                $$
            \end{shaded}

            %\newpage
            \subthreesect{Direct Perturbation} 
            \colorlet{shadecolor}{moosky}
            \begin{shaded}
                We compute the displacement $\theta_T-\theta_0$ to order $\eta^2$ 
                for pure SGD and separately for pure GD.  Expanding
                $
                    \theta_t \in \theta_0 + \eta a(t) + \eta^2 b(t) + o(\eta^2)
                $, we find:
                \begin{align*}
                    \theta_{t+1} &=     \theta_t - \eta \nabla l_{n_t} (\theta_t) \\
                                 &\in       \theta_0
                                        +   \eta a(t) + \eta^2 b(t)
                                        -   \eta (
                                                    \nabla l_{n_t}
                                                +   \eta \nabla^2 l_{n_t} a(t) 
                                            )
                                        +   o(\eta^2) \\
                                 &=     \theta_0
                                    +   \eta (a(t) - \nabla l_{n_t})
                                    +   \eta^2 (b(t) - \nabla^2 l_{n_t} a(t)) 
                                    +   o(\eta^2)
                \end{align*}
                To save space, we write $l_{n_t}$ for $l_{n_t}(\theta_0)$.  It's
                enough to solve the recurrence $a(t+1) = a(t) - \nabla l_{n_t}$ and
                $b(t+1) = b(t) - \nabla^2 l_{n_t} a(t)$.  Since $a(0), b(0)$
                vanish, we have $a(t) =-\sum_{0\leq t<T} \nabla l_{n_t}$ and $b(t)
                = \sum_{0\leq t_0 < t_1 < T} \nabla^2 l_{n_{t_1}} \nabla
                l_{n_{t_0}}$.  We now expand $l$:
                \begin{align*}
                    l(\theta_T) \in    l   &+   (\nabla l) (\eta a(T) + \eta^2 b(T)) \\
                                           &+   \frac{1}{2} (\nabla^2 l) (\eta a(T) + \eta^2 b(T))^2
                                            +   o(\eta^2) \\
                                =      l   &+   \eta ((\nabla l) a(T))
                                            +   \eta^2 ((\nabla l) b(T) + \frac{1}{2} (\nabla^2 l) a(T)^2 )
                                            +   o(\eta^2)
                \end{align*}
                Then $\expct{a(T)} = -MN(\nabla l)$ and, since the $N$ many
                singleton batches in each of $M$ many epochs are pairwise
                independent,
                \begin{align*}
                    \expct{(a(T))^2}
                    ~&=
                    \sum_{0\leq t<T} \sum_{0\leq s<T} \nabla l_{n_t} \nabla l_{n_s} \\
                    ~&= 
                    M^2N(N-1)   \expct{\nabla l}^2 +
                    M^2N        \expct{(\nabla l)^2}
                \end{align*}
                Likewise, 
                \begin{align*}
                    \expct{b(T)}
                    = 
                    ~&\sum_{0\leq t_0 < t_1 < T} \nabla^2 l_{n_{t_1}} \nabla l_{n_{t_0}} \\
                    =
                    ~&\frac{M^2N(N-1)}{2} \expct{\nabla^2 l} \expct{\nabla l} + \\
                    ~&\frac{M(M-1)N}{2}  \expct{(\nabla^2 l) (\nabla l)} 
                \end{align*}
                %
                Similarly, for pure GD, we may demand that $a, b$ obey recurrence
                relations $a(t+1) = a(t) - \sum_n \nabla l_n/N$ and
                $b(t+1) = b(t) - \sum_n \nabla^2 l_n a(t)/N$, meaning that
                $a(t) = -t \sum_n \nabla l_n/N$ and
                $b(t) = {t \choose 2} \sum_{n_0} \sum_{n_1} \nabla^2 l_{n_0} \nabla l_{n_1}/N^2$.
                So $\expct{a(T)} = -MN(\nabla l)$ and
                \begin{align*}
                    \expct{(a(T))^2}
                    ~&=
                    M^2 
                    \sum_{n_0} \sum_{n_1} \nabla l_{n_0} \nabla l_{n_1} \\
                    ~&= 
                    M^2 N(N-1)  \expct{\nabla l}^2 + 
                    M^2 N       \expct{(\nabla l)^2}
                \end{align*}
                and
                \begin{align*}
                    \expct{b(T)}
                    = 
                    ~&{MN \choose 2}\frac{1}{N^2}
                    \sum_{n_0} \sum_{n_1} \nabla^2 l_{n_0} \nabla l_{n_1} \\
                    =
                    ~&\frac{M(MN-1)(N-1)}{2} \expct{\nabla^2 l} \expct{\nabla l} + \\
                    ~&\frac{M(MN-1)}{2}      \expct{(\nabla^2 l) (\nabla l)} 
                \end{align*}
            %    $\cdots$
            %\end{shaded}
            %%\newpage
            %\begin{shaded}
            %   $\cdots$
                We see that the expectations for $a$ and $a^2$ agree
                between pure SGD and pure GD.  So only $b$ contributes.  We
                conclude that pure GD's testing loss exceeds pure SGD's by
                \begin{align*}
                       ~&\eta^2
                        \wrap{\frac{M(MN-1)(N-1)}{2}  - \frac{M^2N(N-1)}{2}}
                        \expct{\nabla^2 l} \expct{\nabla l}^2 \\
                    +   ~&\eta^2 
                        \wrap{\frac{M(MN-1)N}{2} - \frac{M(M-1)N}{2}}
                        \expct{(\nabla^2 l) (\nabla l)} \expct{\nabla l} \\
                    = 
                        ~&\eta^2     \frac{M(N-1)}{2}
                    \expct{\nabla l} \wrap{
                          \expct{(\nabla^2 l) (\nabla l)}
                        - \expct{\nabla^2 l} \expct{\nabla l}
                    }
                \end{align*}
                Since $(\nabla^2 l) (\nabla l) = \nabla((\nabla l)^2)/2$, we can 
                summarize this difference as
                $$
                    \eta^2 \frac{M(N-1)}{4}
                    G \nabla C 
                $$
            \end{shaded}

        \subsubsection{Effect of non-Gaussian noise at a minimum.}
            We consider vanilla SGD initialized at a local minimum of the testing loss.
            One expects $\theta$ to diffuse around that minimum according to
            gradient noise.  We compute the effect on testing loss of non-Gaussian
            diffusion.  Specifically, we compare SGD testing loss on the loss
            landscape to SGD testing loss on a different loss landscape defined as a
            Gaussian process whose every covariance agrees with the original
            landscape's.  We work to order $\eta^3$ because at lower orders,
            the Gaussian landscapes will by construction match their non-Gaussian
            counterparts.

            \subthreesect{Diagram Method}
            \colorlet{shadecolor}{moolime}
            \begin{shaded}
                Because $\expct{\nabla l}$ vanishes at initialization, all diagrams
                with a degree-one vertex that is a singleton vanish.  Because we
                work at order $\eta^3$, we consider $3$-edged diagrams.  Finally,
                because all first and second moments match between the two
                landscapes, we consider only diagrams with at least one partition
                of size at least $3$.  The only such test diagram is
                $\sdia{c(012-3)(03-13-23)}$.  This embeds in $T$ ways (one for each
                spacetime cell of vanilla SGD) and has symmetry factor $1/3!$ for a
                total of
                $$
                    \frac{T \eta^3 }{6}
                    \expct{\nabla^3 l}
                    \expct{\nabla l_{n_{t_a}} \nabla l_{n_{t_b}} \nabla l_{n_{t_c}}}
                $$
            \end{shaded}

            %\newpage
            \subthreesect{Direct Perturbation}
            \colorlet{shadecolor}{moosky}
            \begin{shaded}
                We compute the displacement $\theta_T-\theta_0$ to order $\eta^3$ 
                for vanilla SGD.  Expanding
                $
                    \theta_t \in \theta_0 + \eta a_t + \eta^2 b_t + \eta^3 c_t 
                    + o(\eta^3)
                $, we find:
                \begin{align*}
                    \theta_{t+1}
                    =
                    \theta_t    &-  \eta \nabla l_{n_t} (\theta_t) \\
                    \in\theta_0 &+  \eta a_t + \eta^2 b_t + \eta^3 c_t \\
                                &-  \eta \wrap{
                                         \nabla l_{n_t}
                                        +\nabla^2 l_{n_t} (\eta a_t + \eta^2 b_t)
                                        +\frac{1}{2} \nabla^3 l_{n_t} (\eta a_t)^2
                                    }
                                 +  o(\eta^3) \\
                    =
                    \theta_0    &+   \eta   \wrap{a_t - \nabla l_{n_t}} \\
                                &+   \eta^2 \wrap{b_t - \nabla^2 l_{n_t} a_t} \\ 
                                &+   \eta^3 \wrap{
                                         c_t
                                        -\nabla^2 l_{n_t} b_t
                                        -\frac{1}{2} \nabla^3 l_{n_t} a_t^2
                                     }
                                 +   o(\eta^3)
                \end{align*}
                We thus have the recurrences
                $
                    a_{t+1} = a_t - \nabla l_{n_t}
                $,
                $
                    b_{t+1} = b_t - \nabla^2 l_{n_t} a_t
                $, and
                $
                    c_{t+1} = c_t -\nabla^2 l_{n_t} b_t 
                                  -\frac{1}{2} \nabla^3 l_{n_t} a_t^2
                $
                with solutions:
                $a_t = -\sum_{t} \nabla l_{n_t}$ and
                $\eta^2 b_t = +\eta^2 \sum_{t_0 < t_1} \nabla^2 l_{n_{t_1}} \nabla l_{n_{t_0}}$.
                We do not compute $c_t$ because we will soon see that it will be
                multiplied by $0$.
                %
                To third order, the testing loss of SGD is
                \begin{align*}
                    l(\theta_T)
                    \in
                            l(\theta_0)
                    &+     (\nabla   l)   (\eta a_T + \eta^2 b_T + \eta^3 c_T)                              \\
                    &+\frac{\nabla^2 l}{2}(\eta a_T + \eta^2 b_T             )^2                            \\
                    &+\frac{\nabla^3 l}{6}(\eta a_T                          )^3 
                     +o(\eta)^3                                                                             \\
                    =
                        l(\theta_0)
                    &+  \eta       \wrap{(\nabla l) a_T                               }                     \\
                    &+  \eta^2     \wrap{(\nabla l) b_T + \frac{\nabla^2 l}{2} a_T^2  }                     \\
                    &+  \eta^3     \wrap{(\nabla l) c_T + (\nabla^2 l) a_T b_T + \frac{\nabla^3 l}{6} a_T^3}
                     +o(\eta)^3                                                                             
                \end{align*}
                Because $\expct{\nabla l}$ vanishes at initialization, we neglect
                the $(\nabla l)$ terms.  The remaining $\eta^3$ terms involve
                $a_T b_T$, and $a_T^3$.  So let us
                compute their expectations:
                \begin{align*}
                    \expct{a_T b_T}
                        =&- \sum_{t} \sum_{t_0 < t_1}
                            \expct{\nabla l_{n_t} \nabla^2 l_{n_{t_1}} \nabla l_{n_{t_0}}}
                        \\
                        =&- \sum_{t_0 < t_1}  
                            \sum_{t \notin \{t_0, t_1\}} 
                                \expct{\nabla l_{n_t}} \expct{\nabla^2 l_{n_{t_1}}} \expct{\nabla l_{n_{t_0}}}
                        \\&- \sum_{t_0 < t_1}  
                            \sum_{t = t_0}
                                \expct{\nabla l_{n_t} \nabla l_{n_{t_0}}} \expct{\nabla^2 l_{n_{t_1}}}
                        \\&- \sum_{t_0 < t_1}  
                            \sum_{t = t_1}
                                \expct{\nabla l_{n_t} \nabla^2 l_{n_{t_1}}} \expct{\nabla l_{n_{t_0}}}
                \end{align*}
            %\end{shaded}
            %\newpage
            %\begin{shaded}
                Since $\expct{\nabla l}$ divides $\expct{a_T b_T}$, the latter
                vanishes.
                \begin{align*}
                    \expct{a_T^3}
                        =&- \sum_{t_a, t_b, t_c}
                                \expct{\nabla l_{n_{t_a}} \nabla l_{n_{t_b}} \nabla l_{n_{t_c}}}
                        \\
                        =&- \sum_{\substack{t_a, t_b, t_c\\ \text{disjoint}}}  
                                \expct{\nabla l_{n_{t_a}}} \expct{\nabla l_{n_{t_b}}} \expct{\nabla l_{n_{t_c}}}
                        \\&-3 \sum_{t_a=t_b\neq t_c}  
                                \expct{\nabla l_{n_{t_a}} \nabla l_{n_{t_b}}} \expct{\nabla l_{n_{t_c}}}
                        \\&-\sum_{t_a=t_b=t_c}  
                                \expct{\nabla l_{n_{t_a}} \nabla l_{n_{t_b}} \nabla l_{n_{t_c}}}
                \end{align*}
                As we initialize at a test minimum, only the last line remains, at
                it has $T$ identical summands.
                When we plug into the expression for SGD testing loss, we get
                $$
                    \frac{T \eta^3 }{6}
                    \expct{\nabla^3 l}
                    \expct{\nabla l_{n_{t_a}} \nabla l_{n_{t_b}} \nabla l_{n_{t_c}}}
                $$
            \end{shaded}



%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Math  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\newpage
\section{Mathematics of the theory}\label{appendix:math}
    \subsection{Assumptions and Definitions}                        \label{appendix:assumptions}
        We assume throughout this work the following regularity properties of
        the loss landscape.
        
        \textbf{Existence of Taylor Moments} --- we assume
        that each finite collection of polynomials of the $0$th and higher
        derivatives of the $l_x$, all evaluated at any point $\theta$, may be
        considered together as a random variable insofar as they are equipped
        with a probability measure upon the standard Borel algebra.

        \textbf{Analyticity Uniform in Randomness} --- we assume that
        the functions $\theta \mapsto l_x(\theta)$ --- and the expectations
        of polynomials of their $0$th and higher derivatives --- exist and are
        analytic with radii of convergence bounded from $0$ (by a potentially
        $\theta$-dependent function).  So expectations and derivatives commute. 

        \textbf{Boundedness of Gradients} --- we also assume that the gradients
        $\nabla l_x(\theta)$, considered as random covectors, are bounded by
        some continuous function of $\theta$.\footnote{
            Some of our experiments involve Gaussian noise, which is not
            bounded and so violates the hypothesis.  In practice, Gaussians are
            effectively bounded
            %, on the one hand in that with high
            %probability no standard normal sample encountered on Gigahertz
            %hardware within the age of the universe will much exceed $\sqrt{2
            %\log(10^{30})} \approx 12$, and on the other hand
            in that
            our predictions vary smoothly with the first few moments of this
            distribution, so that a $\pm 12$-clipped Gaussian will yield almost
            the same predictions.
        }
        A metric-independent way of expressing this boundedness constraint
        is that the gradients all lie in some subset $\Ss \subseteq TM$ of
        the tangent bundle of weight space, where, for any compact $\Cc
        \subseteq M$, we have that the topological pullback --- of
        $\Ss \hookrightarrow TM \twoheadrightarrow M$
        and
        $\Cc \hookrightarrow M$ ---
        is compact.
        
        Now we turn to definitions.

        \begin{dfn}[Diagrams] \label{dfn:diagrams}
            A diagram is a finite rooted tree equipped with a partition of
            nodes.  We draw the tree using thin ``edges''.  By
            convention, we draw each node to the right of its children; the
            root is thus always rightmost.  We draw the partition
            by connecting the nodes within each part via fuzzy ``ties''.  For
            example, $\sdia{c(012-3)(03-13-23)}$ has $2$ parts.
            %
            We insist on using as few fuzzy ties as possible so that, if $d$
            counts edges and $c$ counts ties, then $d+1-c$ counts parts. 
            %
            There may
            be multiple ways to draw a single diagram, e.g.
            $\sdia{c(01-23)(03-13-23)}=\sdia{(02-13)(03-13-23)}$. 
        \end{dfn}
        \begin{dfn}[Embedding a Diagram into a Grid]
            An embedding of a diagram into a grid is an assignment of that
            diagram's non-root nodes to pairs $(n,t)$ such that each node
            occurs at a time $t^\prime$ strictly after each of its children and
            such that two nodes occupy the same row $n$ if they
            inhabit the same part of $D$'s partition.
            %
            %We say an embedding is \emph{strict} if it assigns to each part
            %a different datapoint $n$.
        \end{dfn}
        We define $\uvalue(D)$ and $\rvalue_f(D)$ as in
        \S\ref{appendix:evaluate-embeddings}.

    \subsection{A key lemma \`a la Dyson}                           \label{appendix:key-lemma}

        Suppose $s$ is an analytic function defined on the space of weights.
        The following Lemma, reminiscent of \cite{dy49a}, helps us track
        $s(\theta)$ as SGD updates $\theta$:
        \begin{klem*} \label{lem:dyson}
            For all $T$: for $\eta$ sufficiently small, $s(\theta_T)$ is a sum
            over tuples of natural numbers:
            \begin{equation}\label{eq:dyson2}
                s(\theta_T) = 
                \sum_{(d_t: 0\leq t<T) \in \NN^T}
                (-\eta)^{\sum_t d_t}
                \wrap{
                    \prod_{0 \leq t < T}
                        \wrap{\left.
                            \frac{(g \nabla)^{d_t}}{d_t!}
                        \right|_{g = \sum_{n\in \Bb_t} \nabla l_n(\theta) / B}}
                }(s) (\theta_0)
            \end{equation}
            Moreover, the expectation symbol (over training sets) commutes with
            the outer sum.
        \end{klem*}
        Here, we consider each $(g \nabla)^{d_t}$ as a higher order function
        that takes in a function $f$ defined on weight space and outputs a
        function equal to the $d_t$th derivative of $f$, times $g^{d_t}$.
        The above product then indicates composition of $(g \nabla)^{d_t}$'s
        across the different $t$'s.  In total, that product takes the function
        $s$ as input and outputs a function equal to some polynomial of $s$'s
        derivatives.

        \begin{proof}[Proof of the Key Lemma]%
            We work in a neighborhood of the initialization so that the tangent
            space of weight space is a trivial bundle.  For convenience, we fix
            a  coordinate system, and with it the induced flat,
            non-degenerate inverse metric $\tilde\eta$; the benefit is that we
            may compare our varying $\eta$ against one fixed $\tilde\eta$.
            Henceforth, a ``ball'' unless otherwise specified will mean a ball
            with respect to $\tilde\eta$ around the initialization $\theta_0$.
            Since $s$ is analytic, its Taylor series converges to $s$ within
            some positive radius $\rho$ ball.  By assumption, every $l_t$ is
            also analytic with radius of convergence around $\theta_0$ at least
            some $\rho>0$.  Since gradients are $x$-uniformly
            bounded by a continuous function of $\theta$, and since in finite
            dimensions the closed $\rho$-ball is compact, we have a strict
            gradient bound $b$ uniform in both $x$ and $\theta$ on gradient
            norms within that closed ball.  When
            \begin{equation} \label{eq:smalleta}
                2 \eta T b < \rho \tilde\eta
            \end{equation}
            as norms, SGD after $T$ steps on any train set
            will necessarily stay within the $\rho$-ball.\footnote{
                The $2$ ensures that SGD initialized at
                any point within a $\rho/2$ ball will necessarily stay within
                the $\rho$-ball.
            } We note that the above condition on $\eta$ is weak enough to
            permit all $\eta$ within some open neighborhood of $\eta=0$.  

            Condition \ref{eq:smalleta} together with analyticity of $s$ then
            implies that
            $
                \wrap{\exp(-\eta g \nabla) s}(\theta) = s(\theta - \eta g)
            $
            when $\theta$ lies in the $\tilde\eta$ ball (of radius $\rho$) and
            its $\eta$-distance from that $\tilde\eta$ ball's boundary exceeds
            $b$, and that both sides are analytic in $\eta, \theta$ on the same
            domain --- and \emph{a fortiori} when $\theta$ lies in the ball of
            radius $\rho (1 - 1/(2T))$.  Likewise, a routine induction through
            $T$ gives the value of $s$ (after doing $T$ gradient steps from an
            initialization $\theta$) as
            $$
                \wrap{
                    \prod_{0\leq t<T}
                        \left.
                            \exp(-\eta g \nabla)
                        \right|_{g=\nabla l_t(\theta)}
                }
                (s)(\theta)
            $$
            for any $\theta$ in the $\rho (1-T/(2T)$-ball (that is, the
            $\rho/2$-ball), and that both sides are analytic in $\eta, \theta$
            on that same domain.  Note that in each exponential, the
            $\nabla_\nu$ does not act on the $\nabla_\mu l(\theta)$ with which
            it pairs.  

            Now we use the standard expansion of $\exp$.  Because (by
            analyticity) the order $d$ coeffients of $l_t, s$ are bounded by
            some exponential decay in $d$ that has by assumption an $x$-uniform
            rate, we have absolute convergence and may rearrange sums.  We
            choose to group by total degree:
            \begin{equation} \label{eq:expansion}
                \cdots 
                =
                \sum_{0\leq d < \infty} (-\eta)^d
                \sum_{\substack{(d_t: 0\leq t<T) \\ \sum_t d_t = d}}
                \wrap{
                    \prod_{0 \leq t < T} \left.
                        \frac{(g \nabla)^{d_t}}{d_t!}
                    \right|_{g=\nabla l_t(\theta)}
                } s (\theta)
            \end{equation}
            The first part of the Key Lemma is proved.  It remains to show that
            expectations over train sets commute with the above summation.

            We will apply Fubini's Theorem.  To do so, it suffices to show that   
            $$
                \wabs{c_d((l_t: 0\leq t<T))} 
                \triangleq
                \wabs{
                    \sum_{\substack{(d_t: 0\leq t<T) \\ \sum_t d_t = d}}
                    \wrap{
                        \prod_{0 \leq t < T} \left.
                            \frac{(g \nabla)^{d_t}}{d_t!}
                        \right|_{g=\nabla l_t(\theta)}
                    } s (\theta)
                }
            $$
            has an expectation that decays exponentially with $d$.  The symbol
            $c_d$ we introduce purely for convenience; that its value depends
            on the train set we emphasize using function application
            notation.  Crucially, no matter the train set, we have shown
            that the expansion \ref{eq:expansion} (that features $c_d$ appear
            as coefficients) converges to an analytic function for all $\eta$
            bounded as in condition \ref{eq:smalleta}.  The uniformity of this
            demanded bound on $\eta$ implies by the standard relation between
            radii of convergence and decay of coefficients that $\wabs{c_d}$
            decays exponentially in $d$ at a rate uniform over train sets.
            If the expectation of $\wabs{c_d}$ exists at all, then, it will
            likewise decay at that same shared rate.
            
            Finally, $\wabs{c_d}$ indeed has a well-defined expected value, for
            $\wabs{c_d}$ is a bounded continuous function of a
            (finite-dimensional) space of $T$-tuples (each of whose entries can
            specify the first $d$ derivatives of an $l_t$) and because the
            latter space enjoys a joint distribution.  So Fubini's Theorem
            applies.  The Key Lemma follows.   
        \end{proof}

    \subsection{From Dyson to diagrams}                             \label{appendix:toward-diagrams}

        %{\color{red} TODO: define diagrams! FILL IN }

        We now describe the terms that appear in the Key Lemma.  The following
        result looks like Theorem \ref{thm:resum}, except it has $\uvalue(D)$
        instead of $\uvalue_f(D)$, and the sum is over all diagrams, not just
        linkless ones.  In fact, we will use Theorem \ref{thm:pathint} to
        prove Theorem \ref{thm:resum}.

        \begin{thm}[Test Loss as a Path Integral] \label{thm:pathint}
            For all $T$: for $\eta$ sufficiently small, SGD's expected test
            loss is
            \begin{equation*}\label{eq:sgdcoef}
                \sum_{D}
                %\wrap{
                    \sum_{\text{embeddings}~f}
                    \frac{1}{\wabs{\Aut_f(D)}}
                %}
                \frac{\uvalue(D)}{(-B)^{|\edges(D)|}}
            \end{equation*}
            Here, $D$ is a diagram whose root $r$ does not participate in
            any fuzzy edge, $f$ is an embedding of $D$ into a grid, and
            $\wabs{\Aut_f(D)}$ counts the graph-automorphisms of $D$ that
            preserve $f$'s assignment of nodes to cells.
            %
            If we replace $D$ by 
            $
                \wrap{-\sum_{p \in \parts(D)} (D_{rp} - D)/N}
            $, where $r$ is $D$'s root,
            we obtain the expected generalization gap (testing minus training loss).
        \end{thm}

        Theorem \ref{thm:pathint} describe the terms that appear in the Key
        Lemma by matching each term to an embedding of a diagram in a grid,
        so that the infinite sum becomes a sum over all diagram grid 
        configurations.  The main idea is that the combinatorics of diagrams
        parallels the combinatorics of repeated applications of the product
        rule for derivatives applied to the expression in the Key Lemma.
        Balancing against this combinatorial explosion are factorial-style
        denominators, again from the Key Lemma, that we summarize in terms of
        the sizes of automorphism groups.

        \begin{proof}[Proof of Theorem \ref{thm:pathint}]
            We first prove the statement about testing losses.
            Due to the analyticity property established in our proof of the
            Key Lemma, it suffices to show agreement at each degree $d$ and
            train set individually.  That is, it suffices to show --- for
            each train set $(l_n: 0\leq n<N)$, grid $S$, function $\pi:
            S\to [N]$ that induces $\sim$, and natural $d$ --- that
            \begin{align} \label{eq:toprove}
                (-\eta)^d
                \sum_{\substack{
                    (d_t: 0\leq t<T) \\
                    \sum_t d_t = d
                }}
                \wrap{
                    \prod_{0 \leq t < T} \left.
                        \frac{(g \nabla)^{d_t}}{d_t!}
                    \right|_{g=\nabla l_t(\theta)}
                } l (\theta)
                = \nonumber \\
                \sum_{\substack{
                    D \in \image(\Free) \\
                    \textnormal{with $d$ edges}
                }}
                \wrap{
                    \sum_{f: D\to\Free(S)}
                    \frac{1}{\wabs{\Aut_f(D)}}
                }
                \frac{\uvalue_\pi(D, f)}{B^{d}}
            \end{align}
            Here, $\uvalue_\pi$ is the value of a diagram embedding before
            taking expectations over train sets.  We have for all $f$ that
            $\expct{\uvalue_\pi(D, f)} = \uvalue(D)$.
            Observe that both sides of \ref{eq:toprove} are finitary sums.

            \begin{rmk}[Differentiating Products] \label{rmk:leibniz}
                The product rule of Leibniz easily generalizes to higher
                derivatives of finitary products:
                $$
                    \nabla^{\wabs{M}} \prod_{k \in K} p_k
                    = 
                    \sum_{\nu:M\to K} \prod_{k\in K} \wrap{
                        \nabla^{\wabs{\nu^{-1}(k)}} p_k
                    }
                $$
                The above has $\wabs{K}^{\wabs{M}}$ many term indexed by
                functions to $K$ from $M$.
            \end{rmk}

            We proceed by joint induction on $d$ and $S$.  The base cases
            wherein $S$ is empty or $d=0$ both follow immediately from the Key
            Lemma, for then the only embedding is the unique embedding of the
            one-node diagram $\sdia{(0)()}$.  For the induction step, suppose
            $S$ is a sequence of $\Mm = \min S \subseteq S$ followed by a
            strictly smaller $S$ and that the result is proven for $(\tilde d,
            \tilde S)$ for every $\tilde d \leq d$.  Let us group by $d_0$ the
            terms on the left hand side of desideratum \ref{eq:toprove}.
            Applying the induction hypothesis with $\tilde d = d - d_0$, we
            find that that left hand side is:
            \begin{align*}
                \sum_{\substack{
                    0 \leq d_0 \leq d
                }}
                \sum_{\substack{
                    \tilde D \in \image(\Free) \\
                    \textnormal{with $d-d_0$ edges}
                }}
                \frac{1}{d_0!}
                \sum_{\tilde f: \tilde D\to\Free(\tilde S)} \wrap{
                    \frac{1}{\wabs{\Aut_{\tilde f}(\tilde D)}}
                }
                ~\cdot~
                \\ %---------------------------------------------
                (-\eta)^{d_0}
                \left.
                    (g \nabla)^{d_0}
                \right|_{g=\nabla l_0(\theta)}
                \frac{\uvalue_\pi(\tilde D, \tilde f)}{B^{d-d_0}}
            \end{align*}
            Since $\uvalue_\pi(\tilde D, \tilde f)$ is a multilinear product of
            $d-d_0+1$ many tensors, the product rule for derivatives tells us
            that $(g \nabla)^{d_0}$ acts on $\uvalue_\pi(\tilde D, \tilde f)$
            to produce $(d-d_0+1)^{d_0}$ terms.  In fact,
            $
                g = \sum_{m\in \Mm} \nabla l_m(\theta) / B
            $ 
            expands to
            $B^{d_0}(d-d_0+1)^{d_0}$ terms, each conveniently indexed
            by a pair of functions $\beta:[d_0]\to \Mm$ and $\nu:[d_0]\to
            \tilde D$.  The $(\beta, \nu)$-term corresponds to an embedding
            $f$ of a larger diagram $D$ in the sense that it contributes
            $\uvalue_\pi(D, f)/B^{d_0}$ to the sum.  Here, $(f, D)$ is $(\tilde
            f, \tilde D)$ with $\wabs{\wrap{\beta \times \nu}^{-1}(n, v)}$ many
            additional edges from the cell of datapoint $n$ at time $0$ to the
            $v$th node of $\tilde D$ as embedded by $\tilde f$.

            By the Leibniz rule of Remark \label{rmk:leibniz}, this $(\beta,
            \nu)$-indexed sum by corresponds to a sum over embeddings $f$ that
            restrict to $\tilde f$, whose terms are multiples of the value of
            the corresponding embedding of $D$.  Together with the sum over
            $\tilde f$, this gives a sum over all embeddings $f$.  So we now
            only need to check that the coefficients for each $f:D\to S$ are as
            claimed.

            We note that the $(\beta, \nu)$ diagram (and its value) agrees with
            the $(\beta \circ \sigma, \nu \circ \sigma)$ diagram (and its
            value) for any permutation $\sigma$ of $[d_0]$.  The corresponding
            orbit has size
            \begin{align*}
                \frac{d_0!}{
                    \prod_{(m, i) \in \Mm \times \tilde D}
                        \wabs{(\beta \times \nu)^{-1}(m, i)}!
                }
            \end{align*}
            by the Orbit Stabilizer Theorem of elementary group theory.   

            It is thus enough to show that
            \begin{align*} \label{eqn:countclaim}
                \wabs{\Aut_f(D)} = 
                \wabs{\Aut_{\tilde f}(D)}
                \prod_{(m, i) \in \Mm \times \tilde D}
                    \wabs{(\beta \times \nu)^{-1}(m, i)}!
            \end{align*}
            We will show this by a direct bijection.  First, observe that
            $
                f = \beta \sqcup \tilde f:
                    [d_0] \sqcup \tilde D \to \Mm \sqcup \tilde S
            $. 
            So each automorphism $\phi: D\to D$ that commutes with $f$ induces
            both an automorphism
            $
                \Aa = \phi|_{\tilde D}: \tilde D\to \tilde D
            $
            that commutes with $\tilde f$ together with the data of a map
            $
                \Bb = \phi_{[d_0]}: [d_0] \to [d_0] 
            $
            that both commutes with $\beta$.  However, not every such pair of
            maps arises from a $\phi$.  For, in order for $\Aa \sqcup \Bb: D
            \to D$ to be an automorphism, it must respect the order structure
            of $D$.  In particular, if $x\leq_D y$ with $x \in [d_0]$ and $y
            \in \tilde D$, then we need
            $$
                \Bb(x) \leq_D \Aa(y)
            $$
            as well.  The
            pairs $(\Aa, \Bb)$ that thusly preserve order are in bijection with
            the $\phi \in \Aut_f(D)$.  There are $\wabs{\Aut_{\tilde f}(\tilde
            D)}$ many $\Aa$.  For each $\Aa$, there are as many $\Bb$ as there
            are sequences $(\sigma_i: i \in \tilde D)$ of permutations on
            $
                \{j\in [d_0]: j\leq_D i\} \subseteq [d_0]
            $ 
            that commute with $\Bb$.  These permutations may be chosen
            independently; there are 
            $
                \prod_{m\in \Mm}
                    \wabs{(\beta \times \nu)^{-1}(m, i)}!
            $
            many choices for $\sigma_i$.  Claim \ref{eqn:countclaim} follows,
            and with it the correctness of coefficients.
 
            The argument for generalization gaps parallels the above when we
            use $l-\sum_n l_n/N$ instead of $l$ as the value for $s$. 
            Theorem \ref{thm:pathint} is proved.
        \end{proof}

        \begin{rmk}[The Case of $E=B=1$ SGD]
            The grid of $E=B=1$ SGD permits all and only those
            embeddings that assign to each part of a diagram's partition  a
            distinct cell.  Such embeddings factor through a diagram
            ordering and are thus easily counted using factorials per
            Prop \ref{prop:vanilla}.  That prop immediately
            follows from the now-proven Theorem \ref{thm:pathint}.
        \end{rmk}

        \begin{prop} \label{prop:vanilla}
            The order $\eta^d$ contribution to the expected testing loss of
            one-epoch SGD with singleton batches is:
            \begin{equation*}\label{eq:sgdbasiccoef}
                \frac{(-1)^d}{d!} \sum_{D} 
                |\ords(D)| {N \choose P-1} {d \choose d_0,\cdots,d_{P-1}}
                \uvalue(D)
            \end{equation*}
            where $D$ ranges over $d$-edged diagrams.  Here, $D$'s parts have
            sizes $d_p: 0\leq p\leq P$, and $|\ords(D)|$ counts the total
            orderings of $D$ s.t.\ children precede parents and parts are
            contiguous.
        \end{prop}

    %\subsection{Interlude: a review of M\"obius inversion}          \label{appendix:mobius}

    
    \subsection{Proof of Theorem \ref{thm:resum}}    \label{appendix:resum}

        (We say an embedding is \textbf{strict} if it assigns to each part
        a different datapoint $n$.
        Then, by M\"obius inversion (\cite{ro64}), a sum over strict embeddings
        of moment values (\S\ref{appendix:evaluate-embeddings}) matches 
        a sum over all embeddings of $\uvalue$s.)

        The diagrams summed in Theorem \ref{thm:resum} and \ref{thm:converge}
        may be grouped by their geometric realizations.  Each nonempty class of
        diagrams with a given geometric realization has a unique element with
        minimally many edges, and in this way all and only linkless diagrams
        arise. 

        We encounter two complications: on one hand, that the sizes of
        automorphism groups might not be uniform among the class of diagrams
        with a given geometric realization.  On the other hand, that the
        embeddings of a specific member of that class might be hard to count.
        The first we handle using Orbit-Stabilizer.  The second we address as
        described by \S\ref{appendix:mobius} via M\"obius sums.
           
        \begin{proof}[Proof of Theorem \ref{thm:resum}]
            We apply M\"obius inversion (\S\ref{appendix:mobius}) to Theorem
            \ref{thm:pathint} (\S\ref{appendix:toward-diagrams}).
            %The result
            %is that chains of embeddings  
            %{\color{red} FILL IN}

            The difference in loss from the noiseless case is given by all the
            diagram embeddings with at least one fuzzy tie, where the fuzzy tie
            pattern is actually replaced by a difference between noisy and
            noiseless cases as prescribed by the preceding discussion on
            M\"obius Sums.  Beware that even relatively noiseless embeddings
            may have illegal collisions of non-fuzzily-tied nodes within a
            single grid (data) row.  Throughout the rest of this proof, we
            permit such illegal embeddings of the fuzz-less diagrams that arise
            from the aforementioned decomposition.  

            Because the Taylor series for analytic functions converge
            absolutely in the interior of the disk of convergence, the
            rearrangement of terms corresponding to a grouping by geometric
            realizations preserves the convergence result of Theorem
            \ref{thm:pathint}.  

            Let us then focus on those diagrams $\sigma$ with a given geometric
            realization represented by an linkless diagram $\rho$.  By
            Theorem \ref{thm:pathint}, it suffices to show that
            \begin{equation} \label{eq:hard}
                \sum_{f:\rho\to S}
                \sum_{\substack{
                    \tilde f:\sigma\to S \\
                    \exists i_\star: f=\tilde f \circ i_\star
                }}
                \frac{1}{\wabs{\Aut_{\tilde f}(\sigma)}}
                =
                \sum_{f:\rho\to S}
                \sum_{\substack{
                    \tilde f:\sigma\to S \\
                    \exists i_\star: f=\tilde f \circ i_\star
                }}
                \sum_{\substack{
                    i:\rho\to\sigma \\
                    f = \tilde f \circ i
                }}
                \frac{1}{\wabs{\Aut_{f}(\rho)}}
            \end{equation}
            Here, $f$ is considered up to an equivalence defined by
            precomposition with an automorphism of $\rho$.  We likewise
            consider $\tilde f$ up to automorphisms of $\sigma$.  And above,
            $i$ ranges through maps that induce isomorphisms of geometric
            realizations, where $i$ is considered equivalent to $\hat i$ when
            for some automorphism $\phi \in \Aut_{\tilde f}(\sigma)$, we have
            $\hat i = i \circ \phi$.  Name as $X$ the set of all such $i$s
            under this equivalence relation.

            In equation \ref{eq:hard}, we have introduced
            redundant sums to structurally align the two expressions on the
            page; besides this rewriting, we see that equation \ref{eq:hard}'s
            left hand side matches Theorem \ref{thm:pathint} resulting formula
            and tgat its right hand side is the desired formula of Theorem
            \ref{thm:resum}. 

            To prove equation \ref{eq:hard}, it suffices to show (for any
            $f, \tilde f, i$ as above) that
            $$
                \wabs{\Aut_f(\rho)}
                =
                \wabs{\Aut_{\tilde f}(\sigma)}
                \cdot
                \wabs{X}
            $$
            We will prove this using the Orbit Stabilizer Theorem by presenting
            an action of $\Aut_f(\rho)$ on $X$.  We simply use precomposition
            so that $\psi\in \Aut_f(\rho)$ sends $i\in X$ to $i\circ \psi$.
            Since $f\circ\psi = f$, $i\circ \psi \in X$.  Moreover, the action
            is well-defined, because if $i\sim \hat i$ by $\phi$, then $i \circ
            \psi \sim \hat i \circ \psi$ also by $\phi$.
            
            The stabilizer of $i$ has size $\wabs{\Aut_{\tilde f}(\rho)}$.
            For, when $i \sim i \circ \psi$ via $\phi \in \Aut_{\tilde
            f}(\rho)$, we have $i\circ \psi = \phi \circ i$.  This relation in
            fact induces a bijective correspondence: \emph{every} $\phi$
            induces a $\psi$ via $\psi = i^{-1} \circ \phi \circ i$, so we have
            a map $\text{stabilizer}(i) \hookleftarrow \Aut_{\tilde f}(\rho)$
            seen to be well-defined and injective because structure set
            morphisms are by definition strictly increasing and because $i$s
            must induce isomorphisms of geometric realizations.  Conversely,
            every $\psi$ that stabilizes enjoys \emph{only} one $\phi$ via
            which $i \sim i \circ \phi$, again by the same (isomorphism and
            strict increase) properties.  So the stabilizer has the claimed
            size.

            Meanwhile, the orbit is all of $\wabs{X}$.  Indeed, suppose $i_A,
            i_B \in X$.  We will present $\psi \in \Aut_f(\rho)$ such that $i_B
            \sim i_A \circ \psi$ by $\phi=\text{identity}$.  We simply define
            $\psi = i_A^{-1} \circ i_B$, well-defined by the aforementioned
            (isomorphisms and strict increase) properties.  It is then routine
            to verify that
            $
                f \circ \psi
                =
                \tilde f \circ i_A \circ i_A^{-1} \circ i_B
                =
                \tilde f \circ i_B
                = f.
            $
            So the orbit has the claimed size, and by the Orbit Stabilizer
            Theorem, the coefficients in the expansions of Theorems 
            \ref{thm:resum} and \ref{thm:pathint} match.
        \end{proof}

    \subsection{Proof of Theorem \ref{thm:converge}}    \label{appendix:converge}

        Let $L_{d,T}(\eta)$ be the $d$th order truncation of Thm \ref{thm:resum}'s series.
        The following lemma compares $L_d = \lim_{T\to\infty} L_{d,T}$ to
        $L_T = \lim_{d\to\infty} L_{d,T}$.
        %
        For convenience, we fix some inner product on $\Mm$. 
        %
        \begin{lem}\label{lem:strength}
            Fix $U\subseteq \Mm$ open and $\theta_\star\in U$ a local minimum
            of $l$.  Assume Local Strength.  Then for all $d$ and for all
            initializations $\theta_0$ in some neighborhood $V_d$ of
            $\theta_\star$: there exist $T_0,A,B>0$ so that $\sup_{T\geq T_0}
            {\rm ReLU}(|L_d(\eta)-L_T(\eta)|-\exp(A-BT))$ is well-defined and
            is $o(\eta^d)$.
            `Well-defined' means the sup is finite on some
            neighborhood (in the space of symmetric positive semidefinite
            forms) of $\eta=0$.
        \end{lem}
        \begin{proof}
            In what follows, $\epsilon,\delta,s,\epsilon^\pr$ denote positive
            quantities to be regarded as small and under our control.  We
            follow proof-writing tradition by \emph{using} each quantity
            before \emph{setting} it.

            Let $\hat l = \sum_n l_n/N$ denote the training landscape.  Since
            $\nabla l_x, \nabla\nabla l_x$ are universally bounded, Chernoff
            implies that for any compact $K$ and any $\epsilon,\delta>0$:
            there are $T_0, \eta_\star>0$ so that whenever 
            $\eta < \eta_\star$ and $T>T_0$, the error $\sup_{\theta\in K}
            \|\nabla \hat l-\nabla l\|$ is small ($<\epsilon$) with high
            probability ($>1-\delta$).

            Since $l_x$ is universally bounded, the low-probability fringe
            event contributes $O(\delta)$ to the final testing loss $L_T$.  In
            fact, by Chernoff this contribution decays exponentially in $T$.
            (Thence arises the lemma's term $\exp(A-BT)$). 

            Let's thus focus on the high-probability bulk event.  For any
            $s>0$, we may arrange $K, \epsilon$ with reference to Local
            Strength's $Q_-$ so that, conditioned on this bulk event, SGD's
            trajectory is confined to an open radius-$s$ ball
            $B_s(\theta_\star)$ around $\theta_\star$ when it is
            initialized at $\theta_\star$.  This confinement being an open
            condition and dynamics being continuous, we may in fact arrange $K,
            \epsilon$ so that (again conditioned on the bulk event) SGD's
            trajectory is confined to $B_s(\theta_\star)$ when 
            $\theta_0 \in B_{r(s)}(\theta_\star)$.

            Invoking $L_{T_0}$'s analyticity, we re-choose $\eta_\star>0$ as
            needed so that for all lesser $\eta$,
            $|L_{d,T_0}(\eta)-L_{T_0}(\eta)|<\epsilon^\pr$.  
        \end{proof}

        \begin{proof}[Proof of Theorem \ref{thm:converge}]
            Since we assumed hessians are positive: for any $m$, the propagator
            $K^t = \wrap{(I-\eta H)^{\otimes m}}^t$ exponentially decays to $0$
            (at a rate dependent on $m$).  Since up to degree $d$ only a finite
            number of diagrams exist and hence only a finite number of possible
            $m$s, the exponential rates are bounded away from $0$.  Moreover,
            for any fixed $t_{\text{big}}$, the number of diagrams ---
            involving no exponent $t$ exceeding $t_{\text{big}}$ --- is
            eventually constant as $T$ grows.  Meanwhile, the number involving
            at least one exponent $t$ exceeding that threshold grows
            polynomially in $T$ (with degree $d$).  The exponential decay of
            each term overwhelms the polynomial growth in the number of terms,
            and Theorem's first part follows.

            More difficult is the Theorem's statement about Local Strength.
        \end{proof}

    %\subsection{How to modify proofs to handle variants}            \label{appendix:prove-variants}

    \subsection{Proofs of corollaries}                              \label{appendix:corollaries}

        \subsubsection{Corollary \ref{cor:entropic}}

            \begin{proof}
                The relevant linkless diagram is $\sdia{c(01-2-3)(02-12-23)}$
                {color{red} (amputated as in the previous subsubsection)}.   
                An embedding of this diagram into $E=B=1$ SGD's grid 
                is determined by two durations --- 
                $t$ from {\color{moor}red} to {\color{moog}green} and
                $\tilde t$ from {\color{moog}green} to {\color{moob}blue} ---
                obeying $t+\tilde t \leq T$.
                The automorphism group of each embedding has size $2$: identity
                or switch the {\color{moor}red} nodes.  So the answer is: 
                $$
                    C_{\mu \nu}
                    J^{\rho\lambda}_{\sigma}
                    \wrap{\int_{t+\tilde t\leq T}
                        \wrap{\exp(-t \eta H) \eta}^{\mu\rho}
                        \wrap{\exp(-t \eta H) \eta}^{\nu\lambda}
                        \wrap{\exp(-\tilde t \eta H) \eta}^{\sigma\pi}
                    }
                $$
                Standard calculus then gives the desired result.
            \end{proof}

        \subsubsection{Corollary \ref{cor:overfit}'s first part}

            \begin{proof}[Proof.]
                The relevant linkless diagram is $\sdia{(01-2)(02-12)}$
                (which equals $\sdia{c(01-2)(02-12)}$ because we are at a test
                minimum).  This diagram has one embedding for each pair of
                same-row shaded cells, potentially identical, in a grid; for
                GD, the grid has every cell shaded, so each
                \emph{non-decreasing} pair of durations in $[0,T]^2$ is
                represented; the symmetry factor for the case where the cells
                is identical is $1/2$, so we lose no precision by interpreting
                a automorphism-weighted sum over the \emph{non-decreasing}
                pairs as half of a sum over all pairs.  Each of these may embed
                into $N$ many rows, hence the factor below of $N$.  The two
                integration variables (say, $t, \tilde t$) separate, and we
                have:
                $$
                    \frac{N}{B^{\text{degree}}}
                    \frac{C_{\mu\nu}}{2}
                    \int_t \wrap{\exp(-t \eta H)}^\mu_\lambda
                    \int_{\tilde t} \wrap{\exp(-\tilde t \eta H)}^\nu_\rho
                    \eta^{\lambda\sigma}
                    \eta^{\rho\pi}
                    H_{\sigma\pi}
                $$
                Since for GD we have $N=B$ and we are working to degree $2$,
                the prefactor is $1/N$.  Since $\int_t \exp(a t) = (I-\exp(-a
                T))/a$, the desired result follows. 
            \end{proof}

        \subsubsection{Corollary \ref{cor:overfit}'s second part}

            We apply the generalization gap modification (described in
            \S\ref{appendix:solve-variants}) to Theorem \ref{thm:resum}'s
            result about testing losses.

            \begin{proof}[Proof]
                The relevant linkless diagram is $\sdia{c(01)(01)}$.  This
                diagram has one embedding for each shaded cell of grid;
                for GD, the grid has every cell shaded, so each duration
                from $0$ to $T$ is represented.  So the generalization gap is,
                to leading order,
                $$
                    + \frac{C_{\mu\nu}}{N}
                    \int_t \wrap{\exp(-t \eta H)}^\mu_\lambda
                    \eta^{\lambda\nu}
                $$
                Here, the minus sign from the gen-gap modification canceled
                with the minus sign from the odd power of $-\eta$.  Integration
                finishes the proof.
            \end{proof}
 
        \subsubsection{Corollaries \ref{cor:epochs} and \ref{cor:batch}}

            Corollary \ref{cor:epochs} and Corollary \ref{cor:batch} follow
            from plugging appropriate values of $M, N, B$ into the following
            prop.

            \begin{prop}\label{prop:ordtwo}
                To order $\eta^2$, the testing loss of SGD --- on $N$ samples
                for $T=MN$ timesteps with batch size $B$ dividing $N$ and with
                any shuffling scheme --- has expectation
                {\small
                \begin{align*}
                                                            l              
                    &- MN                                   G_\mu G^\mu       
                     + MN\wrap{MN - \frac{1}{2}}            G_\mu H^{\mu}_{\nu} G^\nu \\
                    &+ MN\wrap{\frac{M}{2}}                 C_{\mu \nu} H^{\mu \nu}
                     + MN\wrap{\frac{M-\frac{1}{B}}{2}}     \wrap{\nabla_\mu C^{\nu}_{\nu}} G^\mu / 2
                \end{align*}
                }
            \end{prop}

            \begin{proof}[of Prop \ref{prop:ordtwo}]
                To prove Prop \ref{prop:ordtwo}, we simply count
                the embeddings of the diagrams, noting that the automorphism groups
                are all of size $1$ or $2$. 
                %Since we use fuzzy outlines instead of
                %fuzzy ties, we allow untied nodes to occupy the same row, since the
                %excess will be canceled out by the term subtract in the definition of
                %fuzzy outlines.
                See Table \ref{tbl:ordtwo}.
                \begin{table}[h]
                    \centering
                    \begin{tabular}{cll}
                        diagram                 & embed.s w/ $\wabs{\Aut_f}=1$  & embed.s w/ $\wabs{\Aut_f}=2$   \\ \hline
                        $\sdia{(0)()}$          & $1$                           & $0$                            \\  
                        $\sdia{(0-1)(01)}$      & $MNB$                         & $0$                            \\                  
                        $\sdia{(0-1-2)(01-12)}$ & ${MN\choose 2}B^2$            & $0$                            \\
                        $\sdia{c(01-2)(01-12)}$ & $N{MB\choose 2}$              & $0$                            \\
                        $\sdia{(0-1-2)(02-12)}$ & ${MNB\choose 2}$              & $MNB$                          \\
                        $\sdia{c(01-2)(02-12)}$ & $N{MB\choose 2}$              & $MNB$                             
                    \end{tabular}
                    \caption{Terms used in proof of Prop \ref{prop:ordtwo}}
                    \label{tbl:ordtwo}
                \end{table}
            \end{proof}

        \subsubsection{Corollary \ref{cor:vsode}}

            The corollary's first part follows immediately from 
            Prop \ref{prop:ordtwo}.

            \begin{proof}[Proof of second part]
                Because $\expct{\nabla l}$ vanishes at initialization, all
                diagrams with a degree-one vertex that is a singleton vanish.
                Because we work at order $\eta^3$, we consider $3$-edged
                diagrams.  Finally, because all first and second moments match
                between the two landscapes, we consider only diagrams with at
                least one part (in their partition) of size at least $3$.  The
                only such test diagram is $\sdia{c(012-3)(03-13-23)}$.  This
                embeds in $T$ ways (one for each grid cell --- recall that
                $E=B=1$) and has symmetry factor $1/3!$ for a total of
                $$
                    \frac{T \eta^3 }{6}
                    \expct{\nabla^3 l}
                    \expct{\nabla l_{n_{t_a}} \nabla l_{n_{t_b}} \nabla l_{n_{t_c}}}
                    = 
                    \frac{T \eta^3 }{6}
                    S_{\mu\nu\sigma}J^{\mu\nu\sigma}
                $$
                This is the un-resummed expression.  To obtain the re-summed
                expression, we replace $\eta^{\mu\nu}$ with $(I-\eta H)^{\Delta t-1}\eta)^{\mu\nu}$. 
                The embeddings range over $T$ many times uniformly spaced in $[0,T]$. 
                So we may integrate (let's name our variable of integration $\tau$,
                and let's have it represent $\tau=\Delta t-1 = T-t-1$):
                $$
                    \int_{0\leq \tau < T}
                        \wrap{\exp(-\tau \eta H) \eta}^{\mu\nu}
                        \wrap{\exp(-\tau \eta H) \eta}^{\pi\sigma}
                        \wrap{\exp(-\tau \eta H) \eta}^{\lambda\rho}
                        S_{\mu\pi\lambda}J_{\nu\sigma\rho}
                $$
                Observe that 
                $$
                        \wrap{\exp(-\tau \eta H)}_\mu^\nu 
                        \wrap{\exp(-\tau \eta H)}_\pi^\sigma 
                        \wrap{\exp(-\tau \eta H)}_\lambda^\rho
                    =
                        \wrap{\exp(-\tau Y)}_{\mu\nu\pi}^{\sigma\lambda\rho}
                $$                         
                where $Y = \eta H \otimes I \otimes I + I \otimes \eta H \otimes I + I \otimes I \otimes \eta H$
                is a six-index tensor.  We finish by recalling that 
                $$
                    \int_t \exp(A t) = \frac{\exp(A t)}{A} |_t 
                $$
            \end{proof}                     

            %\newpage
    \subsection{Future topics}                                      \label{appendix:future}

        Our diagrams invite exploration of Lagrangian formalisms and curved
        backgrounds:\footnote{
            \cite{la60, la51} review these concepts.
        }
        \begin{quest}
            Does some least-action principle govern SGD; if not, what is an
            essential obstacle to this characterization?
        \end{quest}
        Lagrange's least-action formalism intimately intertwines with the
        diagrams of physics.  Together, they afford a modular framework for
        introducing new interactions as new terms or diagram nodes.  In fact,
        we find that some \emph{higher-order} methods --- such as the
        Hessian-based update
        $
            \theta \leftsquigarrow
            \theta -
            (\eta^{-1} + \lambda \nabla \nabla l_t(\theta))^{-1}
            \nabla l_t(\theta)
        $
        parameterized by small $\eta, \lambda$ --- admit diagrammatic analysis
        when we represent the $\lambda$ term as a second type of diagram node.
        Though diagrams suffice for computation, it is Lagrangians that most
        deeply illuminate scaling and conservation laws.

        Our work assumes a flat metric $\eta^{\mu\nu}$, but it might
        generalize to weight spaces curved in the sense of Riemann.\footnote{
            One may represent the affine connection as a node, thus giving
            rise to non-tensorial and hence gauge-dependent diagrams.
        }  Such curvature finds concrete application in the \emph{learning on
        manifolds} paradigm of \cite{ab07, zh16}, notably specialized to
        \cite{am98}'s \emph{natural gradient descent} and \cite{ni17}'s
        \emph{hyperbolic embeddings}.  While that work focuses on
        \emph{optimization} on curved weight spaces, in machine learning we
        also wish to analyze \emph{generalization}.
        %
        Starting with the intuition that ``smaller'' hypothesis classes
        generalize better and that curvature controls the volume of small
        neighborhoods, we conjecture that sectional curvature regularizes
        learning:
        \begin{conj}[Sectional curvature regularizes]
            If $\eta(\tau)$ is a Riemann metric on weight space, smoothly
            parameterized by $\tau$, and if the sectional curvature through
            every $2$-form at $\theta_0$ increases as $\tau$ grows, then
            the gen.\ gap attained by fixed-$T$ SGD with learning rate $c
            \eta(\tau)$ (when initialized from $\theta_0$) decreases as $\tau$
            grows, for all sufficiently small $c>0$.
        \end{conj}
        We are optimistic our formalism may resolve conjectures such as above.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  Experiments  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\newpage
\section{Experimental methods}\label{appendix:experiments}

    \subsection{What artificial landscapes did we use?}             \label{appendix:artificial}

        We define three artificial landscapes, called
        \Gauss, \Helix, and \MeanEstimation.

        \subsubsection{\Gauss}
            Consider fitting a centered normal $\Nn(0, \sigma^2)$ to some
            centered standard normal data.  We parameterize the landscape by
            $h=\log(\sigma^2)$ so that the Fisher information matches the
            standard dot product \citep{am98}.   
            %
            More explicitly, the \Gauss\, landscape is a probability
            distribution $\Dd$ over functions $l_x:\RR^1\to \RR$ on
            $1$-dimensional weight space, indexed by standard-normally
            distributed $1$-dimensional datapoints $x$ and defined by the
            expression:
            $$
                l_x(h)
                \triangleq
                \frac{1}{2} \wrap{h + x^2 \exp(-h)}
            $$
            The gradient at sample $x$ and weight $\sigma$ is then $g_x(h) =
            (1-x^2\exp(-h))/2$.  Since $x\sim \Nn(0, 1)$, the gradient $g_x(h)$
            will be affinely related to a chi-squared, and in particular
            non-Gaussian.
            
            To measure overfitting, we initialize at the true test minimum
            $h=0$, then train and see how much the testing loss increases.  At
            $h=0$, the expected gradient vanishes, and the testing loss of SGD
            involves only diagrams that have no leaves of size one.
            
        \subsubsection{\Helix}
            The \Helix\ landscape has chirality, much like Archimedes'
            screw.
            %\cite{vi00}. 
            Specifically, the \Helix\ landscape has
            %
            weights     $\theta = (u,v,z) \in \RR^3$,
            %
            data points $x \sim \Nn(0, 1)$,
            %
            and loss:
            %
            $$
                l_x(\theta)
                \triangleq
                \frac{1}{2} H(\theta) + x \cdot S(\theta)
            $$
            %
            Here,
            $$
                H(\theta) = u^2 + v^2 + (\cos(z) u + \sin(z) v)^2
            $$
            is quadratic in $u, v$, and
            $$
                S(\theta) = \cos(z-\pi/4) u + \sin(z-\pi/4) v
            $$
            is linear in $u, v$.
            Also, since $x \sim \Nn(0,1)$, the $x \cdot S(\theta)$ term has
            expectation $0$.
            %
            In fact, the landscape has a three-dimensional continuous screw
            symmetry consisting of translation along $z$ and simulateous
            rotation in the $u-v$ plane.  Our experiments are initialized at
            $u=v=z=0$, which lies within a valley of global minima defined by
            $u=v=0$.  

            The paper body showed that SGD travels in \Helix' $+z$
            direction.  By topologically quotienting the weight space, say by
            identifying points related by a translation by $\Delta z = 200\pi$,
            we may turn the line-shaped valley into a circle-shaped valley.
            Then SGD eternally travels, say, counterclockwise.  Alternatively,
            one may preserve the homotopy type of the underlying weight space
            by Nash-embedding a flat solid torus
            $$
                [-10^1,+10^1]\times[-10^1,+10^1]\times[-10^3,+10^3]/((x,y,-10^3)\sim(x,y,+10^3))
            $$
            in a higher-dimensional Euclidean space and extending \Helix\ from
            that torus to the ambient space.

            Slightly modifying \Helix\ by adding a linear term $\alpha\cdot z$
            to $l$ for $\eta\alpha^2 \ll \eta^2/6$ leads SGD to perpetually ascend.
           
        \subsubsection{\MeanEstimation}
            The \MeanEstimation\, family of landscapes has $1$ dimensional
            weights $\theta$ and $1$-dimensional datapoints $x$.  It is defined
            by the expression:
            $$
                l_x(\theta)
                \triangleq
                \frac{1}{2} H \theta^2 + x S \theta
            $$
            Here, $H, S$ are positive reals parameterizing the family; they
            give the hessian and (square root of) gradient covariance,
            respectively.

            For our hyperparameter-selection experiment (Figure
            \ref{fig:takreg}\ofthree{2}) we introduce an $l_2$
            regularization term as follows:
            $$
                l_x(\theta, \lambda)
                \triangleq
                \frac{1}{2} (H + \lambda) \theta^2 + x S \theta
            $$
            Here, we constrain $\lambda\geq 0$ during optimization using
            projections; we found similar results when parameterizing $\lambda
            = \exp(h)$, which obviates the need for projection but necessitates
            a non-canonical choice of initialization.  We initialize
            $\lambda=0$.

    \subsection{What image-classification landscapes did we use?}   \label{appendix:natural}

        \subsubsection{Architectures}
            In addition to the artificial loss landscapes
                \Gauss, \Helix, and \MeanEstimation, 
                we tested our predictions on logistic linear regression
                and simple convolutional networks (2 convolutional weight layers
                each with kernel $5$, stride $2$, and $10$ channels, followed by
                two dense weight layers with hidden dimension $10$) for the
                CIFAR-10 \cite{kr09} and Fashion-MNIST datasets \cite{xi17}.  The
                convolutional architectures used $\tanh$ activations and Gaussian
                Xavier initialization.  To set a standard distance scale on weight
                space, we parameterized the model so that the
                Gaussian-Xavier initialization of the linear maps in each layer
                differentially pulls back to standard normal initializations of the
                parameters.

                Some of our experiments involve Gaussian noise, which is not
                bounded and so violates the our assumptions.  In practice,
                Gaussians are effectively bounded in that our predictions vary
                smoothly with the first few moments of this distribution, so
                that a $\pm 12$-clipped Gaussian will yield almost the same
                predictions.
                %
                Even more experiments permit arbitrarily large losses and  
                thus also violate our boundedness assumptions; since in practice
                SGD with small learning rates does not explore regions of
                very-large loss, we consider this violation negligible.

            \subsubsection{Datasets}
                For image classification landscapes, we regard the finite amount of
                available data as the true (sum of diracs) distribution $\Dd$ from
                which we sample testing and training sets in i.i.d.\ manner (and hence
                ``with replacement'').  We do this to gain practical access to a
                ground truth against which we may compare our predictions.  One
                might object that this sampling procedure would cause testing and
                training sets to overlap, hence biasing testing loss measurements.  In
                fact, testing and training sets overlap only in reference, not in
                sense: the situation is analogous to a text prediction task in
                which two training points culled from different corpora happen to
                record the same sequence of words, say, ``Thank you!''.  In any
                case, all of our experiments focus on the limited-data regime, e.g.
                $10^1$ datapoints out of $\sim 10^{4.5}$ dirac masses, so overlaps
                are rare.

        \subsection{Measurement process}                                \label{appendix:measure}

            \subsubsection{Diagram evaluation on real landscapes}
                We implemented the formulae of \S\ref{appendix:bessel} in order
                to estimate diagram values from real data measured at
                initialization from batch averages of products of derivatives.

            \subsubsection{Descent simulations}
                We recorded testing and training losses for each of the trials below.  To
                improve our estimation of average differences, when we compared two
                optimizers, we gave them the same random seed (and hence the same
                training sets).

                We ran $2 \cdot 10^5$ trials of \Gauss\, with SDE and SGD,
                initialized at the test minimum with $T=1$ and $\eta$ ranging from
                $5\cdot 10^{-2}$ to $2.5\cdot 10^{-1}$.
                We ran $5 \cdot 10^1$ trials of \Helix with SGD with $T=10^4$
            and $\eta$ ranging from $10^{-2}$ to $10^{-1}$.
            We ran $10^3$ trials of \MeanEstimation with GD and STIC
            with $T=10^2$, $H$ ranging from $10^{-4}$ to $4 \cdot 10^0$,
            a covariance of gradients of $10^2$, and the true mean $0$ or
            $10$ units away from initialization.

            We ran $5 \cdot 10^4$ trials of the CIFAR-10 convnet on each of $6$
            Glorot-Xavier initializations we fixed once and for all through
            these experiments for the optimizers SGD, GD, and GDC, with $T=10$
            and $\eta$ between $10^{-3}$ and $2.5 \cdot 10^{-2}$.  We did
            likewise for the linear logistic model on the one initialization of
            $0$.

            We ran $4 \cdot 10^4$ trials of the Fashion-MNIST convnet on each
            of $6$ Glorot-Xavier initializations we fixed once and for all
            through these experiments for the optimizers SGD, GD, and GDC with
            $T=10$ and $\eta$ between $10^{-3}$ and $2.5 \cdot 10^{-2}$.  We
            did likewise for the linear logistic model on the one
            initialization of $0$. 

    \subsection{Implementing optimizers}                            \label{appendix:optimizers}

        We approximated SDE by refining time discretization by a factor of
        $16$, scaling learning rate down by a factor of $16$, and introducing
        additional noise in the shape of the covariance in proportion as
        prescribed by the Wiener process scaling.

        Our GDC regularizer was implemented using the unbiased estimator
        $$
            \hat{C} \triangleq (l_x - l_y)_\mu {l_x}_\nu / 2
        $$
        
        For our tests of regularization based on Corollary \ref{cor:overfit},
        we exploited the low-dimensional special structure of the artificial
        landscape in order to avoid diagonalizing to perform the matrix
        exponentiation: precisely, we used that, even on training landscapes,
        the covariance of gradients would be degenerate in all but one
        direction, and so we need only exponentiate a scalar.

    \subsection{Software frameworks and hardware}                   \label{appendix:frameworks}

        All code and data-wrangling scripts can be found on
        {\color{mooteal}github.com/???????/perturb}.  This link will be made
        available after the period of double-blind review.
        %
        Our code uses PyTorch 0.4.0 \citep{pa19} on Python 3.6.7; there are no
        other substantive dependencies.  The code's randomness is parameterized
        by random seeds and hence reproducible.
        %
        We ran experiments on a Lenovo laptop and on our institution's
        clusters; we consumed about $100$ GPU-hours.

    \subsection{Unbiased estimators of landscape statistics}        \label{appendix:bessel}
        %
        We use the following method --- familiar to some but apparently nowhere
        described in writing --- for obtaining unbiased estimates for
        various statistics of the loss landscape.  The method is merely an
        elaboration of Bessel's factor \citep{ga23}.  For completeness, we
        explain it here. 
        
        Given samples from a joint probability space $\prod_{0\leq d<D} X_d$,
        we seek unbiased estimates of \emph{multipoint correlators} (i.e.\ products of
        expectations of products) such as $\wang{x_0 x_1 x_2}\wang{x_3}$.  Here,
        angle brackets denote expectations over the population. 
        For
        example, say $D=2$ and from $2S$ samples we'd like to estimate
        $\wang{x_0 x_1}$.  Most simply, we could use $\Avg_{0\leq s<2S}
        x_0^{(s)} x_1^{(s)}$, where $\Avg$ denotes averaging over the sample.  In fact, the
        following also works:
        %
        \begin{equation} \label{eq:bessel}
            S
            \wrap{\Avg_{0\leq s< S} x_0^{(s)}}
            \wrap{\Avg_{0\leq s< S} x_1^{(s)}}
            +
            (1-S)
            \wrap{\Avg_{0\leq s< S} x_0^{(s)}}
            \wrap{\Avg_{S\leq s<2S} x_1^{(s)}}
        \end{equation}
        %
        When multiplication is expensive (e.g. when each $x_d^{(s)}$ is a
        tensor and multiplication is tensor contraction), we prefer the latter,
        since it uses $O(1)$ rather than $O(S)$ multiplications.  This in turn
        allows more efficient use of batch computations on GPUs.  We now
        generalize this estimator to higher-point correlators (and $D\cdot S$
        samples).

        For uniform notation, we assume without loss that each of the $D$
        factors appears exactly once in the multipoint expression of interest;
        such expressions then correspond to partitions on $D$ elements, which
        we represent as maps $\mu:\wasq{D}\to \wasq{D}$ with $\mu(d)\leq d$ and
        $\mu\circ \mu=\mu$.  Note that $\wabs{\mu} \coloneqq \wabs{im(\mu)}$
        counts $\mu$'s parts.  We then define the statistic
        %
        $$
            \wurl{x}_\mu
            \triangleq
            \prod_{0\leq d<D} \Avg_{0\leq s<S} x_d^{(\mu(d)\cdot S + s)}
        $$
        %
        and the correlator $\wang{x}_\mu$ we define to be the expectation of 
        $\wurl{x}_\mu$ when $S=1$.  In this notation, \ref{eq:bessel} says: 
        $$
            \wang{x}_{\partitionbox{0}\partitionbox{1}}
            =
            \expct{
                S       \cdot \wurl{x}_{\partitionbox{0 1}} +
                (1-S)   \cdot \wurl{x}_{\partitionbox{0}\partitionbox{1}}
            }
        $$
        %
        Here, the boxes indicate partitions of $\wasq{D}=\wasq{2}=\{0,1\}$.
        Now, for general $\mu$, we have:
        %
        \begin{equation} \label{eq:newbessel}
            \expct{S^D \wurl{x}_\mu}
            =
            \sum_{\tau\leq \mu} \wrap{
                \prod_{0\leq d<D}
                    \frac{S!}{\wrap{S-\wabs{\tau(\mu^{-1}(d))}}!}
            }
            \wang{x}_\tau
        \end{equation}
        %
        where `$\tau \leq \mu$' ranges through partitions \emph{finer} than 
        $\mu$, i.e. maps $\tau$ through which $\mu$ factors.   
        In smaller steps, \ref{eq:newbessel} holds because
        %
        \begin{align*}
            \expct{S^D \wurl{x}_\mu}
            &=
            \expct{
                \sum_{(0\leq s_d<S) \in \wasq{S}^D}
                \prod_{0\leq d<D}
                x_d^{\wrap{\mu(d)\cdot S + s_d}}
            }\\
            &=
            \sum_{\substack{(0\leq s_d<S) \\ \in \wasq{S}^D}}
            \expct{
                \prod_{0\leq d<D}
                x_d^{\wrap{\min \wurl{
                    \tilde{d}~:~\mu(\tilde{d})\cdot S+s_{\tilde{d}} = \mu(d)\cdot S+s_d
                }}}
            }\\
            &=
            \sum_{\tau} \wabs{\wurl{\substack{
                (0\leq s_d<S)~\in~[S]^D~: \\
                \wrap{\substack{
                    \mu(d)=\mu(\tilde{d}) \\
                    \wedge~s_d=s_{\tilde{d}}
                }}
                \Leftrightarrow
                \tau(d)=\tau(\tilde{d})
            }}}
            \wang{x}_\tau \\
            &=
            \sum_{\tau\leq \mu} \wrap{
                \prod_{0\leq d<D}
                    \frac{S!}{\wrap{S-\wabs{\tau(\mu^{-1}(d))}}!}
            }
            \wang{x}_\tau
        \end{align*}

        Solving \ref{eq:newbessel} for $\wang{x}_\mu$, we find:
        %
        \begin{equation*}
            \text{\fbox{$
            \wang{x}_\mu
            =
            \frac{S^D}{S^{\wabs{\mu}}}
            \expct{
                \wurl{x}_\mu
            }
            -
            \sum_{\tau < \mu} \wrap{
                \prod_{d\in im(\mu)}
                \frac{\wrap{S-1}!}{\wrap{S-\wabs{\tau(\mu^{-1}(d))}}!}
            }
            \wang{x}_\tau
            $}}
        \end{equation*}
        %
        This expresses $\wang{x}_\mu$ in terms of the batch-friendly estimator
        $\wurl{x}_\mu$ as well as correlators $\wang{x}_\tau$ for $\tau$
        \emph{strictly} finer than $\mu$.  We may thus (use dynamic programming
        to) obtain unbiased estimators $\wang{x}_\mu$ for all partitions $\mu$.
        Symmetries of the joint distribution and of the multilinear
        multiplication may further streamline estimation by turning a sum over
        $\tau$ into a multiplication by a combinatorial factor.  For example,
        in the case of complete symmetry:
        %
        $$
            \wang{x}_{\partitionbox{012}}
            =
            S^2
            \wurl{x}_{\partitionbox{012}}
            -
            \frac{(S-1)!}{(S-3)!}
            \wurl{x}_{\partitionbox{0}\partitionbox{1}\partitionbox{2}}
            -
            3\frac{(S-1)!}{(S-2)!}
            \wurl{x}_{\partitionbox{0}\partitionbox{12}}
        $$

    \subsection{Additional figures}                                 \label{appendix:figures}

        In the rightmost figure, 
        we add Corollary \ref{cor:overfit}'s
            generalization gap estimate to $l$.  By descending on this
            regularized loss, we may tune smooth hyperparameters such as $l_2$
            regularization coefficients for small datasets ($H \ll C/N$)
            (\S\ref{appendix:figures}).  Since matrix exponentiation takes time
            cubic in dimension, this regularizer is most useful for small
            models.


        \begin{figure}[h] 
            \centering
            \centering
            \pmoo{3.5cm}{multi-fashion-logistic-0}
            \pmoo{3.5cm}{vs-sde}
            \pmoo{3.5cm}{tak-reg}
            \caption{
                \textbf{Further experimental results}.
                %
                \textbf{Left}: SGD with $2, 3, 5, 8$ epochs incurs greater test
                loss than one-epoch SGD (difference shown in I bars) by the
                predicted amounts (predictions shaded) for a range of learning
                rates.  Here, all SGD runs have $N=10$; we scale the learning
                rate for $E$-epoch SGD by $1/E$ to isolate the effect of
                inter-epoch correlations away from the effect of larger $\eta
                T$.
                %
                \textbf{Center}: SGD's difference from SDE after $\eta T
                \approx 10^{-1}$ with maximal coarseness on \Gauss.  Two
                effects not modeled by SDE --- time-discretization and
                non-Gaussian noise oppose on this landscape but do not
                completely cancel.  Our theory approximates the above curve
                with a correct sign and order of magnitude; we expect that the
                fourth order corrections would improve it further.
                %
                \textbf{Right}: Blue intervals show regularization using Corollary
                \ref{cor:overfit}.  When the blue intervals fall below the
                black bar, this proposed method outperforms plain GD.  For
                \MeanEstimation\ with fixed $C$ and a range of $H$s, initialized
                a fixed distance \emph{away} from the true minimum, descent on
                an $l_2$ penalty coefficient $\lambda$ improves on plain GD for
                most Hessians.  The new method does not always outperform GD,
                because $\lambda$ is not perfectly tuned according to STIC but
                instead descended on for finite $\eta T$.
            }
            \label{fig:takreg}
        \end{figure}

    \section{Review of Tensors}
        \subsection{What is a tensor?}
        \subsection{Vectors versus covectors}
        \subsection{Contraction}
        \subsection{Linear maps as tensors}

\end{document}

%%This section prepares for \S\ref{sect:using}, which uses
%%            diagrams to generalize \S\ref{sect:exegesis}'s separation, tame
%%            \S\ref{sect:exegesis}'s combinatorial explosion of terms, and
%%            temper \S\ref{sect:exegesis}'s polynomial divergence.  
%%            %
%%            We start by characterizing the higher order terms.  Suppose
%%            $s$ is an analytic function on $\Mm$.  For example, $s$ might be
%%            the testing loss $l$.  The following Lemma, reminiscent of
%%            \cite{dy49a}'s, tracks $s(\theta)$ as SGD updates $\theta$:
%%            \begin{klem*} \label{lem:dyson}
%%                For all $T$: for $\eta$ sufficiently small, $s(\theta_T)$ is a
%%                sum over tuples of natural numbers:
%%                \begin{equation}\label{eq:dyson}
%%                    \sum_{(d_t: 0\leq t<T) \in \NN^T}
%%                    (-\eta)^{\sum_t d_t}
%%                    \wrap{
%%                        \prod_{0 \leq t < T}
%%                            \wrap{\left.
%%                                \frac{(g \nabla)^{d_t}}{d_t!}
%%                            \right|_{g = \sum_{n\in \Bb_t} \nabla l_n(\theta) / B}}
%%                    }(s) (\theta_0)
%%                \end{equation}
%%                Moreover, an expectation symbol (over training sets) commutes
%%                with the outer sum.
%%            \end{klem*}
%%            Here, we consider each $(g \nabla)^{d_t}$ as a higher order
%%            function that takes in a function $f$ defined on weight space and
%%            outputs a function equal to the $d_t$th derivative of $f$, times
%%            $g^{d_t}$.  The above product then indicates composition of $(g
%%            \nabla)^{d_t}$'s across the different $t$'s.  In total, that
%%            product takes the function $s$ as input and outputs a function
%%            equal to some polynomial of $s$'s derivatives.
%%
%%            For example, the $\eta^3$ terms that appear in the above
%%            (for $s=l$) include:
%%            $$
%%                -\nabla_{\mu} l_{t=2} \nabla_{\nu} l_{t=2}
%%                 \nabla^{\mu}\nabla^{\nu}\nabla_{\lambda} l_{t=5}
%%                 \nabla^{\lambda} l
%%                %
%%                \hspace{2cm}
%%                %
%%                -\nabla_{\mu} l_{t=2} \nabla_{\lambda} l_{t=2}
%%                 \nabla^{\mu}\nabla_{\nu} l_{t=5}
%%                 \nabla^{\nu}\nabla^{\lambda} l
%%            $$
%%            Let us take expectations over training sets.  Suppose $B=1$ and
%%            $N>5$; then batches at $t=2,5$ are independent so the
%%            expectations factor (we use ``$\leftrightsquigarrow$'', not
%%            of ``$=$'', due to page's \pageref{dfn:uvalue-body}'s footnote):  
%%            \begin{align*}
%%               -\expc[\nabla_{\mu} l_{t=2} \nabla_{\nu} l_{t=2}]
%%                \expc[\nabla^{\mu}\nabla^{\nu}\nabla_{\lambda} l_{t=5}]
%%                \expc[\nabla^{\lambda} l]
%%                &&
%%               -\expc[\nabla_{\mu} l_{t=2} \nabla_{\nu} l_{t=2}]
%%                \expc[\nabla^{\mu}\nabla_{\lambda} l_{t=5}]
%%                \expc[\nabla^{\nu}\nabla^{\lambda} l] \\
%%               = -(GG+C)_{\mu\nu}J^{\mu\nu}_{\lambda}G^{\lambda} 
%%                && 
%%               = -(GG+C)_{\mu\nu}H^{\mu}_{\lambda} H^{\nu\lambda} \\
%%                \leftrightsquigarrow -\uvalue(\sdia{c(01-2-3)(02-12-23)})
%%                && 
%%                \leftrightsquigarrow -\uvalue(\sdia{c(01-2-3)(02-13-23)})
%%            \end{align*}
%%            To prepare for \S\ref{sect:using}, we write terms (such as
%%            above) as uvalues (Def.\ \ref{dfn:uvalue-body}) of diagrams.
%%            %(e.g.\ $\sdia{c(01-2-3)(02-12-23)}, \sdia{c(01-2-3)(02-13-23)}$).
%%            See Fig.\ \ref{fig:uvalue-example}.
%%            %We notated the tensors above using diagrams.  Diagrams help us
%%            %organize (\ref{eq:dyson})'s terms.
%%            %Each diagram evaluates to a
%%            %tensor: its \emph{un-resummed value} or
%%            %\translucent{moolime}{\emph{\textbf{uvalue}}, defined as a product containing a $d$th}
%%            %\translucent{moolime}{derivative of $l_x$ for each degree-$d$ node, grouped under
%%            %expectation symbols per the diagram's gray}
%%            %\translucent{moolime}{outlines, and tensor-contracted per the diagram's black edges}
%%            %(\S\ref{appendix:evaluate-embeddings} provides details).\footnote{
%%            %    We write $\leftrightsquigarrow$ instead of $=$ since diagrams
%%            %    evaluate to products of cumulants $C$ rather than of moments
%%            %    $GG+C$: \S\ref{appendix:evaluate-embeddings}.
%%            %}
%%            %%
%%            %We express (\ref{eq:dyson}) as a weighted sum of the $\uvalue$s of
%%            %all diagrams formally defined below.
%%            %We supplu The following examples of
%%            %(in)valid diagrams supply an informal sense sufficient to
%%            %read this paper's body.\footnote{
%%            %    Throughout, colors help us refer to parts of diagrams; colors
%%            %    lack mathematical meaning.
%%            %}
%%            \noindent
%%
%            So
%            \begin{align*}
%                \uvalue(\sdia{c(01-2-3)(02-12-23)}) &=
%                \textstyle
%                \sum_{\substack{\mu\nu\xi \\ \omicron\pi\rho}}
%                \expc[\nabla_{\mu} l_{t=2} \nabla_{\nu} l_{t=2}]
%                \expc[\nabla_{\omicron}\nabla_{\pi}\nabla_{\xi} l_{t=5}]
%                \expc[\nabla_{\rho} l]\eta^{\mu\omicron}\eta^{\nu\pi}\eta^{\xi\rho}
%                \\
%                \uvalue(\sdia{c(01-2-3)(02-13-23)}) &= 
%                \textstyle
%                \sum_{\substack{\mu\nu\xi \\ \omicron\pi\rho}}
%                \expc[\nabla_{\mu} l_{t=2} \nabla_{\nu} l_{t=2}]
%                \expc[\nabla_{\omicron}\nabla_{\xi} l_{t=5}]
%                \expc[\nabla_{\pi}\nabla_{\rho} l]\eta^{\mu\omicron}\eta^{\nu\pi}\eta^{\xi\rho} \\
%            \end{align*}
%            %\translucent{moolime}{\noindent\parbox{\textwidth}{
