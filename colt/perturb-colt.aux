\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{bo91}
\citation{fe49}
\citation{pe71}
\providecommand \oddpage@label [2]{}
\jmlr@workshop{34rd Annual Conference on Learning Theory}
\jmlr@title{SGD at Small Learning Rates}{SGD at Small Learning Rates}
\jmlr@author{\Name {Samuel C.\ Tenka} \Email {\sammail } \\ \addr MIT, CSAIL }{\Name {Samuel C.\ Tenka} \Email {\sammail } \\ \addr MIT, CSAIL }
\providecommand {\FN@pp@footnotehinttrue }{}
\providecommand {\FN@pp@footnote@aux }[2]{}
\newlabel{jmlrstart}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Intuitions about SGD}{1}{subsection.0.1.1}\protected@file@percent }
\FN@pp@footnote@aux{1}{1}
\FN@pp@footnote@aux{2}{1}
\citation{ko93}
\citation{bo13}
\citation{ne04}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Background, notation, assumptions}{2}{subsection.0.1.2}\protected@file@percent }
\FN@pp@footnote@aux{3}{2}
\newlabel{prop:nest}{{0}{2}{Background, notation, assumptions}{prop.0}{}}
\FN@pp@footnote@aux{4}{2}
\citation{ki52}
\citation{ca47}
\citation{ro51}
\citation{we74}
\citation{bo91}
\citation{le15}
\citation{ne17a}
\citation{ba17}
\citation{zh17}
\citation{ne17b}
\citation{ch18}
\citation{ya19a}
\citation{ro18}
\citation{li18}
\citation{ho17}
\citation{ke17}
\citation{wa18}
\citation{st56}
\citation{di17}
\citation{wu18}
\citation{dy19}
\citation{ch18}
\citation{li17}
\citation{ro12,ku19}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Related work}{3}{subsection.0.1.3}\protected@file@percent }
\FN@pp@footnote@aux{5}{3}
\@writefile{toc}{\contentsline {section}{\numberline {2}Perturbative theory of SGD}{4}{section.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Trivial example}{4}{subsection.0.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Perturbation as technique}{4}{subsection.0.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Introducing diagrams}{4}{subsection.0.2.3}\protected@file@percent }
\newlabel{sect:what-are-diagrams}{{2.3}{4}{Introducing diagrams}{subsection.0.2.3}{}}
\FN@pp@footnote@aux{6}{4}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Insights from diagrams}{4}{subsection.0.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Resummation}{4}{subsection.0.2.5}\protected@file@percent }
\FN@pp@footnote@aux{7}{4}
\citation{we19b}
\citation{ya19b}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Main result}{5}{subsection.0.2.6}\protected@file@percent }
\newlabel{thm:resum}{{1}{5}{Special case of $E=B=1$}{thm.1}{}}
\newlabel{eq:resum}{{1}{5}{Special case of $E=B=1$}{thm.1}{}}
\newlabel{rmk:integrate}{{1}{5}{Main result}{rmk.1}{}}
\newlabel{thm:converge}{{2}{5}{Main result}{thm.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}SGD descends on a $C$-smoothed landscape and prefers minima flat w.r.t.\ $C$.}{5}{subsection.0.2.7}\protected@file@percent }
\newlabel{cor:entropic}{{1}{5}{Computed from $\sdia {c(01-2-3)(02-12-23)}$}{cor.1}{}}
\FN@pp@footnote@aux{8}{5}
\citation{we19b}
\citation{we19b}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Both flat and sharp minima overfit less}{6}{subsection.0.2.8}\protected@file@percent }
\newlabel{subsect:curvature-and-overfitting}{{2.8}{6}{Both flat and sharp minima overfit less}{subsection.0.2.8}{}}
\newlabel{cor:overfit}{{2}{6}{from $\sdia {c(01-2)(02-12)}$, $\sdia {c(01)(01)}$}{cor.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Geometric intuition for curvature-noise interactions.} \textbf  {Left}: Gradient noise pushes SGD toward flat minima (Corollary \ref  {cor:entropic}). The red densities show the typical $\theta $s, perturbed from the minimum due to noise $C$, in two cross sections of the loss valley. $J = \nabla H$ measures how curvature changes across the valley. Our theory does not assume separation between ``fast'' and ``slow'' modes, but we label them in the picture to ease comparison with \cite  {we19b}. Compare with Figure \ref  {fig:archimedes}. \textbf  {\obsoletefontcs  {bf}Right}: Both curvature and the structure of noise affect overfitting. In each of the four subplots, the $\leftrightarrow $ axis represents weight space and the $\updownarrow $ axis represents loss. \offive {1}: \emph  {covector}-perturbed landscapes favor large $H$s. \offive {2}: \emph  {vector}-perturbed landscapes favor small $H$s. SGD's implicit regularization interpolates between these rows (Corollary \ref  {cor:overfit}). \relax }}{6}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:cubicandspring}{{1}{6}{\textbf {Geometric intuition for curvature-noise interactions.} \textbf {Left}: Gradient noise pushes SGD toward flat minima (Corollary \ref {cor:entropic}). The red densities show the typical $\theta $s, perturbed from the minimum due to noise $C$, in two cross sections of the loss valley. $J = \nabla H$ measures how curvature changes across the valley. Our theory does not assume separation between ``fast'' and ``slow'' modes, but we label them in the picture to ease comparison with \cite {we19b}. Compare with Figure \ref {fig:archimedes}. \textbf {\bf Right}: Both curvature and the structure of noise affect overfitting. In each of the four subplots, the $\leftrightarrow $ axis represents weight space and the $\updownarrow $ axis represents loss. \protect \offive {1}: \emph {covector}-perturbed landscapes favor large $H$s. \protect \offive {2}: \emph {vector}-perturbed landscapes favor small $H$s. SGD's implicit regularization interpolates between these rows (Corollary \ref {cor:overfit}). \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Consequences of the theory}{6}{section.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}High-$C$ regions repel small-$(E,B)$ SGD more than large-$(E,B)$ SGD}{6}{subsection.0.3.1}\protected@file@percent }
\newlabel{subsect:epochs-batch}{{3.1}{6}{High-$C$ regions repel small-$(E,B)$ SGD more than large-$(E,B)$ SGD}{subsection.0.3.1}{}}
\citation{ch87}
\citation{li18}
\citation{ch18}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  \textbf  {Chladni plate}. Grains of sand on a vibrating plate tend toward stationary regions. \relax }}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:chladni}{{2}{7}{\textbf {Chladni plate}. Grains of sand on a vibrating plate tend toward stationary regions. \relax }{figure.caption.4}{}}
\FN@pp@footnote@aux{9}{7}
\newlabel{cor:batch}{{3}{7}{$\sdia {c(01-2)(01-12)}$}{cor.3}{}}
\newlabel{cor:epochs}{{4}{7}{$\sdia {c(01-2)(01-12)}$}{cor.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Non-Gaussian noise affects SGD but not SDE}{7}{subsection.0.3.2}\protected@file@percent }
\newlabel{cor:vsode}{{5}{7}{$\sdia {c(01-2)(02-12)}$, $\sdia {c(012-3)(03-13-23)}$}{cor.5}{}}
\FN@pp@footnote@aux{10}{7}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{7}{section.0.4}\protected@file@percent }
\citation{di18}
\citation{di18}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Training time, epochs, and batch size; $C$ repels SGD more than GD}{8}{subsection.0.4.1}\protected@file@percent }
\citation{ch18}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  {\obsoletefontcs  {bf}Experiments on natural and artificial losses.} The label \texttt  {rvalue} refers to Theorem \ref  {thm:resum}'s predictions, approximated as in Remark \ref  {rmk:integrate}. Curves marked \texttt  {uvalue} are polynomial approximations to Theorem \ref  {thm:resum}'s result (see \S  \ref  {appendix:sum-embeddings}). \texttt  {uvalue}s are simpler to work with but (see \ofsix {4}) may be less accurate. \newline  {\obsoletefontcs  {bf}Left: Perturbation models SGD for small $\eta T$.} Fashion-MNIST convnet's test loss vs learning rate. In this small $T$ setting, we choose to use our theory's simpler un-resummed values (\ref  {appendix:evaluate-embeddings}) instead of the more precise $\text  {\textnormal  {rvalue}}$s. \ofsix {0}: For all init.s tested ($1$ shown, $11$ unshown), the order $3$ prediction agrees with experiment through $\eta T \approx 10^0$, corresponding to a decrease in $0\mbox  {-}1$ error of $\approx 10^{-3}$. \ofsix {1}: For large $\eta T$, our predictions break down. Here, the order-$3$ prediction holds until the $0\mbox  {-}1$ error improves by $5\cdot 10^{-3}$. Beyond this, $2$nd order agreement with experiment is coincidental. \newline  {\obsoletefontcs  {bf}Center: $C$ controls gen.\ gap and distinguishes GD from SGD.} With equal-scaled axes, \ofsix {2} shows that GDC matches SGD (small vertical varianec) better than GD matches SGD (large horizontal variance) in test loss for a range of $\eta $ ($\approx 10^{-3}-10^{-1}$) and init.s\ (zero and several Xavier-Glorot trials) for logistic regression and convnets. Here, $T=10$. \ofsix {3}: CIFAR-10 generalization gaps. For all init.s tested ($1$ shown, $11$ unshown), the degree-$2$ prediction agrees with experiment through $\eta T \approx 5\cdot 10^{-1}$. \newline  {\obsoletefontcs  {bf}Right: Predictions near minima excel for large $\eta T$.}\ofsix {4}: SGD travels \textsc  {Archimedes}' valley of global minima in the positive $z$ direction. Note: $H$ and $C$ are bounded across the valley, we see drift for all small $\eta $, and we see displacement exceeding the landscape's period of $2\pi $. So: the drift is not a pathology of well-chosen $\eta $, of divergent noise, or of ephemeral initial conditions. \ofsix {5}: For \textsc  {Mean Estimation}\tmspace  +\thinmuskip {.1667em} with fixed $C$ and a range of $H$s, initialized at the truth, the test losses after fixed-$T$ GD are smallest for very sharp and very flat $H$. Near $H=0$, our predictions improve on TIC \citep  {di18} and thus on AIC. \relax }}{9}{figure.caption.5}\protected@file@percent }
\newlabel{fig:vanilla}{{3}{9}{{\bf Experiments on natural and artificial losses.} The label \texttt {rvalue} refers to Theorem \ref {thm:resum}'s predictions, approximated as in Remark \ref {rmk:integrate}. Curves marked \texttt {uvalue} are polynomial approximations to Theorem \ref {thm:resum}'s result (see \S \ref {appendix:sum-embeddings}). \texttt {uvalue}s are simpler to work with but (see \protect \ofsix {4}) may be less accurate. \newline {\bf Left: Perturbation models SGD for small $\eta T$.} Fashion-MNIST convnet's test loss vs learning rate. In this small $T$ setting, we choose to use our theory's simpler un-resummed values (\ref {appendix:evaluate-embeddings}) instead of the more precise $\rvalue $s. \protect \ofsix {0}: For all init.s tested ($1$ shown, $11$ unshown), the order $3$ prediction agrees with experiment through $\eta T \approx 10^0$, corresponding to a decrease in $0\mbox {-}1$ error of $\approx 10^{-3}$. \protect \ofsix {1}: For large $\eta T$, our predictions break down. Here, the order-$3$ prediction holds until the $0\mbox {-}1$ error improves by $5\cdot 10^{-3}$. Beyond this, $2$nd order agreement with experiment is coincidental. \newline {\bf Center: $C$ controls gen.\ gap and distinguishes GD from SGD.} With equal-scaled axes, \protect \ofsix {2} shows that GDC matches SGD (small vertical varianec) better than GD matches SGD (large horizontal variance) in test loss for a range of $\eta $ ($\approx 10^{-3}-10^{-1}$) and init.s\ (zero and several Xavier-Glorot trials) for logistic regression and convnets. Here, $T=10$. \protect \ofsix {3}: CIFAR-10 generalization gaps. For all init.s tested ($1$ shown, $11$ unshown), the degree-$2$ prediction agrees with experiment through $\eta T \approx 5\cdot 10^{-1}$. \newline {\bf Right: Predictions near minima excel for large $\eta T$.}\protect \ofsix {4}: SGD travels \Archimedes ' valley of global minima in the positive $z$ direction. Note: $H$ and $C$ are bounded across the valley, we see drift for all small $\eta $, and we see displacement exceeding the landscape's period of $2\pi $. So: the drift is not a pathology of well-chosen $\eta $, of divergent noise, or of ephemeral initial conditions. \protect \ofsix {5}: For \MeanEstimation \, with fixed $C$ and a range of $H$s, initialized at the truth, the test losses after fixed-$T$ GD are smallest for very sharp and very flat $H$. Near $H=0$, our predictions improve on TIC \citep {di18} and thus on AIC. \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Minima that are flat \emph  {with respect to} $C$ attract SGD}{9}{subsection.0.4.2}\protected@file@percent }
\newlabel{subsect:entropic}{{4.2}{9}{Minima that are flat \emph {with respect to} $C$ attract SGD}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  \textbf  {\textsc  {Archimedes}.} A \textbf  {green} level surface of $l$ twists around a valley of minima ($z$ axis) at its center; $l$ is large outside this surface. Due to anisotropic noise, $\theta $ scatters away from the $z$ axis toward the \textbf  {purple} tubes. SGD pushes the scattered $\theta $s toward lower loss, i.e.\ toward the level surface, and so toward larger $z$. The $z$ axis points into the page (\textbf  {left}) or upward (\textbf  {right}). \relax }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:archimedes}{{4}{9}{\textbf {\Archimedes .} A \textbf {green} level surface of $l$ twists around a valley of minima ($z$ axis) at its center; $l$ is large outside this surface. Due to anisotropic noise, $\theta $ scatters away from the $z$ axis toward the \textbf {purple} tubes. SGD pushes the scattered $\theta $s toward lower loss, i.e.\^^Mtoward the level surface, and so toward larger $z$. The $z$ axis points into the page (\textbf {left}) or upward (\textbf {right}). \relax }{figure.caption.6}{}}
\citation{we19b}
\citation{ro18}
\citation{bo91}
\citation{go18}
\FN@pp@footnote@aux{11}{10}
\FN@pp@footnote@aux{12}{10}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Sharp and flat minima both overfit less than medium minima}{10}{subsection.0.4.3}\protected@file@percent }
\newlabel{subsect:overfit}{{4.3}{10}{Sharp and flat minima both overfit less than medium minima}{subsection.0.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{10}{section.0.5}\protected@file@percent }
\bibdata{perturb}
\bibcite{ba17}{{1}{2017}{{Bartlett et~al.}}{{Bartlett, Foster, and Telgarsky}}}
\bibcite{bo13}{{2}{2013}{{Bonnabel}}{{}}}
\bibcite{bo91}{{3}{1991}{{Bottou}}{{}}}
\bibcite{ca47}{{4}{1847}{{Cauchy}}{{}}}
\bibcite{ch18}{{5}{2018}{{Chaudhari and Soatto}}{{}}}
\bibcite{ch87}{{6}{1787}{{Chladni}}{{}}}
\bibcite{di17}{{7}{2017}{{Dinh et~al.}}{{Dinh, Pascanu, Bengio, and Bengio}}}
\bibcite{di18}{{8}{2018}{{Dixon and Ward}}{{}}}
\bibcite{dy19}{{9}{2019}{{Dyer and Gur-Ari}}{{}}}
\bibcite{fe49}{{10}{1949}{{Feynman}}{{}}}
\bibcite{go18}{{11}{2018}{{Goyal et~al.}}{{Goyal, Doll\'{a}r, Girshick, Noordhuis, Wesolowski, Kyrola, Tulloch, Jia, and He}}}
\bibcite{ho17}{{12}{2017}{{Hoffer et~al.}}{{Hoffer, Hubara, and Soudry}}}
\bibcite{ke17}{{13}{2017}{{Keskar et~al.}}{{Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Future work}{11}{subsection.0.5.1}\protected@file@percent }
\bibcite{ki52}{{14}{1952}{{Kiefer and Wolfowitz}}{{}}}
\bibcite{ko93}{{15}{1993}{{Kol\'{a}\u {r} et~al.}}{{Kol\'{a}\u {r}, Michor, and Slov\'{a}k}}}
\bibcite{ku19}{{16}{2019}{{Kunstner et~al.}}{{Kunstner, Hennig, and Balles}}}
\bibcite{le15}{{17}{2015}{{LeCun et~al.}}{{LeCun, Bengio, and Hinton}}}
\bibcite{li17}{{18}{2017}{{Li et~al.}}{{Li, Tai, and E}}}
\bibcite{li18}{{19}{2018}{{Liao et~al.}}{{Liao, Miranda, Banburski, Hidary, and Poggio}}}
\bibcite{ne04}{{20}{2004}{{Nesterov}}{{}}}
\bibcite{ne17a}{{21}{2017{a}}{{Neyshabur et~al.}}{{Neyshabur, Bhojanapalli, McAllester, and Srebro}}}
\bibcite{ne17b}{{22}{2017{b}}{{Neyshabur et~al.}}{{Neyshabur, Tomioka, Salakhutdinov, and Srebro}}}
\bibcite{pe71}{{23}{1971}{{Penrose}}{{}}}
\bibcite{ro51}{{24}{1951}{{Robbins and Monro}}{{}}}
\bibcite{ro18}{{25}{2018}{{Roberts}}{{}}}
\bibcite{ro12}{{26}{2012}{{Roux et~al.}}{{Roux, Bengio, and Fitzgibbon}}}
\bibcite{st56}{{27}{1956}{{Stein}}{{}}}
\bibcite{wa18}{{28}{2018}{{Wang et~al.}}{{Wang, Keskar, Xiong, and Socher}}}
\bibcite{we19b}{{29}{2019}{{Wei and Schwab}}{{}}}
\bibcite{we74}{{30}{1974}{{Werbos}}{{}}}
\bibcite{wu18}{{31}{2018}{{Wu et~al.}}{{Wu, Ma, and E}}}
\bibcite{ya19a}{{32}{2019{a}}{{Yaida}}{{}}}
\bibcite{ya19b}{{33}{2019{b}}{{Yaida}}{{}}}
\bibcite{zh17}{{34}{2017}{{Zhang et~al.}}{{Zhang, Bengio, Hardt, Recht, and Vinyals}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}My Proof of Theorem 1}{13}{section.0.A}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}My Proof of Theorem 2}{13}{section.0.B}\protected@file@percent }
\newlabel{jmlrend}{{B}{13}{end of SGD at Small Learning Rates}{section*.9}{}}
\FN@pp@footnotehinttrue 
