\documentclass{article}
\usepackage[
    top   =1in,
    bottom=1in,
    left  =1in,
    right =1in,
]{geometry}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath, amssymb, amsthm, bm}

\newcommand{\Ra}{\textmd{\textsf{\color{purple!50} {R1}}}}
\newcommand{\Rb}{\textmd{\textsf{\color{green!50}  {R2}}}}
\newcommand{\Rc}{\textmd{\textsf{\color{blue!50}   {R3}}}}
\newcommand{\PP}{\mathbb{P}}

\newcommand{\cor}[1]{\textmd{\textsf{Cor #1}}} 
\newcommand{\dfn}[1]{\textmd{\textsf{Defn#1}}}
\newcommand{\apx}[1]{\textmd{\textsf{Apdx#1}}}
\newcommand{\pag}[1]{\textmd{\textsf{Page#1}}}
\newcommand{\fig}[1]{\textmd{\textsf{Fig #1}}}
\newcommand{\thm}[1]{\textmd{\textsf{Thm #1}}}
\newcommand{\prp}[1]{\textmd{\textsf{Prop#1}}}

\newcommand{\cit}[1]{[\textbf{#1}]}

\newcommand{\moosect}[1]{\par\noindent\textbf{#1} ---}
\newtheorem*{proposition*}{{\textsc{Proposition A}}}

\usepackage{amsfonts, makerobust, float}
\usepackage{mathtools, nicefrac, xstring, enumitem, pdflscape, multicol}
\usepackage[export]{adjustbox}
%---------------------  graphics and figures  ---------------------------------
\usepackage{wrapfig, caption}
\usepackage{hanging, txfonts, ifthen}

\definecolor{moor}{rgb}{0.8,0.2,0.2}
\definecolor{moog}{rgb}{0.2,0.8,0.2}
\definecolor{moob}{rgb}{0.2,0.2,0.8}


\newcommand{\offive}[1]{
    {\tiny
        \raisebox{-0.04cm}{\color{gray}\scalebox{2.5}{$\substack{
            \ifthenelse{\equal{#1}{0}}{{\color{moor}\blacksquare}}{\square} 
        }$}}%
        \raisebox{0.04cm}{$\substack{
            \IfSubStr{#1}{1}{{\color{moor}\blacksquare}}{\square}   
            \IfSubStr{#1}{1}{{\color{moor}\blacksquare}}{\square} \\
            \IfSubStr{#1}{2}{{\color{moor}\blacksquare}}{\square}    
            \IfSubStr{#1}{2}{{\color{moor}\blacksquare}}{\square}    
        }$}%
    }%
}


\newcommand{\ofsix}[1]{
    {\tiny \raisebox{0.04cm}{$\substack{
        \IfSubStr{#1}{0}{{\color{moor}\blacksquare}}{\square}    
        \IfSubStr{#1}{1}{{\color{moor}\blacksquare}}{\square}    
        \IfSubStr{#1}{2}{{\color{moor}\blacksquare}}{\square}  \\ 
        \IfSubStr{#1}{3}{{\color{moor}\blacksquare}}{\square}    
        \IfSubStr{#1}{4}{{\color{moor}\blacksquare}}{\square}    
        \IfSubStr{#1}{5}{{\color{moor}\blacksquare}}{\square}    
    }$}}%
}


\begin{document}

    We thank reviewers \Ra, \Rb, \Rc\ for their feedback.  Reviewers had
    concerns over our work's correctness (\Ra), counterintuitiveness (\Rc), 
    citations (\Rb), and clarity (\Ra,\Rb,\Rc).  We address these concerns in
    sqeuence. 

\moosect{Limits}
    view the expected testing loss as a function
    $L(\eta, T)$.  For each $d$ is a $d$th order truncation $L_d(\eta, T)$, a
    degree-$d$ polynomial in $\eta$ whose coefficients depend on $T$.
    \thm{2} gives a sufficient condition for $L_{d,\infty}(\eta) \triangleq
    \lim_{T\to\infty} L_{d}(\eta,T)$ to exist as well as 
    a formula for $L_{d,\infty}$. %; \prp{1} specializes \thm{2}.
    %
    \Ra\ observes that, though \thm{2} controls
    $\text{LHS}(\eta)\triangleq \lim_{d\to\infty} \lim_{T\to \infty}
    L_d(\eta,T)$, it is $\text{RHS}(\eta)\triangleq \lim_{T\to\infty}
    \lim_{d\to\infty} L_d(\eta,T)$ that more interests us.  How do LHS and
    RHS relate?
    %
    \begin{proposition*}\vspace{-0.25cm}
        Assume \S B.1's boundedness and analyticity properties.
        \textsc{Supposing} that $\nabla l(\theta_\star)=0$ and that on some
        open neighborhood $U$ of $\theta_\star$ the hessian $\nabla\nabla
        l_x(\theta)$ is lower-bounded by some strictly positive definite form
        $Q(\theta)$ continuous in $\theta$, \textsc{then} for any
        initialization $\theta_0\in V$ in some open neighborhood $V$ of
        $\theta_\star$ and for any homogeneous polynomial $p(\eta)$ (of
        $\eta$'s $\text{dim}\times\text{dim}$ many components) with no roots
        (besides $\eta=0$): $\lim_{\eta\to 0}
        (\text{LHS}(\eta)-\text{RHS}(\eta))/p(\eta) = 0$.\vspace{-0.25cm}
    \end{proposition*}
    \begin{proof}[\textbf{Proof idea}]
        Gradient and hessian bounds give for all $\epsilon$ some
        $\delta,\delta^\prime$ so that for all $T$, all $\eta<\delta$, and all
        $\theta_0$ with $|\theta_0-\theta_\star|<\delta^\prime$:
        $|\theta_T-\theta_0|<\epsilon$ with probability $1$.
        %
        In fact, 
        \vspace{-0.25cm}
    \end{proof}
    \text{\color{moor} TODO: prove, discuss, and discuss irrelevance!}
    %
    \prp{A} is a straightforward extension of the proof of \thm{2}.  But from
    our viewpoint, \prp{A} is inessential: in practice (e.g.,
    with CIFAR conv-nets) we have observed none of the pathologies that \prp{A}
    seeks to control (\pag{6, last par}).  We the authors prefer to admit both
    mathematical analysis and scientific measurement as means of
    discovering.   

\moosect{Sharp Minima}
    \Rb\ finds \cor{5}'s statement ({\small that overfitting
    (defined as the increase in testing loss $l$ upon initializing at a local
    minimum of $l$ and then training) is, to second order in $\eta$, greatest
    when the $\eta H$ has moderate eigenvalues}) 
    counterintutive.  We do, too.
    %
    Compare \fig{5\offive{1}} to \cit{Ke}'s \fig{1} and note that SGD's noise
    structure is \emph{not} that of \emph{displacements} in weight space;
    rather, it is that of error terms $\nabla l_x(\theta)-\nabla l(x)$ in the
    \emph{gradient} estimate.  
    %
    Say $\text{dim}=1$ and imagine a testing loss
    $l(\theta)=a \theta^2/2$ and a training loss $\hat
    l(\theta)=l(\theta)+b\theta$.  At the training minimum $\theta=-b/a$, the
    testing loss is $b^2/(2a)$.  So for fixed $b$, sharp minima ($a\gg 1$)
    overfit less (we invite \Rb\ to run the gist at
    \href{https://gist.github.com/anonymous-taylor-series/60ee7ca824e44a9e8f25e69ceb60995e}{gist.github.com/anonymous-taylor-series} to 
    see this `in person').
    %The gradient covariance $C$ controls $b^2$, explaining
    %\cor{5}'s $C/2H$ factor.  
    %
    This example suggests that (\textbf{A}) if we optimize to
    convergence, sharp minima overfit less; that (\textbf{B}) convergence is slow near
    flat minima explains why theory and measurement find that flat minima also
    overfit little.  (Our small-$\eta$ assumption rules out the possibility
    that $H$ is so sharp that SGD diverges: in the regime $1/T \ll \eta H \ll
    1$, sharper minima overfit less).
    %
    Prior work (see \pag{12, par 5}) finds that (contrary to \cit{Ke}) sharp
    minima overfit little.  By explicating $\eta$'s role in translating
    gradients into displacements, our theory accounts for both (\textbf{A}) and
    (\textbf{B}), thus unifying existing pro-flat and pro-sharp intuitions
    (e.g., \cit{Ke} and \cit{Di}).
    %
    We view it as a merit that our formalism makes such counterintuitive
    phenomena visible.
    
\moosect{Implicit Regularization}
    
\moosect{Assumptions} Reviewers  

\moosect{Notation}
    Our paper used a tensor-index convention found in tensor statistics
    (\cit{Mc}, \cit{Dy}) but not in CS at large.  Our camera-ready will make
    all `$\sum$s explicit.  Based on \Rb,\Rc's feedback, we believe this small
    change will substantially improve a reader's experience.

\moosect{Organization}
    Reviewers \Rb,\Rc\ struggled with our paper's organization.  We believe 
    that decomposing the paper into three tracks (to be selected between based
    on a reader's goals) will help us tell our story without causing the burden
    of too many frontloaded concepts or the confusion of too many
    forward references to backloaded concepts.
    %
    \textbf{Track A} [pgs 1-4], for the reader who wants a `free sample', will
    eschew diagrams, general theorems, and \S
    1.1, 2.2's heavy notations.  It will illustrate Taylor series via \S
    2.1's proof, identify the concrete terms relevant to \S 3.3, state
    (with \S B.1's assumptions explicit and with \prp{A}'s level of precision)
    \cor{4}, and conclude with \S 4.2's verification of SGD's sensitivity to
    curl.  
    %
    \textbf{Track B} [pgs 1-4, 5-12], for she who seeks physical intuition for
    our corollaries, will take Track A as motivation for the formalism of \S
    1.1 and \S 2.2, which we will illustrate as in \S A.4's.  \S 2.3 will 
    discuss of re-summation physically, in the style of \fig{5, 7}.
    \S 2.4 will include \prp{A} per \Ra's feedback.  For space, we'll
    move \S 4 to \S C; but each of \S 3.1, 3.2, 3.4 will briefly summarize the
    relevant empirical confirmation.
    %
    \textbf{Track C} [pgs 4-12, 15-42], for she who wishes to use and extend
    our formalism, will .
    %
    We will re-organize the paper accordingly.

\moosect{Surprise}
    We are glad that some of our results surprised \Rb, \Rc.  We believe these
    results, and more importantly the physical-geometric viewpoint that led to
    them, are worth sharing.  Recalling that many of our favorite papers
    are those offering a disorientingly fresh viewpoint, we hope that the
    reviewers can feel the same about our work.

\par\noindent
\vspace{0.1cm}
\hrule

    \noindent
    \cit{Ba} D.G.Barrett, B.Dherin.  Implicit Gradient Regularization.  ICLR 2021.

    \noindent
    \cit{Di} L.Dinh, R.Pascanu, S.Bengio, Y.Bengio.  Sharp Minima Can Generalize for Deep Nets.  ICML 2017.

    \noindent
    \cit{Dy} E.Dyer, G.Gur-Ari.  Asymptotics of Wide Networks from Feynman Diagrams.  ICLR 2020.

    \noindent
    %\cit{Ke} N.S.Keskar, D.Mudigere, J.Nocedal, M.Smelyanskiy, P.T.P.Tang.  On Large-Batch Training for Deep Learning.  ICLR 2017.
    \cit{Ke} N.S.Keskar et alia.  On Large-Batch Training for Deep Learning.  ICLR 2017.

    \noindent
    \cit{Mc} P.McCullagh.  Tensor Methods in Statistics.  Dover Books 2017.

    %\noindent
    %\cit{Mc} J.Ragan-Kelley et alia.  Easy Optimization of Image Processing Pipelines.  ACM Transactions July 2012.

\end{document}
