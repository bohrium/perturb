\documentclass{article}
\usepackage[
    top   =1in,
    bottom=1in,
    left  =1in,
    right =1in,
]{geometry}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[clock,weather]{ifsym}

\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath, amssymb, amsthm, bm}

\newcommand{\Ra}{\textmd{\textsf{\color{purple!50} {R1}}}}
\newcommand{\Rb}{\textmd{\textsf{\color{green!60}  {R2}}}}
\newcommand{\Rc}{\textmd{\textsf{\color{blue!50}   {R3}}}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\Tt}{\mathcal{T}}
\newcommand{\Mm}{\mathcal{M}}
\newcommand{\Hh}{\mathcal{H}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\RR}{\mathbb{R}}

\newcommand{\cor}[1]{\textmd{\textsf{Cor #1}}} 
\newcommand{\dfn}[1]{\textmd{\textsf{Defn#1}}}
\newcommand{\apx}[1]{\textmd{\textsf{Apdx#1}}}
\newcommand{\pag}[1]{\textmd{{\color{gray}Pg}{#1}}}
\newcommand{\pgph}[1]{\textmd{{\color{gray}Par}{#1}}}
\newcommand{\fig}[1]{\textmd{{\color{gray}Fig}#1}}
\newcommand{\thm}[1]{\textmd{{\color{gray}Thm}{#1}}}
\newcommand{\prp}[1]{\textmd{{\color{gray}Prp}{#1}}}

\newcommand{\cit}[1]{[\textbf{#1}]}

\newcommand{\moosect}[1]{\par\textsc{\textbf{#1}} ---}
\newtheorem*{propA*}{{Prop A}}
\newtheorem*{thm2*}{{Thm 2}}

\usepackage{amsfonts, makerobust, float}
\usepackage{mathtools, nicefrac, xstring, enumitem, pdflscape, multicol}
\usepackage[export]{adjustbox}
%---------------------  graphics and figures  ---------------------------------
\usepackage{wrapfig, caption}
\usepackage{hanging, txfonts, ifthen}

\definecolor{moor}{rgb}{0.8,0.2,0.2}
\definecolor{moog}{rgb}{0.2,0.8,0.2}
\definecolor{moob}{rgb}{0.2,0.2,0.8}


\newcommand{\offive}[1]{
    {\tiny
        \raisebox{-0.04cm}{\color{gray}\scalebox{2.5}{$\substack{
            \ifthenelse{\equal{#1}{0}}{{\color{moor}\blacksquare}}{\square} 
        }$}}%
        \raisebox{0.04cm}{$\substack{
            \IfSubStr{#1}{1}{{\color{moor}\blacksquare}}{\square}   
            \IfSubStr{#1}{1}{{\color{moor}\blacksquare}}{\square} \\
            \IfSubStr{#1}{2}{{\color{moor}\blacksquare}}{\square}    
            \IfSubStr{#1}{2}{{\color{moor}\blacksquare}}{\square}    
        }$}%
    }%
}


\newcommand{\ofsix}[1]{
    {\tiny \raisebox{0.04cm}{$\substack{
        \IfSubStr{#1}{0}{{\color{moor}\blacksquare}}{\square}    
        \IfSubStr{#1}{1}{{\color{moor}\blacksquare}}{\square}    
        \IfSubStr{#1}{2}{{\color{moor}\blacksquare}}{\square}  \\ 
        \IfSubStr{#1}{3}{{\color{moor}\blacksquare}}{\square}    
        \IfSubStr{#1}{4}{{\color{moor}\blacksquare}}{\square}    
        \IfSubStr{#1}{5}{{\color{moor}\blacksquare}}{\square}    
    }$}}%
}


\newcommand{\sizeddia}[2]{%
    \begin{gathered}%
        \includegraphics[scale=#2]{../diagrams/#1.png}%
    \end{gathered}%
}
\newcommand{\bdia}[1]{\protect \sizeddia{#1}{0.22}}
\newcommand{\dia} [1]{\protect \sizeddia{#1}{0.18}}
\newcommand{\mdia}[1]{\protect \sizeddia{#1}{0.14}}
\newcommand{\sdia}[1]{\protect \sizeddia{#1}{0.10}}



\begin{document}

    We thank reviewers \Ra, \Rb, \Rc\ for their substantial time investment
    and for incisive feedback.  We address in turn your concerns over
    our work's correctness (\Ra), counterintuitiveness (\Rc), citations (\Rb),
    and clarity (\Ra,\Rb,\Rc): 

\moosect{Limits}
    \newcommand{\LaT}{\Lambda_{\text{\tiny\VarClock}}}
    \newcommand{\Lad}{\Lambda_{\text{\tiny\Thermo{4}}}}
    We thank \Ra\ for highlighting ways our precision must improve.  For
    instance, Thm 2 gives convergence of \emph{truncated} series
    (\pag{6}\pgph{(-1)}).  Explicitly ({\Rc: note \thm{2} defines
    `non-degenerate' as `{$H(\theta_\star)$ is positive definite}'}):
    \begin{thm2*}
        \vspace{-0.25cm}
        For each $d$, each non-deg.\ local min.\ $\theta_\star$ has a
        nbhd $U$ whose every member $\theta_0$ induces, via Thm 1, a
        $T$-indexed sequence (of degree-$d$ polynomials $f_T\in \RR[\eta]$)
        that converges (uniformly on any compact set of $\eta$s) as $T\to
        \infty$ to some element $f_!\in\RR[\eta]$.  Here, $\RR[\eta]$ is the
        formal polynomial ring in $\dim(\Mm)^2$ many variables.
        \vspace{-0.25cm}
    \end{thm2*}
    %
    \noindent
    So if $L_{d,T}(\eta)$ is \thm{1}'s truncation, \thm{2}
    controls $L_d(\eta) = \lim_{\tilde T\to\infty} L_{d,\tilde T}(\eta)$ but not
    $L_T(\eta) = \lim_{\tilde d\to\infty} L_{\tilde d,T}(\eta)$, even for $d,T\gg 1$. 
    What makes \thm{1,2} significant is our \textbf{empirical} finding that
    their formal power series bear on SGD practice (w.r.t.\ which \emph{any}
    infinities are idealizations).\footnote{In the
    classical continuum mechanics (CCM) of thermalized solids, ice cubes have
    infinite ergy; one manipulates \emph{formal} power series.  Yet in
    experience, CCM well-models many phenomena.  \cit{Qu} D.A.McQuarrie,
    J.D.Simon.  \emph{Physical Chemistry}, \S{1-1}.  University Science Books
    1997.}   
    Formal power series are logically consistent algebraic objects \cit{Wi},
    and, as \Rb\ noted, they offer empirically supported intuitive insights into
    SGD. 
    %
    Since our mathematical analysis was but an inspiring probe and a strong
    heuristic, we did not examine conditions (such as \prp{A}'s) under which
    $L_d, L_T$ agree:
    %(\textbf{B}) 
        \begin{propA*}\vspace{-0.25cm}
            Fix $U\subseteq \Mm$ open with compact closure $\bar{U}$, 
            $\theta_\star\in U$ a non-deg.\ local min.\ of $l$.
            Assume \S{B.1} as well as global, prob.-$1$ bounds $(|l_x(\theta)|,\|\nabla
            l_x(\theta_\star)\|)<C$.
            \textbf{If} some $Q_-,Q_+\in \text{SPD}$ bound the hessian
            ($Q_- < \nabla\nabla l_x(\theta)<Q_+$) on $\bar{U}$,
            \textbf{then} for all $d$ and for any $\theta_0$ in
            some nbhd $V_d$ of $\theta_\star$: for some $T_0$ and
            $|g|$ in $\exp(-{\rm big}\Omega(T))$:
            $\sup_{T\geq T_0} |L_d(\eta)-L_T(\eta)-g(T)|$ is
            $o(\eta^d)$
            (and exists on some nbhd in $\text{SPSD}$ of
            $\eta=0$).  Here, $\text{SP(S)D}$ consists of symmetric positive
            (semi)definite forms.
        \end{propA*}

    %We believe that mathematical analysis mixed with pragmatic empiricism  

%- prong 0: X does not have to be close to Y for theory to be useful.  In
%practice, o works with finite T,eta and empirically Thm 1 makes good
%predictions.  We work as physicists.
%
%-- prong 1: Thm 2 has significance as a relation between formal power series:
%that each degree d term of Thm 1's expression converges as T grows is not a
%priori obvious.  Two followup directions: when does Thm 1's expression match
%the limlim (smooth vs analytic) [see prop A]; what is the significance of these
%formal d terms [process interaction interpretation; boundedss<< and 
%empirically good fit occam]
%
%-- prong 2: in special cases, X is provably and quantifiably close to Y
%[proposition A].
%
%-- merged prong: the theory's precise quantitative predictions and novel
%qualitative predictions agree with experiment; the work ahead is that of
%deliating the theory's assumptions and scope.  We take empirics as primary
%answer to this, and we can refi analytic hypotheses in revision
%
%    view the testing loss for fixed initialization as
%    a random function $L(\eta, T)$ (randomss from the training set).  For
%    each natural $d$ is a truncation $L_d(\eta, T)$, a degree-$d$
%    polynomial in $\eta$ whose coefficients depend on $T$.  \thm{2} gives a
%    sufficient existence criterion as well as a formula for
%    $\lim_{T\to\infty} \EE[L_{d}(\eta,T)]$.
%    %
%    \Ra\ stresses that $\text{LHS}_d(\eta)\triangleq
%    \lim_{T\to \infty} \EE[L_d(\eta,T)]$ may differ substantially from
%    $\text{RHS}_T(\eta)\triangleq \lim_{d\to\infty}
%    \EE[L_d(\eta,T)]$, even for $d,T$ large.  However:
%    %
    %\begin{proof}[\textbf{Proof idea}]
    %        Hessian bounds give compact nbhd $\Hh$ of $0$ and $\Tt$ of
    %    $\theta_\star$ so that for all $(T,\eta,\theta_0) \in
    %    \NN\times\Hh\times\Tt$: almost surely $\theta_T\in \Tt$\!.  By gradient
    %    bounds and Chernoff, we may choose $N=T$ so large that with
    %    probability $\geq 1-\delta$: each `{\color{moor} superbatch}' has a sample 
    %    gradient and hessian that agrees with $l$'s to within $\epsilon$. 
    %    %
    %    %By \thm{2}'s formula, $\lim_T \EE[L_{d}(\eta,T)]$ has a degree-$k$
    %    %coefficient bounded in magnitude by $l(\theta_0)$'s degree-$k$
    %    %coefficient times a expontial function $\exp(\beta k)$, where $\beta$
    %    %depends on $Q_-, \theta_0$ but not $T, d$.  Since $l$ is analytic, LHS
    %    %exists.
    %    {\color{moor}{FILL IN}}
    %    \vspace{-0.25cm}
    %\end{proof}
    %\noindent
    %%
    %For instance, \prp{A}'s hypotheses hold for cross-entropy ts defined by
    %analytic functions, applied to 
    %bounded data, and with sufficient ($L_2$) weight decay.
    %
    %But \prp{A} seems relevant more to math than to science:
       %We view our contribution as scientific rather than mathematical and our
    %experiments (e.g., with CIFAR conv-ts) as the key test of our theory's
    %substance.

\moosect{Sharp Minima}
    \Rc\ finds \cor{5}\footnote{i.e.: that overfitting
    ($\triangleq$ $l(\theta_T)-l(\theta_0)$ after initializing at a testing
    minimum)'s second order term is greatest
    when $\eta H$ has moderate eigenvalues}
    counterintutive.  We do, too.
    %
    Compare \fig{5\offive{1}} to \cit{Ke}'s \fig{1}; note that SGD's noise
    consists not of weight \emph{displacements} but of
    error terms $\nabla l_x(\theta)-\nabla l(x)$ in the \emph{gradient}
    estimate.  
    %
    Say $\text{dim}=1$ with testing loss $l(\theta)=a \theta^2/2$ training loss
    $\hat l(\theta)=l(\theta)+b\theta$
    (\href{https://gist.github.com/anonymous-taylor-series/60ee7ca824e44a9e8f25e69ceb60995e}{anonymized code here}).  At the training minimum $\theta=-b/a$, the testing loss is
    $b^2/(2a)$.  So for fixed $b$, sharp minima ($a\gg 1$) overfit less.  The
    covariance $C$ controls $b^2$, explaining
    \cor{5}'s $C/2H$ factor.  
    In this case, optimization to convergence favors sharp minima
    ($\star$); but convergence is slow ar flat minima, so 
    flat minima also overfit little ($\diamond$).  (Our
    small-$\eta$ assumption rules out the possibility that $H$ is so sharp that
    SGD diverges: in the regime $1/T \ll \eta H \ll 1$, sharper minima overfit
    less).
    %
    Prior work (see \pag{12}\pgph{5}) finds that (contrary to \cit{Ke}) sharp
    minima overfit little.  Recognizing $\eta$'s role in translating
    gradients to displacements, our theory accounts for both ($\star$) and
    ($\diamond$) and unifies existing pro-flat and pro-sharp intuitions
    (e.g., \cit{Ke} and \cit{Di}).
    %
    We view it as a merit that our formalism makes such counterintuitive
    phenomena visible.
    
\moosect{Implicit Regularization and ODE}
    We thank \Rb\ for the highly relevant article.  In brief,  
    {\color{moor} FILL IN}
    
\moosect{Assumptions and Verification} \Rc,\Ra\ raise concerns about verifiability.
        {\color{moor}{FILL IN}}

\moosect{Notation}
    \Ra\ notes that we work with vector quantities, not scalars {\color{moor}FILL IN}.
    %
    \Rb,\Rc\ identify `Einstein notation' (though found in tensor
    statistics literature (\cit{Cu}, \cit{Am})) as alien to CS at large.  Our camera-ready
    will make all `$\sum$'s explicit while also referring the reader to
    \cit{Cu} (and to as a w pedagogical Einstein-free appendix \S{D}) for tensor manipulation examples.
    %
    We are also open to translating all diagrams to a more text-friendly
    representation, e.g., $[a][ab:c:d][bcd]$ for $\sdia{MOOc(0-123-4)(01-14-34-24)}$
    (letters name edges).


\moosect{Organization}
    \Rb,\Rc\ stress the challenge of 
    organizing our paper so as to avoid both the burden of too many
    frontloaded concepts and the confusion of too many pointers to
    concepts delayed.  We'll address this challenge by segmenting the paper
    into three tracks (to be selected by a reader based on her goals), each
    with a self-contaid subset of concepts: 
    %
    \textbf{Track A} [pgs 1-4], for readers who want a `free sample', will
    eschew diagrams, general theorems, and \S1.1/\S2.2's heavy notations.  It will illustrate Taylor series via \S
    2.1's proof, identify the concrete terms relevant to \S 3.3, state \cor{4}
    (with \S B.1's assumptions explicit and with \prp{A}'s level of precision),
    and conclude with \S 4.2's verification of SGD's sensitivity to curl.  
    %
    \textbf{Track B} [pgs 1-4, 5-12], for she who seeks our results and their
    physical intuitions, will use Track A to motivate (and \S A.4 to
    illustrate) \S1.1/2.2/2.3's definitions, relegating \S2.2/2.1's Lemma and
    discussions of terms to \S B.  \S 2.4 will include \prp{A} per \Ra's
    feedback as well as a cartoon (in \fig{5,7}'s style) of resummation's
    `physics'.  For space, we'll move \S 4 to \S C; but each of \S 3.1/3.2/3.4
    will briefly summarize the relevant empirical confirmation.
    %
    \textbf{Track C} [pgs 5-12, 15-42], for she who wishes to use and extend
    our formalism, will {\color{moor}{FILL IN}}.

\moosect{References}
    \small%footnotesize
    \cit{Am} S-I.Amari, H.Nagaoka. \emph{Information Geometry}, pg 5.  Oxford UP 1993.  
    \cit{Ba} D.G.Barrett, B.Dherin.  \emph{Implicit Gradient Regularization}.  ICLR 2021.
    \cit{Cu} P.McCullagh.  \emph{Tensor Methods in Statistics}, \S{1.1-1.4},\S{1.8}.  Dover 2017.
    \cit{Di} L.Dinh, R.Pascanu, S.Bengio, Y.Bengio.  \emph{Sharp Minima Can Generalize for Deep Nets}, \S{1},\S{5}.  ICML 2017.
    \cit{Ke} N.S.Keskar et alia.  \emph{Large-Batch Training for Deep Learning}, \S{4}.  ICLR 2017.
    \cit{Wi} H.Wilf.  \emph{Generatingfunctionology}, \S{2.1-2.3}.  Academic Press 1994.

    %---
    %\cit{Tr} C.Truesdell.   \emph{Six Lectures on Modern Natural Philosophy}, pgs 1,2,89. Springer-Verlag 1966.
    %---
    %\cit{Mo} L.d.Moura et alia.  \emph{The Lean Theorem Prover}.  CADE 2015.
%\cit{Dy} E.Dyer, G.Gur-Ari.  \emph{Asymptotics of Wide Networks from Feynman Diagrams}, \S{2.2, B.1}.  ICLR 2020.
    %---

\end{document}

%\moosect{Surprise}
%    We are glad that some of our results surprised \Rb, \Rc.  We believe these
%    results, and more importantly the physical-geometric viewpoint that led to
%    them, are worth sharing.  Recalling that many of our favorite papers
%    are those offering a disorientingly fresh viewpoint, we hope that the
%    reviewers can feel the same about our work.
    %\cit{Ke} N.S.Keskar, D.Mudigere, J.Nocedal, M.Smelyanskiy, P.T.P.Tang.  On Large-Batch Training for Deep Learning.  ICLR 2017.
    %\cit{Mc} J.Ragan-Kelley et alia.  Easy Optimization of Image Processing Pipelis.  ACM Transactions July 2012.


    %We the authors prefer to admit both
    %mathematical analysis and scientific measurement as means of
    %discovering.   

